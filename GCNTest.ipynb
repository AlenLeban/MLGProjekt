{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "3.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0021)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = R2Score()\n",
    "tens1 = torch.rand(900, 30)\n",
    "tens2 = torch.rand(900, 30)\n",
    "#print(tens1, tens2)\n",
    "metric.update(tens1, tens2)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the modified to_networkx function that doesn't throw exceptions\n",
    "\n",
    "def from_networkx(\n",
    "    G: Any,\n",
    "    group_node_attrs: Optional[Union[List[str], all]] = None,\n",
    "    group_edge_attrs: Optional[Union[List[str], all]] = None,\n",
    ") -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "        group_node_attrs (List[str] or all, optional): The node attributes to\n",
    "            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)\n",
    "        group_edge_attrs (List[str] or all, optional): The edge attributes to\n",
    "            be concatenated and added to :obj:`data.edge_attr`.\n",
    "            (default: :obj:`None`)\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must\n",
    "        be numeric.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> edge_index = torch.tensor([\n",
    "        ...     [0, 1, 1, 2, 2, 3],\n",
    "        ...     [1, 0, 2, 1, 3, 2],\n",
    "        ... ])\n",
    "        >>> data = Data(edge_index=edge_index, num_nodes=4)\n",
    "        >>> g = to_networkx(data)\n",
    "        >>> # A `Data` object is returned\n",
    "        >>> from_networkx(g)\n",
    "        Data(edge_index=[2, 6], num_nodes=4)\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        edges = list(G.edges(keys=False))\n",
    "    else:\n",
    "        edges = list(G.edges)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    if G.number_of_nodes() > 0:\n",
    "        node_attrs = list(next(iter(G.nodes(data=True)))[-1].keys())\n",
    "    else:\n",
    "        node_attrs = {}\n",
    "\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_attrs = list(next(iter(G.edges(data=True)))[-1].keys())\n",
    "    else:\n",
    "        edge_attrs = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        if set(feat_dict.keys()) != set(node_attrs):\n",
    "            raise ValueError('Not all nodes contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        if set(feat_dict.keys()) != set(edge_attrs):\n",
    "            raise ValueError('Not all edges contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for key, value in G.graph.items():\n",
    "        key = f'graph_{key}' if key in node_attrs else key\n",
    "        data[str(key)] = value\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (tuple, list)) and isinstance(value[0], Tensor):\n",
    "            data[key] = torch.stack(value, dim=0)\n",
    "        else:\n",
    "            try:\n",
    "                data[key] = torch.tensor(value)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = Data.from_dict(data)\n",
    "\n",
    "    if group_node_attrs is all:\n",
    "        group_node_attrs = list(node_attrs)\n",
    "    if group_node_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_node_attrs:\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if group_edge_attrs is all:\n",
    "        group_edge_attrs = list(edge_attrs)\n",
    "    if group_edge_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_edge_attrs:\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.edge_attr = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if data.x is None and data.pos is None:\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "def to_edge_index(adj: Union[Tensor, SparseTensor]) -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Converts a :class:`torch.sparse.Tensor` or a\n",
    "    :class:`torch_sparse.SparseTensor` to edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.sparse.Tensor or SparseTensor): The adjacency matrix.\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "        ...                            [1, 0, 2, 1, 3, 2]])\n",
    "        >>> adj = to_torch_coo_tensor(edge_index)\n",
    "        >>> to_edge_index(adj)\n",
    "        (tensor([[0, 1, 1, 2, 2, 3],\n",
    "                [1, 0, 2, 1, 3, 2]]),\n",
    "        tensor([1., 1., 1., 1., 1., 1.]))\n",
    "    \"\"\"\n",
    "    if isinstance(adj, SparseTensor):\n",
    "        row, col, value = adj.coo()\n",
    "        if value is None:\n",
    "            value = torch.ones(row.size(0), device=row.device)\n",
    "        return torch.stack([row, col], dim=0), value\n",
    "\n",
    "    if adj.requires_grad:\n",
    "        # Calling adj._values() will return a detached tensor.\n",
    "        # Use `adj.coalesce().values()` instead to track gradients.\n",
    "        adj = adj.coalesce()\n",
    "        return adj.indices(), adj.values()\n",
    "\n",
    "    return adj._indices(), adj._values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "        \"Point\": 0,\n",
    "        \"Line\": 1,\n",
    "        \"Circle\": 2,\n",
    "        \"Ellipse\": 3,\n",
    "        \"Spline\": 4,\n",
    "        \"Conic\": 5,\n",
    "        \"Arc\": 6,\n",
    "        \"External\": 7,\n",
    "        \"Stop\": 8,\n",
    "        \"Unknown\": 9,\n",
    "        \"SN_Start\": 11,\n",
    "        \"SN_End\": 12,\n",
    "        \"SN_Center\": 13\n",
    "    }\n",
    "\n",
    "edge_dict = {\n",
    "    \"Coincident\": 0,\n",
    "    \"Projected\": 1,\n",
    "    \"Mirror\": 2,\n",
    "    \"Distance\": 3,\n",
    "    \"Horizontal\": 4,\n",
    "    \"Parallel\": 5,\n",
    "    \"Vertical\": 6,\n",
    "    \"Tangent\": 7,\n",
    "    \"Length\": 8,\n",
    "    \"Perpendicular\": 9,\n",
    "    \"Midpoint\": 10,\n",
    "    \"Equal\": 11,\n",
    "    \"Diameter\": 12,\n",
    "    \"Offset\": 13,\n",
    "    \"Radius\": 14,\n",
    "    \"Concentric\": 15,\n",
    "    \"Fix\": 16,\n",
    "    \"Angle\": 17,\n",
    "    \"Circular_Pattern\": 18,\n",
    "    \"Pierce\": 19,\n",
    "    \"Linear_Pattern\": 20,\n",
    "    \"Centerline_Dimension\": 21,\n",
    "    \"Intersected\": 22,\n",
    "    \"Silhoutted\": 23,\n",
    "    \"Quadrant\": 24,\n",
    "    \"Normal\": 25,\n",
    "    \"Minor_Diameter\": 26,\n",
    "    \"Major_Diameter\": 27,\n",
    "    \"Rho\": 28,\n",
    "    \"Unknown\": 29,\n",
    "    \"Subnode\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "def get_sketch_features(graph, feature_dim):\n",
    "    x = torch.zeros([graph.num_nodes, feature_dim])\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx, p in enumerate(graph.parameters):\n",
    "        \n",
    "        # add one hot encoding to feature vector for node label\n",
    "        onePos = label_dict[graph.label[idx]]/7\n",
    "        for i in range(0, 14):\n",
    "            x[idx, i] = 1 if onePos==i else 0\n",
    "        \n",
    "        # convert label text into a feature value\n",
    "        x[idx, 14] = label_dict[graph.label[idx]]/7\n",
    "        \n",
    "        param_dict = json.loads(p)\n",
    "        for i, k in enumerate(param_dict.keys()):\n",
    "            \n",
    "            if i+2 == feature_dim:\n",
    "                break\n",
    "            \n",
    "            # convert each parameter value into a feature value\n",
    "            x[idx, i+15] = float(param_dict[k])\n",
    "        \n",
    "        x[idx, -1] = degree(graph.edge_index[0], graph.num_nodes)[idx]\n",
    "        #print(idx, p)\n",
    "        #print(x[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_attr_y(graph):\n",
    "    y = torch.zeros([graph.num_nodes, 1], dtype=torch.int64)\n",
    "    #rint(graph.label)\n",
    "    \n",
    "    \n",
    "    for i, l in enumerate(graph.label):\n",
    "        y[i, 0] = label_dict[l]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_adj(graph):\n",
    "    tst = T.ToSparseTensor()\n",
    "    return tst(graph).adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_large_graph(graphs):\n",
    "    bestN = 0\n",
    "    bestI = 0\n",
    "    for i, g in enumerate(graphs):\n",
    "        if len(g) > bestN:\n",
    "            bestN = len(g)\n",
    "            bestI = i\n",
    "    #print(bestI)\n",
    "    return graphs[bestI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_edge_attr(graph):\n",
    "    dim = 31\n",
    "    edge_attr = torch.zeros([len(graph.edge_label), dim])\n",
    "    for idx, l in enumerate(graph.edge_label):\n",
    "        edge_attr[idx, edge_dict[l]] = 1\n",
    "    return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom dataset class of custom attributes\n",
    "from torch_geometric.data import Dataset\n",
    "class SketchgraphDataset(Dataset):\n",
    "    def __init__(self, start_idx, end_idx, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data = []\n",
    "        seq_data = flat_array.load_dictionary_flat('datasets/sg_t16_validation.npy')\n",
    "        print(len(seq_data['sequences']))\n",
    "\n",
    "        #test_graph_seq = find_large_graph(seq_data['sequences'])\n",
    "        test_graph_seq = seq_data['sequences'][206778]\n",
    "        test_graph_seq1 = seq_data['sequences'][10]\n",
    "\n",
    "        sketchgraps_list = seq_data['sequences'][start_idx:end_idx]\n",
    "\n",
    "        for sg in sketchgraps_list:\n",
    "            #print(test_graph_seq)\n",
    "\n",
    "            # convert first to pyGraphViz graph using sketchgraph's function\n",
    "            pgv_graph = sketchgraphs.data.sequence.pgvgraph_from_sequence(sg)\n",
    "            #print(pgv_graph)\n",
    "\n",
    "            # then to networkx graph\n",
    "            nx_graph = nx.Graph(pgv_graph)\n",
    "            #print(nx_graph)\n",
    "\n",
    "            # finally to pyTorch graph\n",
    "            graph = from_networkx(nx_graph)\n",
    "            \n",
    "            if not hasattr(graph, 'edge_label'):\n",
    "                continue\n",
    "\n",
    "            # next we need to add required attributes: x, y, adj_t\n",
    "            graph.x = get_sketch_features(graph, 30)\n",
    "            graph.y = get_sketch_attr_y(graph)\n",
    "            graph.adj_t = get_sketch_adj(graph)\n",
    "            #print(graph.x)\n",
    "            graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "            graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "            #print(len(graph.edge_label))\n",
    "            self.data.append(graph)\n",
    "            \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\"\"\"def custom_collate(batch, b):\n",
    "    print(\"AAAAA\")\n",
    "    edge_label_batch = []\n",
    "    for b in batch:\n",
    "        #print(b.edge_label)\n",
    "        edge_label_batch.append(b.edge_label)\n",
    "    return edge_label_batch\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "315228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 4.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 3.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sketchgraphs\n",
    "import networkx as nx\n",
    "from sketchgraphs.data import flat_array\n",
    "\n",
    "train_dataset = SketchgraphDataset(0, 1023)\n",
    "test_dataset = SketchgraphDataset(1024, 2047)\n",
    "    \n",
    "data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "next(iter(data_loader)).x\n",
    "#for b in iter(data_loader):\n",
    "#    print(b)\n",
    "#print(graph)\n",
    "#print(graph.x)\n",
    "#print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n",
      "Device: cuda\n",
      "tensor([     0,      1,      2,  ..., 169145, 169148, 169251])\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    dataset_name = 'ogbn-arxiv'\n",
    "    dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    #data = batch\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the adjacency matrix to symmetric\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    #print(data.y)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # If you use GPU, the device should be cuda\n",
    "    print('Device: {}'.format(device))\n",
    "    data = data.to(device)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    print(split_idx['train'])\n",
    "    #train_idx = torch.LongTensor(range(0, data.num_nodes)).to(device)\n",
    "    #print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class nnconvnn(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(nnconvnn, self).__init__()\n",
    "        \n",
    "        self.simpleLin = torch.nn.Linear(31, input_dim*output_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.simpleLin.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.simpleLin(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        #self.testnnconv = \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim)))\n",
    "        self.convs.append(NNConv(hidden_dim, hidden_dim, nnconvnn(hidden_dim, hidden_dim)))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        F.dropout.training = self.training\n",
    "        #x = self.testnnconv(x, edge_index, edge_attr)\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        x = self.linear(x)\n",
    "        #if self.return_embeds == False:\n",
    "        #    x = self.softmax(x)\n",
    "        out = x\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loader, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    accLoss = 0\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    for batch in iter(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        # o = o[train_idx] # we train on the whole graphs now\n",
    "        #print(o)\n",
    "        #print(batch.y.squeeze())\n",
    "        loss = loss_fn(o, batch.x)\n",
    "        accLoss += loss.item()\n",
    "        #########################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return accLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, loader, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        batch = batch.to(device)\n",
    "        ############# Your code here ############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. No index slicing here\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        #########################################\n",
    "\n",
    "        y_pred = out\n",
    "        #y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        if count == 1:\n",
    "            print(out, batch.x)\n",
    "        #print(out.shape)\n",
    "        #print(\"-----\")\n",
    "        #print(batch.x.shape)\n",
    "        #score = r2_score(out, batch.x)\n",
    "        #print(score)\n",
    "        score = torch.sum((out - batch.x)**2)\n",
    "        train_acc += score\n",
    "        valid_acc += score\n",
    "        test_acc += score\n",
    "        \n",
    "        #train_acc += evaluator.eval({\n",
    "        #    'y_true': batch.x,\n",
    "        #    'y_pred': y_pred,\n",
    "        #})['acc']\n",
    "        #valid_acc += evaluator.eval({\n",
    "        #    'y_true': batch.x,\n",
    "        #    'y_pred': y_pred,\n",
    "        #})['acc']\n",
    "        #test_acc += evaluator.eval({\n",
    "        #    'y_true': batch.x,\n",
    "        #    'y_pred': y_pred,\n",
    "        #})['acc']\n",
    "\n",
    "    #train_acc /= count\n",
    "    #valid_acc /= count\n",
    "    #test_acc /= count\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "    }\n",
    "    args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    model = GCN(30, args['hidden_dim'],\n",
    "              30, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')\n",
    "    \n",
    "    # 14 is number of classes\n",
    "    # 30 is number of dimensions in node feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 238.9453, Train: 1514343.62%, Valid: 1514343.62% Test: 1514343.62%\n",
      "Epoch: 02, Loss: 42.3953, Train: 469863.31%, Valid: 469863.31% Test: 469863.31%\n",
      "Epoch: 03, Loss: 20.2381, Train: 295761.12%, Valid: 295761.12% Test: 295761.12%\n",
      "Epoch: 04, Loss: 8.8774, Train: 182223.69%, Valid: 182223.69% Test: 182223.69%\n",
      "Epoch: 05, Loss: 5.9361, Train: 105968.76%, Valid: 105968.76% Test: 105968.76%\n",
      "Epoch: 06, Loss: 4.2974, Train: 91951.96%, Valid: 91951.96% Test: 91951.96%\n",
      "Epoch: 07, Loss: 3.8151, Train: 84782.12%, Valid: 84782.12% Test: 84782.12%\n",
      "Epoch: 08, Loss: 2.6899, Train: 78232.77%, Valid: 78232.77% Test: 78232.77%\n",
      "Epoch: 09, Loss: 2.4607, Train: 67114.23%, Valid: 67114.23% Test: 67114.23%\n",
      "Epoch: 10, Loss: 2.1254, Train: 58105.36%, Valid: 58105.36% Test: 58105.36%\n",
      "Epoch: 11, Loss: 2.3530, Train: 57820.50%, Valid: 57820.50% Test: 57820.50%\n",
      "Epoch: 12, Loss: 1.9196, Train: 52058.04%, Valid: 52058.04% Test: 52058.04%\n",
      "Epoch: 13, Loss: 1.7275, Train: 46947.95%, Valid: 46947.95% Test: 46947.95%\n",
      "Epoch: 14, Loss: 1.8646, Train: 44899.76%, Valid: 44899.76% Test: 44899.76%\n",
      "Epoch: 15, Loss: 1.8406, Train: 44307.68%, Valid: 44307.68% Test: 44307.68%\n",
      "Epoch: 16, Loss: 1.9840, Train: 41165.30%, Valid: 41165.30% Test: 41165.30%\n",
      "Epoch: 17, Loss: 1.8660, Train: 43411.95%, Valid: 43411.95% Test: 43411.95%\n",
      "Epoch: 18, Loss: 1.6292, Train: 47449.45%, Valid: 47449.45% Test: 47449.45%\n",
      "Epoch: 19, Loss: 1.4498, Train: 36968.59%, Valid: 36968.59% Test: 36968.59%\n",
      "Epoch: 20, Loss: 1.6933, Train: 38572.92%, Valid: 38572.92% Test: 38572.92%\n",
      "Epoch: 21, Loss: 1.1738, Train: 33909.26%, Valid: 33909.26% Test: 33909.26%\n",
      "Epoch: 22, Loss: 1.4474, Train: 33767.21%, Valid: 33767.21% Test: 33767.21%\n",
      "Epoch: 23, Loss: 1.3289, Train: 35864.29%, Valid: 35864.29% Test: 35864.29%\n",
      "Epoch: 24, Loss: 1.4336, Train: 39683.26%, Valid: 39683.26% Test: 39683.26%\n",
      "Epoch: 25, Loss: 1.4333, Train: 32998.36%, Valid: 32998.36% Test: 32998.36%\n",
      "Epoch: 26, Loss: 1.4316, Train: 29818.32%, Valid: 29818.32% Test: 29818.32%\n",
      "Epoch: 27, Loss: 1.1371, Train: 30542.56%, Valid: 30542.56% Test: 30542.56%\n",
      "Epoch: 28, Loss: 1.1766, Train: 28771.42%, Valid: 28771.42% Test: 28771.42%\n",
      "Epoch: 29, Loss: 1.2980, Train: 30565.24%, Valid: 30565.24% Test: 30565.24%\n",
      "Epoch: 30, Loss: 1.2622, Train: 36860.69%, Valid: 36860.69% Test: 36860.69%\n",
      "Epoch: 31, Loss: 1.6070, Train: 27322.47%, Valid: 27322.47% Test: 27322.47%\n",
      "Epoch: 32, Loss: 1.0446, Train: 26950.29%, Valid: 26950.29% Test: 26950.29%\n",
      "Epoch: 33, Loss: 1.7895, Train: 34412.66%, Valid: 34412.66% Test: 34412.66%\n",
      "Epoch: 34, Loss: 1.2677, Train: 34398.27%, Valid: 34398.27% Test: 34398.27%\n",
      "Epoch: 35, Loss: 1.3585, Train: 25014.49%, Valid: 25014.49% Test: 25014.49%\n",
      "Epoch: 36, Loss: 1.0347, Train: 24694.91%, Valid: 24694.91% Test: 24694.91%\n",
      "Epoch: 37, Loss: 1.0557, Train: 24969.57%, Valid: 24969.57% Test: 24969.57%\n",
      "Epoch: 38, Loss: 1.1538, Train: 25345.20%, Valid: 25345.20% Test: 25345.20%\n",
      "Epoch: 39, Loss: 1.0487, Train: 24404.05%, Valid: 24404.05% Test: 24404.05%\n",
      "Epoch: 40, Loss: 0.8702, Train: 24422.64%, Valid: 24422.64% Test: 24422.64%\n",
      "Epoch: 41, Loss: 0.9749, Train: 25781.53%, Valid: 25781.53% Test: 25781.53%\n",
      "Epoch: 42, Loss: 1.1664, Train: 23262.35%, Valid: 23262.35% Test: 23262.35%\n",
      "Epoch: 43, Loss: 1.6622, Train: 26647.03%, Valid: 26647.03% Test: 26647.03%\n",
      "Epoch: 44, Loss: 1.0434, Train: 22848.32%, Valid: 22848.32% Test: 22848.32%\n",
      "Epoch: 45, Loss: 1.0636, Train: 26011.34%, Valid: 26011.34% Test: 26011.34%\n",
      "Epoch: 46, Loss: 1.4227, Train: 23851.69%, Valid: 23851.69% Test: 23851.69%\n",
      "Epoch: 47, Loss: 0.9063, Train: 25063.57%, Valid: 25063.57% Test: 25063.57%\n",
      "Epoch: 48, Loss: 0.8630, Train: 21192.66%, Valid: 21192.66% Test: 21192.66%\n",
      "Epoch: 49, Loss: 0.9322, Train: 23975.44%, Valid: 23975.44% Test: 23975.44%\n",
      "Epoch: 50, Loss: 0.8299, Train: 21425.27%, Valid: 21425.27% Test: 21425.27%\n",
      "Epoch: 51, Loss: 1.1743, Train: 27996.81%, Valid: 27996.81% Test: 27996.81%\n",
      "Epoch: 52, Loss: 1.7817, Train: 30695.68%, Valid: 30695.68% Test: 30695.68%\n",
      "Epoch: 53, Loss: 1.0618, Train: 24689.25%, Valid: 24689.25% Test: 24689.25%\n",
      "Epoch: 54, Loss: 1.1798, Train: 27703.27%, Valid: 27703.27% Test: 27703.27%\n",
      "Epoch: 55, Loss: 1.4086, Train: 34337.27%, Valid: 34337.27% Test: 34337.27%\n",
      "Epoch: 56, Loss: 2.5506, Train: 26876.37%, Valid: 26876.37% Test: 26876.37%\n",
      "Epoch: 57, Loss: 1.0758, Train: 36899.99%, Valid: 36899.99% Test: 36899.99%\n",
      "Epoch: 58, Loss: 1.2888, Train: 29881.34%, Valid: 29881.34% Test: 29881.34%\n",
      "Epoch: 59, Loss: 1.2540, Train: 28552.53%, Valid: 28552.53% Test: 28552.53%\n",
      "Epoch: 60, Loss: 0.8279, Train: 26412.73%, Valid: 26412.73% Test: 26412.73%\n",
      "Epoch: 61, Loss: 0.8591, Train: 22099.81%, Valid: 22099.81% Test: 22099.81%\n",
      "Epoch: 62, Loss: 1.1334, Train: 24073.78%, Valid: 24073.78% Test: 24073.78%\n",
      "Epoch: 63, Loss: 1.1833, Train: 27371.25%, Valid: 27371.25% Test: 27371.25%\n",
      "Epoch: 64, Loss: 1.2116, Train: 24497.09%, Valid: 24497.09% Test: 24497.09%\n",
      "Epoch: 65, Loss: 1.2809, Train: 31255.61%, Valid: 31255.61% Test: 31255.61%\n",
      "Epoch: 66, Loss: 1.8543, Train: 37016.29%, Valid: 37016.29% Test: 37016.29%\n",
      "Epoch: 67, Loss: 2.0154, Train: 44455.89%, Valid: 44455.89% Test: 44455.89%\n",
      "Epoch: 68, Loss: 3.7508, Train: 41343.93%, Valid: 41343.93% Test: 41343.93%\n",
      "Epoch: 69, Loss: 1.3979, Train: 39055.91%, Valid: 39055.91% Test: 39055.91%\n",
      "Epoch: 70, Loss: 2.2597, Train: 43960.52%, Valid: 43960.52% Test: 43960.52%\n",
      "Epoch: 71, Loss: 3.6651, Train: 52735.53%, Valid: 52735.53% Test: 52735.53%\n",
      "Epoch: 72, Loss: 1.8237, Train: 34986.28%, Valid: 34986.28% Test: 34986.28%\n",
      "Epoch: 73, Loss: 1.8453, Train: 45098.89%, Valid: 45098.89% Test: 45098.89%\n",
      "Epoch: 74, Loss: 1.6208, Train: 37812.58%, Valid: 37812.58% Test: 37812.58%\n",
      "Epoch: 75, Loss: 1.9824, Train: 50287.79%, Valid: 50287.79% Test: 50287.79%\n",
      "Epoch: 76, Loss: 3.1721, Train: 28710.67%, Valid: 28710.67% Test: 28710.67%\n",
      "Epoch: 77, Loss: 1.4899, Train: 34176.34%, Valid: 34176.34% Test: 34176.34%\n",
      "Epoch: 78, Loss: 1.1734, Train: 24999.71%, Valid: 24999.71% Test: 24999.71%\n",
      "Epoch: 79, Loss: 1.1848, Train: 29126.31%, Valid: 29126.31% Test: 29126.31%\n",
      "Epoch: 80, Loss: 1.4201, Train: 26192.70%, Valid: 26192.70% Test: 26192.70%\n",
      "Epoch: 81, Loss: 2.8804, Train: 65857.98%, Valid: 65857.98% Test: 65857.98%\n",
      "Epoch: 82, Loss: 3.9301, Train: 38059.59%, Valid: 38059.59% Test: 38059.59%\n",
      "Epoch: 83, Loss: 1.4803, Train: 39708.22%, Valid: 39708.22% Test: 39708.22%\n",
      "Epoch: 84, Loss: 1.2913, Train: 27547.34%, Valid: 27547.34% Test: 27547.34%\n",
      "Epoch: 85, Loss: 0.9617, Train: 23941.46%, Valid: 23941.46% Test: 23941.46%\n",
      "Epoch: 86, Loss: 1.3378, Train: 27322.05%, Valid: 27322.05% Test: 27322.05%\n",
      "Epoch: 87, Loss: 0.9580, Train: 26000.78%, Valid: 26000.78% Test: 26000.78%\n",
      "Epoch: 88, Loss: 1.4275, Train: 25313.75%, Valid: 25313.75% Test: 25313.75%\n",
      "Epoch: 89, Loss: 1.2319, Train: 25986.95%, Valid: 25986.95% Test: 25986.95%\n",
      "Epoch: 90, Loss: 1.3028, Train: 28346.43%, Valid: 28346.43% Test: 28346.43%\n",
      "Epoch: 91, Loss: 1.2702, Train: 30597.48%, Valid: 30597.48% Test: 30597.48%\n",
      "Epoch: 92, Loss: 1.7372, Train: 27140.06%, Valid: 27140.06% Test: 27140.06%\n",
      "Epoch: 93, Loss: 1.0125, Train: 25930.39%, Valid: 25930.39% Test: 25930.39%\n",
      "Epoch: 94, Loss: 1.1537, Train: 23450.91%, Valid: 23450.91% Test: 23450.91%\n",
      "Epoch: 95, Loss: 1.1406, Train: 32717.28%, Valid: 32717.28% Test: 32717.28%\n",
      "Epoch: 96, Loss: 1.3478, Train: 24665.67%, Valid: 24665.67% Test: 24665.67%\n",
      "Epoch: 97, Loss: 1.3394, Train: 26523.15%, Valid: 26523.15% Test: 26523.15%\n",
      "Epoch: 98, Loss: 2.0148, Train: 30139.93%, Valid: 30139.93% Test: 30139.93%\n",
      "Epoch: 99, Loss: 1.4759, Train: 27185.38%, Valid: 27185.38% Test: 27185.38%\n",
      "Epoch: 100, Loss: 1.5526, Train: 31021.29%, Valid: 31021.29% Test: 31021.29%\n"
     ]
    }
   ],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    # reset the parameters to initial random value\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "        loss = train(model, data_loader, [], optimizer, loss_fn)\n",
    "        result = test(model, test_loader, [], evaluator)\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.2f}%, '\n",
    "              f'Valid: {valid_acc:.2f}% '\n",
    "              f'Test: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.8090e-02,  9.4044e-01,  4.4156e-03,  ..., -3.3673e-03,\n",
      "          1.2061e-02,  1.6240e+00],\n",
      "        [ 3.3149e-02, -1.5415e-02, -1.2416e-02,  ...,  1.9163e-04,\n",
      "         -3.2548e-03,  3.1709e+00],\n",
      "        [ 1.5245e-03, -5.3883e-03, -1.0226e-02,  ..., -1.4620e-02,\n",
      "         -2.1817e-02,  3.0139e+00],\n",
      "        ...,\n",
      "        [ 2.9446e-02,  2.1011e-02,  2.0782e-04,  ..., -6.3004e-03,\n",
      "         -1.5868e-02,  2.2036e+00],\n",
      "        [ 2.5275e-03, -1.9062e-02,  1.9872e-03,  ..., -6.7043e-03,\n",
      "          4.9330e-03,  2.2431e+00],\n",
      "        [ 2.6234e-02, -3.7411e-02,  1.2463e-03,  ..., -9.5431e-03,\n",
      "          3.3765e-03,  2.1507e+00]], device='cuda:0') tensor([[0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 3.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 3.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 2.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "result = test(model, data_loader, [], evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
