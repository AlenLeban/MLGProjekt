{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import RGCNConv\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the modified to_networkx function that doesn't throw exceptions\n",
    "\n",
    "def from_networkx(\n",
    "    G: Any,\n",
    "    group_node_attrs: Optional[Union[List[str], all]] = None,\n",
    "    group_edge_attrs: Optional[Union[List[str], all]] = None,\n",
    ") -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "        group_node_attrs (List[str] or all, optional): The node attributes to\n",
    "            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)\n",
    "        group_edge_attrs (List[str] or all, optional): The edge attributes to\n",
    "            be concatenated and added to :obj:`data.edge_attr`.\n",
    "            (default: :obj:`None`)\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must\n",
    "        be numeric.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> edge_index = torch.tensor([\n",
    "        ...     [0, 1, 1, 2, 2, 3],\n",
    "        ...     [1, 0, 2, 1, 3, 2],\n",
    "        ... ])\n",
    "        >>> data = Data(edge_index=edge_index, num_nodes=4)\n",
    "        >>> g = to_networkx(data)\n",
    "        >>> # A `Data` object is returned\n",
    "        >>> from_networkx(g)\n",
    "        Data(edge_index=[2, 6], num_nodes=4)\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        edges = list(G.edges(keys=False))\n",
    "    else:\n",
    "        edges = list(G.edges)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    if G.number_of_nodes() > 0:\n",
    "        node_attrs = list(next(iter(G.nodes(data=True)))[-1].keys())\n",
    "    else:\n",
    "        node_attrs = {}\n",
    "\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_attrs = list(next(iter(G.edges(data=True)))[-1].keys())\n",
    "    else:\n",
    "        edge_attrs = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        if set(feat_dict.keys()) != set(node_attrs):\n",
    "            raise ValueError('Not all nodes contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        if set(feat_dict.keys()) != set(edge_attrs):\n",
    "            raise ValueError('Not all edges contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for key, value in G.graph.items():\n",
    "        key = f'graph_{key}' if key in node_attrs else key\n",
    "        data[str(key)] = value\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (tuple, list)) and isinstance(value[0], Tensor):\n",
    "            data[key] = torch.stack(value, dim=0)\n",
    "        else:\n",
    "            try:\n",
    "                data[key] = torch.tensor(value)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = Data.from_dict(data)\n",
    "\n",
    "    if group_node_attrs is all:\n",
    "        group_node_attrs = list(node_attrs)\n",
    "    if group_node_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_node_attrs:\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if group_edge_attrs is all:\n",
    "        group_edge_attrs = list(edge_attrs)\n",
    "    if group_edge_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_edge_attrs:\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.edge_attr = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if data.x is None and data.pos is None:\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "def to_edge_index(adj: Union[Tensor, SparseTensor]) -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Converts a :class:`torch.sparse.Tensor` or a\n",
    "    :class:`torch_sparse.SparseTensor` to edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.sparse.Tensor or SparseTensor): The adjacency matrix.\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "        ...                            [1, 0, 2, 1, 3, 2]])\n",
    "        >>> adj = to_torch_coo_tensor(edge_index)\n",
    "        >>> to_edge_index(adj)\n",
    "        (tensor([[0, 1, 1, 2, 2, 3],\n",
    "                [1, 0, 2, 1, 3, 2]]),\n",
    "        tensor([1., 1., 1., 1., 1., 1.]))\n",
    "    \"\"\"\n",
    "    if isinstance(adj, SparseTensor):\n",
    "        row, col, value = adj.coo()\n",
    "        if value is None:\n",
    "            value = torch.ones(row.size(0), device=row.device)\n",
    "        return torch.stack([row, col], dim=0), value\n",
    "\n",
    "    if adj.requires_grad:\n",
    "        # Calling adj._values() will return a detached tensor.\n",
    "        # Use `adj.coalesce().values()` instead to track gradients.\n",
    "        adj = adj.coalesce()\n",
    "        return adj.indices(), adj.values()\n",
    "\n",
    "    return adj._indices(), adj._values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "        \"Point\": 0,\n",
    "        \"Line\": 1,\n",
    "        \"Circle\": 2,\n",
    "        \"Ellipse\": 3,\n",
    "        \"Spline\": 4,\n",
    "        \"Conic\": 5,\n",
    "        \"Arc\": 6,\n",
    "        \"External\": 7,\n",
    "        \"Stop\": 8,\n",
    "        \"Unknown\": 9,\n",
    "        \"SN_Start\": 11,\n",
    "        \"SN_End\": 12,\n",
    "        \"SN_Center\": 13\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "def get_sketch_features(graph, feature_dim):\n",
    "    x = torch.zeros([graph.num_nodes, feature_dim])\n",
    "    for idx, p in enumerate(graph.parameters):\n",
    "        \n",
    "        # convert label text into a feature value\n",
    "        x[idx, 0] = label_dict[graph.label[idx]]/7\n",
    "        \n",
    "        param_dict = json.loads(p)\n",
    "        for i, k in enumerate(param_dict.keys()):\n",
    "            \n",
    "            if i+2 == feature_dim:\n",
    "                break\n",
    "            \n",
    "            # convert each parameter value into a feature value\n",
    "            x[idx, i+1] = float(param_dict[k])\n",
    "        \n",
    "        x[idx, -1] = degree(graph.edge_index[0], graph.num_nodes)[idx]\n",
    "        #print(idx, p)\n",
    "        #print(x[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_attr_y(graph):\n",
    "    y = torch.zeros([graph.num_nodes, 1], dtype=torch.int64)\n",
    "    #rint(graph.label)\n",
    "    \n",
    "    \n",
    "    for i, l in enumerate(graph.label):\n",
    "        y[i, 0] = label_dict[l]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_adj(graph):\n",
    "    tst = T.ToSparseTensor()\n",
    "    return tst(graph).adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_large_graph(graphs):\n",
    "    bestN = 0\n",
    "    bestI = 0\n",
    "    for i, g in enumerate(graphs):\n",
    "        if len(g) > bestN:\n",
    "            bestN = len(g)\n",
    "            bestI = i\n",
    "    #print(bestI)\n",
    "    return graphs[bestI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def custom_collate(batch, b):\\n    print(\"AAAAA\")\\n    edge_label_batch = []\\n    for b in batch:\\n        #print(b.edge_label)\\n        edge_label_batch.append(b.edge_label)\\n    return edge_label_batch'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom dataset class of custom attributes\n",
    "from torch_geometric.data import Dataset\n",
    "class SketchgraphDataset(Dataset):\n",
    "    def __init__(self, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data = []\n",
    "        seq_data = flat_array.load_dictionary_flat('datasets/sg_t16_validation.npy')\n",
    "        print(len(seq_data['sequences']))\n",
    "\n",
    "        #test_graph_seq = find_large_graph(seq_data['sequences'])\n",
    "        test_graph_seq = seq_data['sequences'][206778]\n",
    "        test_graph_seq1 = seq_data['sequences'][10]\n",
    "\n",
    "        sketchgraps_list = seq_data['sequences'][0:1023]\n",
    "\n",
    "        for sg in sketchgraps_list:\n",
    "            #print(test_graph_seq)\n",
    "\n",
    "            # convert first to pyGraphViz graph using sketchgraph's function\n",
    "            pgv_graph = sketchgraphs.data.sequence.pgvgraph_from_sequence(sg)\n",
    "            #print(pgv_graph)\n",
    "\n",
    "            # then to networkx graph\n",
    "            nx_graph = nx.Graph(pgv_graph)\n",
    "            #print(nx_graph)\n",
    "\n",
    "            # finally to pyTorch graph\n",
    "            graph = from_networkx(nx_graph)\n",
    "\n",
    "            # next we need to add required attributes: x, y, adj_t\n",
    "            graph.x = get_sketch_features(graph, 8)\n",
    "            graph.y = get_sketch_attr_y(graph)\n",
    "            graph.adj_t = get_sketch_adj(graph)\n",
    "            #print(graph.x[0])\n",
    "            graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "            if not hasattr(graph, 'edge_label'):\n",
    "                continue\n",
    "            #print(len(graph.edge_label))\n",
    "            self.data.append(graph)\n",
    "            \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\"\"\"def custom_collate(batch, b):\n",
    "    print(\"AAAAA\")\n",
    "    edge_label_batch = []\n",
    "    for b in batch:\n",
    "        #print(b.edge_label)\n",
    "        edge_label_batch.append(b.edge_label)\n",
    "    return edge_label_batch\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=819,\n",
      "  x=[819, 8],\n",
      "  y=[819, 1],\n",
      "  adj_t=[819, 819, nnz=1984],\n",
      "  edge_index=[2, 1984],\n",
      "  batch=[819],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=954,\n",
      "  x=[954, 8],\n",
      "  y=[954, 1],\n",
      "  adj_t=[954, 954, nnz=2401],\n",
      "  edge_index=[2, 2401],\n",
      "  batch=[954],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=853,\n",
      "  x=[853, 8],\n",
      "  y=[853, 1],\n",
      "  adj_t=[853, 853, nnz=2044],\n",
      "  edge_index=[2, 2044],\n",
      "  batch=[853],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=824,\n",
      "  x=[824, 8],\n",
      "  y=[824, 1],\n",
      "  adj_t=[824, 824, nnz=2055],\n",
      "  edge_index=[2, 2055],\n",
      "  batch=[824],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=869,\n",
      "  x=[869, 8],\n",
      "  y=[869, 1],\n",
      "  adj_t=[869, 869, nnz=2141],\n",
      "  edge_index=[2, 2141],\n",
      "  batch=[869],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=896,\n",
      "  x=[896, 8],\n",
      "  y=[896, 1],\n",
      "  adj_t=[896, 896, nnz=2229],\n",
      "  edge_index=[2, 2229],\n",
      "  batch=[896],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=847,\n",
      "  x=[847, 8],\n",
      "  y=[847, 1],\n",
      "  adj_t=[847, 847, nnz=2089],\n",
      "  edge_index=[2, 2089],\n",
      "  batch=[847],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=996,\n",
      "  x=[996, 8],\n",
      "  y=[996, 1],\n",
      "  adj_t=[996, 996, nnz=2545],\n",
      "  edge_index=[2, 2545],\n",
      "  batch=[996],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=955,\n",
      "  x=[955, 8],\n",
      "  y=[955, 1],\n",
      "  adj_t=[955, 955, nnz=2422],\n",
      "  edge_index=[2, 2422],\n",
      "  batch=[955],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=710,\n",
      "  x=[710, 8],\n",
      "  y=[710, 1],\n",
      "  adj_t=[710, 710, nnz=1739],\n",
      "  edge_index=[2, 1739],\n",
      "  batch=[710],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=779,\n",
      "  x=[779, 8],\n",
      "  y=[779, 1],\n",
      "  adj_t=[779, 779, nnz=1859],\n",
      "  edge_index=[2, 1859],\n",
      "  batch=[779],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=954,\n",
      "  x=[954, 8],\n",
      "  y=[954, 1],\n",
      "  adj_t=[954, 954, nnz=2410],\n",
      "  edge_index=[2, 2410],\n",
      "  batch=[954],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=928,\n",
      "  x=[928, 8],\n",
      "  y=[928, 1],\n",
      "  adj_t=[928, 928, nnz=2341],\n",
      "  edge_index=[2, 2341],\n",
      "  batch=[928],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=825,\n",
      "  x=[825, 8],\n",
      "  y=[825, 1],\n",
      "  adj_t=[825, 825, nnz=2019],\n",
      "  edge_index=[2, 2019],\n",
      "  batch=[825],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[64],\n",
      "  label=[64],\n",
      "  parameters=[64],\n",
      "  edge_label=[64],\n",
      "  edge_parameters=[64],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  edge={\n",
      "    index=[64],\n",
      "    label=[64],\n",
      "    parameters=[64]\n",
      "  },\n",
      "  num_nodes=828,\n",
      "  x=[828, 8],\n",
      "  y=[828, 1],\n",
      "  adj_t=[828, 828, nnz=2052],\n",
      "  edge_index=[2, 2052],\n",
      "  batch=[828],\n",
      "  ptr=[65]\n",
      ")\n",
      "DataBatch(\n",
      "  index=[61],\n",
      "  label=[61],\n",
      "  parameters=[61],\n",
      "  edge_label=[61],\n",
      "  edge_parameters=[61],\n",
      "  graph={},\n",
      "  node={\n",
      "    index=[61],\n",
      "    label=[61],\n",
      "    parameters=[61]\n",
      "  },\n",
      "  edge={\n",
      "    index=[61],\n",
      "    label=[61],\n",
      "    parameters=[61]\n",
      "  },\n",
      "  num_nodes=778,\n",
      "  x=[778, 8],\n",
      "  y=[778, 1],\n",
      "  adj_t=[778, 778, nnz=1907],\n",
      "  edge_index=[2, 1907],\n",
      "  batch=[778],\n",
      "  ptr=[62]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sketchgraphs\n",
    "import networkx as nx\n",
    "from sketchgraphs.data import flat_array\n",
    "\n",
    "dataset = SketchgraphDataset()\n",
    "    \n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for b in iter(data_loader):\n",
    "    print(b)\n",
    "#print(graph)\n",
    "#print(graph.x)\n",
    "#print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n",
      "Device: cuda\n",
      "tensor([     0,      1,      2,  ..., 169145, 169148, 169251])\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    dataset_name = 'ogbn-arxiv'\n",
    "    dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    #data = batch\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the adjacency matrix to symmetric\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    #print(data.y)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # If you use GPU, the device should be cuda\n",
    "    print('Device: {}'.format(device))\n",
    "    data = data.to(device)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    print(split_idx['train'])\n",
    "    #train_idx = torch.LongTensor(range(0, data.num_nodes)).to(device)\n",
    "    #print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        F.dropout.training = self.training\n",
    "        \n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = self.linear(x)\n",
    "        if self.return_embeds == False:\n",
    "            x = self.softmax(x)\n",
    "        out = x\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    for batch in iter(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o = model(batch.x, batch.edge_index)\n",
    "        # o = o[train_idx] # we train on the whole graphs now\n",
    "        loss = loss_fn(o, batch.y.squeeze(1))\n",
    "        #########################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, loader, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        batch = batch.to(device)\n",
    "        ############# Your code here ############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. No index slicing here\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        #########################################\n",
    "\n",
    "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        #if count == 1:\n",
    "            #print(y_pred, batch.y)\n",
    "        \n",
    "        train_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        valid_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        test_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "\n",
    "    train_acc /= count\n",
    "    valid_acc /= count\n",
    "    test_acc /= count\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "    }\n",
    "    args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    model = GCN(8, args['hidden_dim'],\n",
    "              dataset.num_classes, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.7319, Train: 27.70%, Valid: 27.70% Test: 27.70%\n",
      "Epoch: 02, Loss: 1.3135, Train: 30.91%, Valid: 30.91% Test: 30.91%\n",
      "Epoch: 03, Loss: 1.2082, Train: 35.77%, Valid: 35.77% Test: 35.77%\n",
      "Epoch: 04, Loss: 1.2980, Train: 34.62%, Valid: 34.62% Test: 34.62%\n",
      "Epoch: 05, Loss: 1.2521, Train: 42.05%, Valid: 42.05% Test: 42.05%\n",
      "Epoch: 06, Loss: 1.2406, Train: 45.48%, Valid: 45.48% Test: 45.48%\n",
      "Epoch: 07, Loss: 1.1285, Train: 41.85%, Valid: 41.85% Test: 41.85%\n",
      "Epoch: 08, Loss: 1.1418, Train: 43.57%, Valid: 43.57% Test: 43.57%\n",
      "Epoch: 09, Loss: 1.1611, Train: 41.11%, Valid: 41.11% Test: 41.11%\n",
      "Epoch: 10, Loss: 1.2368, Train: 45.30%, Valid: 45.30% Test: 45.30%\n",
      "Epoch: 11, Loss: 1.2377, Train: 46.82%, Valid: 46.82% Test: 46.82%\n",
      "Epoch: 12, Loss: 1.0983, Train: 39.12%, Valid: 39.12% Test: 39.12%\n",
      "Epoch: 13, Loss: 1.1505, Train: 41.93%, Valid: 41.93% Test: 41.93%\n",
      "Epoch: 14, Loss: 1.3311, Train: 44.37%, Valid: 44.37% Test: 44.37%\n",
      "Epoch: 15, Loss: 1.1408, Train: 39.28%, Valid: 39.28% Test: 39.28%\n",
      "Epoch: 16, Loss: 1.1062, Train: 46.79%, Valid: 46.79% Test: 46.79%\n",
      "Epoch: 17, Loss: 1.0664, Train: 46.56%, Valid: 46.56% Test: 46.56%\n",
      "Epoch: 18, Loss: 1.1027, Train: 45.60%, Valid: 45.60% Test: 45.60%\n",
      "Epoch: 19, Loss: 1.1495, Train: 40.80%, Valid: 40.80% Test: 40.80%\n",
      "Epoch: 20, Loss: 1.0429, Train: 40.34%, Valid: 40.34% Test: 40.34%\n",
      "Epoch: 21, Loss: 1.1151, Train: 42.85%, Valid: 42.85% Test: 42.85%\n",
      "Epoch: 22, Loss: 1.0681, Train: 50.00%, Valid: 50.00% Test: 50.00%\n",
      "Epoch: 23, Loss: 1.1319, Train: 44.70%, Valid: 44.70% Test: 44.70%\n",
      "Epoch: 24, Loss: 1.0651, Train: 47.65%, Valid: 47.65% Test: 47.65%\n",
      "Epoch: 25, Loss: 1.1600, Train: 50.51%, Valid: 50.51% Test: 50.51%\n",
      "Epoch: 26, Loss: 1.1375, Train: 49.23%, Valid: 49.23% Test: 49.23%\n",
      "Epoch: 27, Loss: 1.0177, Train: 48.66%, Valid: 48.66% Test: 48.66%\n",
      "Epoch: 28, Loss: 1.0075, Train: 42.96%, Valid: 42.96% Test: 42.96%\n",
      "Epoch: 29, Loss: 0.9420, Train: 46.79%, Valid: 46.79% Test: 46.79%\n",
      "Epoch: 30, Loss: 1.0357, Train: 53.16%, Valid: 53.16% Test: 53.16%\n",
      "Epoch: 31, Loss: 1.0632, Train: 47.68%, Valid: 47.68% Test: 47.68%\n",
      "Epoch: 32, Loss: 0.9727, Train: 47.08%, Valid: 47.08% Test: 47.08%\n",
      "Epoch: 33, Loss: 0.9486, Train: 48.48%, Valid: 48.48% Test: 48.48%\n",
      "Epoch: 34, Loss: 0.9706, Train: 48.33%, Valid: 48.33% Test: 48.33%\n",
      "Epoch: 35, Loss: 1.0057, Train: 53.79%, Valid: 53.79% Test: 53.79%\n",
      "Epoch: 36, Loss: 1.0769, Train: 46.35%, Valid: 46.35% Test: 46.35%\n",
      "Epoch: 37, Loss: 0.9418, Train: 49.10%, Valid: 49.10% Test: 49.10%\n",
      "Epoch: 38, Loss: 1.0008, Train: 49.89%, Valid: 49.89% Test: 49.89%\n",
      "Epoch: 39, Loss: 1.0400, Train: 50.79%, Valid: 50.79% Test: 50.79%\n",
      "Epoch: 40, Loss: 0.9911, Train: 46.21%, Valid: 46.21% Test: 46.21%\n",
      "Epoch: 41, Loss: 0.9888, Train: 45.44%, Valid: 45.44% Test: 45.44%\n",
      "Epoch: 42, Loss: 1.1225, Train: 46.84%, Valid: 46.84% Test: 46.84%\n",
      "Epoch: 43, Loss: 1.0319, Train: 49.32%, Valid: 49.32% Test: 49.32%\n",
      "Epoch: 44, Loss: 0.9245, Train: 54.35%, Valid: 54.35% Test: 54.35%\n",
      "Epoch: 45, Loss: 1.0230, Train: 54.85%, Valid: 54.85% Test: 54.85%\n",
      "Epoch: 46, Loss: 1.0064, Train: 49.07%, Valid: 49.07% Test: 49.07%\n",
      "Epoch: 47, Loss: 1.0558, Train: 49.44%, Valid: 49.44% Test: 49.44%\n",
      "Epoch: 48, Loss: 1.0013, Train: 55.72%, Valid: 55.72% Test: 55.72%\n",
      "Epoch: 49, Loss: 0.9409, Train: 53.57%, Valid: 53.57% Test: 53.57%\n",
      "Epoch: 50, Loss: 0.9788, Train: 51.87%, Valid: 51.87% Test: 51.87%\n",
      "Epoch: 51, Loss: 1.0300, Train: 52.25%, Valid: 52.25% Test: 52.25%\n",
      "Epoch: 52, Loss: 0.9904, Train: 48.95%, Valid: 48.95% Test: 48.95%\n",
      "Epoch: 53, Loss: 0.9638, Train: 55.03%, Valid: 55.03% Test: 55.03%\n",
      "Epoch: 54, Loss: 0.9186, Train: 52.62%, Valid: 52.62% Test: 52.62%\n",
      "Epoch: 55, Loss: 0.9809, Train: 47.43%, Valid: 47.43% Test: 47.43%\n",
      "Epoch: 56, Loss: 0.9668, Train: 47.43%, Valid: 47.43% Test: 47.43%\n",
      "Epoch: 57, Loss: 0.9100, Train: 54.25%, Valid: 54.25% Test: 54.25%\n",
      "Epoch: 58, Loss: 0.8740, Train: 54.68%, Valid: 54.68% Test: 54.68%\n",
      "Epoch: 59, Loss: 0.9699, Train: 55.03%, Valid: 55.03% Test: 55.03%\n",
      "Epoch: 60, Loss: 1.0281, Train: 54.90%, Valid: 54.90% Test: 54.90%\n",
      "Epoch: 61, Loss: 0.9413, Train: 50.93%, Valid: 50.93% Test: 50.93%\n",
      "Epoch: 62, Loss: 1.0266, Train: 49.68%, Valid: 49.68% Test: 49.68%\n",
      "Epoch: 63, Loss: 1.0233, Train: 52.15%, Valid: 52.15% Test: 52.15%\n",
      "Epoch: 64, Loss: 1.0282, Train: 52.54%, Valid: 52.54% Test: 52.54%\n",
      "Epoch: 65, Loss: 0.9616, Train: 55.66%, Valid: 55.66% Test: 55.66%\n",
      "Epoch: 66, Loss: 1.0017, Train: 53.52%, Valid: 53.52% Test: 53.52%\n",
      "Epoch: 67, Loss: 1.0130, Train: 49.33%, Valid: 49.33% Test: 49.33%\n",
      "Epoch: 68, Loss: 0.9264, Train: 51.04%, Valid: 51.04% Test: 51.04%\n",
      "Epoch: 69, Loss: 0.9005, Train: 50.68%, Valid: 50.68% Test: 50.68%\n",
      "Epoch: 70, Loss: 1.0079, Train: 52.18%, Valid: 52.18% Test: 52.18%\n",
      "Epoch: 71, Loss: 0.9559, Train: 54.30%, Valid: 54.30% Test: 54.30%\n",
      "Epoch: 72, Loss: 0.9825, Train: 56.65%, Valid: 56.65% Test: 56.65%\n",
      "Epoch: 73, Loss: 0.9672, Train: 53.89%, Valid: 53.89% Test: 53.89%\n",
      "Epoch: 74, Loss: 0.8982, Train: 53.87%, Valid: 53.87% Test: 53.87%\n",
      "Epoch: 75, Loss: 0.9715, Train: 48.40%, Valid: 48.40% Test: 48.40%\n",
      "Epoch: 76, Loss: 0.9673, Train: 54.49%, Valid: 54.49% Test: 54.49%\n",
      "Epoch: 77, Loss: 1.0502, Train: 54.89%, Valid: 54.89% Test: 54.89%\n",
      "Epoch: 78, Loss: 1.0455, Train: 57.74%, Valid: 57.74% Test: 57.74%\n",
      "Epoch: 79, Loss: 0.8926, Train: 53.00%, Valid: 53.00% Test: 53.00%\n",
      "Epoch: 80, Loss: 1.1572, Train: 54.47%, Valid: 54.47% Test: 54.47%\n",
      "Epoch: 81, Loss: 0.9505, Train: 58.24%, Valid: 58.24% Test: 58.24%\n",
      "Epoch: 82, Loss: 0.9665, Train: 49.73%, Valid: 49.73% Test: 49.73%\n",
      "Epoch: 83, Loss: 0.9055, Train: 51.83%, Valid: 51.83% Test: 51.83%\n",
      "Epoch: 84, Loss: 0.8335, Train: 57.21%, Valid: 57.21% Test: 57.21%\n",
      "Epoch: 85, Loss: 0.9600, Train: 49.68%, Valid: 49.68% Test: 49.68%\n",
      "Epoch: 86, Loss: 0.8434, Train: 52.75%, Valid: 52.75% Test: 52.75%\n",
      "Epoch: 87, Loss: 0.9914, Train: 54.07%, Valid: 54.07% Test: 54.07%\n",
      "Epoch: 88, Loss: 0.9371, Train: 55.92%, Valid: 55.92% Test: 55.92%\n",
      "Epoch: 89, Loss: 0.8556, Train: 55.52%, Valid: 55.52% Test: 55.52%\n",
      "Epoch: 90, Loss: 0.9172, Train: 50.33%, Valid: 50.33% Test: 50.33%\n",
      "Epoch: 91, Loss: 0.8539, Train: 55.56%, Valid: 55.56% Test: 55.56%\n",
      "Epoch: 92, Loss: 0.9366, Train: 53.61%, Valid: 53.61% Test: 53.61%\n",
      "Epoch: 93, Loss: 0.9712, Train: 58.25%, Valid: 58.25% Test: 58.25%\n",
      "Epoch: 94, Loss: 0.8774, Train: 56.67%, Valid: 56.67% Test: 56.67%\n",
      "Epoch: 95, Loss: 1.0825, Train: 54.83%, Valid: 54.83% Test: 54.83%\n",
      "Epoch: 96, Loss: 0.9411, Train: 56.50%, Valid: 56.50% Test: 56.50%\n",
      "Epoch: 97, Loss: 0.9400, Train: 50.92%, Valid: 50.92% Test: 50.92%\n",
      "Epoch: 98, Loss: 0.9805, Train: 57.18%, Valid: 57.18% Test: 57.18%\n",
      "Epoch: 99, Loss: 1.0221, Train: 57.64%, Valid: 57.64% Test: 57.64%\n",
      "Epoch: 100, Loss: 0.8850, Train: 53.91%, Valid: 53.91% Test: 53.91%\n"
     ]
    }
   ],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    # reset the parameters to initial random value\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    loss_fn = F.nll_loss\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "        loss = train(model, data_loader, [], optimizer, loss_fn)\n",
    "        result = test(model, data_loader, [], evaluator)\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_acc:.2f}%, '\n",
    "              f'Valid: {100 * valid_acc:.2f}% '\n",
    "              f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(model, data_loader, [], evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
