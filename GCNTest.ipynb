{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the modified to_networkx function that doesn't throw exceptions\n",
    "\n",
    "def from_networkx(\n",
    "    G: Any,\n",
    "    group_node_attrs: Optional[Union[List[str], all]] = None,\n",
    "    group_edge_attrs: Optional[Union[List[str], all]] = None,\n",
    ") -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "        group_node_attrs (List[str] or all, optional): The node attributes to\n",
    "            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)\n",
    "        group_edge_attrs (List[str] or all, optional): The edge attributes to\n",
    "            be concatenated and added to :obj:`data.edge_attr`.\n",
    "            (default: :obj:`None`)\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must\n",
    "        be numeric.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> edge_index = torch.tensor([\n",
    "        ...     [0, 1, 1, 2, 2, 3],\n",
    "        ...     [1, 0, 2, 1, 3, 2],\n",
    "        ... ])\n",
    "        >>> data = Data(edge_index=edge_index, num_nodes=4)\n",
    "        >>> g = to_networkx(data)\n",
    "        >>> # A `Data` object is returned\n",
    "        >>> from_networkx(g)\n",
    "        Data(edge_index=[2, 6], num_nodes=4)\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        edges = list(G.edges(keys=False))\n",
    "    else:\n",
    "        edges = list(G.edges)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    if G.number_of_nodes() > 0:\n",
    "        node_attrs = list(next(iter(G.nodes(data=True)))[-1].keys())\n",
    "    else:\n",
    "        node_attrs = {}\n",
    "\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_attrs = list(next(iter(G.edges(data=True)))[-1].keys())\n",
    "    else:\n",
    "        edge_attrs = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        if set(feat_dict.keys()) != set(node_attrs):\n",
    "            raise ValueError('Not all nodes contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        if set(feat_dict.keys()) != set(edge_attrs):\n",
    "            raise ValueError('Not all edges contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for key, value in G.graph.items():\n",
    "        key = f'graph_{key}' if key in node_attrs else key\n",
    "        data[str(key)] = value\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (tuple, list)) and isinstance(value[0], Tensor):\n",
    "            data[key] = torch.stack(value, dim=0)\n",
    "        else:\n",
    "            try:\n",
    "                data[key] = torch.tensor(value)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = Data.from_dict(data)\n",
    "\n",
    "    if group_node_attrs is all:\n",
    "        group_node_attrs = list(node_attrs)\n",
    "    if group_node_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_node_attrs:\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if group_edge_attrs is all:\n",
    "        group_edge_attrs = list(edge_attrs)\n",
    "    if group_edge_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_edge_attrs:\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.edge_attr = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if data.x is None and data.pos is None:\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "def to_edge_index(adj: Union[Tensor, SparseTensor]) -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Converts a :class:`torch.sparse.Tensor` or a\n",
    "    :class:`torch_sparse.SparseTensor` to edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.sparse.Tensor or SparseTensor): The adjacency matrix.\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "        ...                            [1, 0, 2, 1, 3, 2]])\n",
    "        >>> adj = to_torch_coo_tensor(edge_index)\n",
    "        >>> to_edge_index(adj)\n",
    "        (tensor([[0, 1, 1, 2, 2, 3],\n",
    "                [1, 0, 2, 1, 3, 2]]),\n",
    "        tensor([1., 1., 1., 1., 1., 1.]))\n",
    "    \"\"\"\n",
    "    if isinstance(adj, SparseTensor):\n",
    "        row, col, value = adj.coo()\n",
    "        if value is None:\n",
    "            value = torch.ones(row.size(0), device=row.device)\n",
    "        return torch.stack([row, col], dim=0), value\n",
    "\n",
    "    if adj.requires_grad:\n",
    "        # Calling adj._values() will return a detached tensor.\n",
    "        # Use `adj.coalesce().values()` instead to track gradients.\n",
    "        adj = adj.coalesce()\n",
    "        return adj.indices(), adj.values()\n",
    "\n",
    "    return adj._indices(), adj._values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "        \"Point\": 0,\n",
    "        \"Line\": 1,\n",
    "        \"Circle\": 2,\n",
    "        \"Ellipse\": 3,\n",
    "        \"Spline\": 4,\n",
    "        \"Conic\": 5,\n",
    "        \"Arc\": 6,\n",
    "        \"External\": 7,\n",
    "        \"Stop\": 8,\n",
    "        \"Unknown\": 9,\n",
    "        \"SN_Start\": 10,\n",
    "        \"SN_End\": 11,\n",
    "        \"SN_Center\": 12\n",
    "    }\n",
    "\n",
    "edge_dict = {\n",
    "    \"Coincident\": 0,\n",
    "    \"Projected\": 1,\n",
    "    \"Mirror\": 2,\n",
    "    \"Distance\": 3,\n",
    "    \"Horizontal\": 4,\n",
    "    \"Parallel\": 5,\n",
    "    \"Vertical\": 6,\n",
    "    \"Tangent\": 7,\n",
    "    \"Length\": 8,\n",
    "    \"Perpendicular\": 9,\n",
    "    \"Midpoint\": 10,\n",
    "    \"Equal\": 11,\n",
    "    \"Diameter\": 12,\n",
    "    \"Offset\": 13,\n",
    "    \"Radius\": 14,\n",
    "    \"Concentric\": 15,\n",
    "    \"Fix\": 16,\n",
    "    \"Angle\": 17,\n",
    "    \"Circular_Pattern\": 18,\n",
    "    \"Pierce\": 19,\n",
    "    \"Linear_Pattern\": 20,\n",
    "    \"Centerline_Dimension\": 21,\n",
    "    \"Intersected\": 22,\n",
    "    \"Silhoutted\": 23,\n",
    "    \"Quadrant\": 24,\n",
    "    \"Normal\": 25,\n",
    "    \"Minor_Diameter\": 26,\n",
    "    \"Major_Diameter\": 27,\n",
    "    \"Rho\": 28,\n",
    "    \"Unknown\": 29,\n",
    "    \"Subnode\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "def get_sketch_features(graph, feature_dim):\n",
    "    x = torch.zeros([graph.num_nodes, feature_dim])\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx, p in enumerate(graph.parameters):\n",
    "        \n",
    "        # add one hot encoding to feature vector for node label\n",
    "        #onePos = label_dict[graph.label[idx]]/7\n",
    "        #for i in range(0, 14):\n",
    "        #    x[idx, i] = 1 if onePos==i else 0\n",
    "        x[idx, label_dict[graph.label[idx]]] = 1\n",
    "        # convert label text into a feature value\n",
    "        #x[idx, 14] = label_dict[graph.label[idx]]/7\n",
    "        \n",
    "        param_dict = json.loads(p)\n",
    "        for i, k in enumerate(param_dict.keys()):\n",
    "            \n",
    "            if i+2 == feature_dim:\n",
    "                break\n",
    "            \n",
    "            # convert each parameter value into a feature value\n",
    "            x[idx, i+15] = float(param_dict[k])\n",
    "        \n",
    "        x[idx, -1] = degree(graph.edge_index[0], graph.num_nodes)[idx]\n",
    "        #print(idx, p)\n",
    "        #print(x[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_attr_y(graph):\n",
    "    y = torch.zeros([graph.num_nodes, 1], dtype=torch.int64)\n",
    "    #rint(graph.label)\n",
    "    \n",
    "    \n",
    "    for i, l in enumerate(graph.label):\n",
    "        y[i, 0] = label_dict[l]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_adj(graph):\n",
    "    tst = T.ToSparseTensor()\n",
    "    return tst(graph).adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_large_graph(graphs):\n",
    "    bestN = 0\n",
    "    bestI = 0\n",
    "    for i, g in enumerate(graphs):\n",
    "        if len(g) > bestN:\n",
    "            bestN = len(g)\n",
    "            bestI = i\n",
    "    #print(bestI)\n",
    "    return graphs[bestI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_edge_attr(graph):\n",
    "    dim = 31\n",
    "    edge_attr = torch.zeros([len(graph.edge_label), dim])\n",
    "    for idx, l in enumerate(graph.edge_label):\n",
    "        edge_attr[idx, edge_dict[l]] = 1\n",
    "    return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for now we will do node class sequence prediction\n",
    "\n",
    "def get_sketch_construction_node_sequence(sg, graph):\n",
    "    edge_sequence_indices = []\n",
    "    node_sequence_indices = []\n",
    "    node_sequence = torch.zeros((0, 15), dtype=torch.float) # 15th element is stop sign\n",
    "    edge_sequence = torch.zeros((0, 32), dtype=torch.float) # 32nd element is stop sign\n",
    "    start_node_idx=0\n",
    "    start_edge_idx=0\n",
    "    node_idx = 0\n",
    "    edge_idx=0\n",
    "    is_edge_sequence = False\n",
    "    for elem in sg:\n",
    "        \n",
    "        if node_idx == len(graph.label):\n",
    "            break\n",
    "            \n",
    "        if type(elem) == sketchgraphs.data.sequence.NodeOp:\n",
    "            if is_edge_sequence:\n",
    "                is_edge_sequence=False\n",
    "                stop_token = torch.zeros((1, 32))\n",
    "                stop_token[0, 31] = 1\n",
    "                edge_sequence = torch.cat((edge_sequence, stop_token))\n",
    "                edge_sequence_indices.append((start_edge_idx, edge_idx))\n",
    "                start_node_idx=node_idx\n",
    "                edge_idx+=1\n",
    "            one_hot = torch.zeros((1, 15))\n",
    "            one_hot[0, label_dict[graph.label[node_idx]]] = 1\n",
    "            node_sequence = torch.cat((node_sequence, one_hot))\n",
    "            node_idx+=1\n",
    "        elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "            #print(elem)\n",
    "            if not is_edge_sequence:\n",
    "                is_edge_sequence=True\n",
    "                start_edge_idx=edge_idx\n",
    "                stop_token = torch.zeros((1, 15))\n",
    "                stop_token[0, 14] = 1\n",
    "                node_sequence = torch.cat((node_sequence, stop_token))\n",
    "                node_sequence_indices.append((start_node_idx, node_idx))\n",
    "                node_idx+=1\n",
    "            constraintNumber = edge_dict[graph.edge_label[edge_idx]]\n",
    "            one_hot = torch.zeros((1, 32))\n",
    "            one_hot[0, constraintNumber] = 1\n",
    "            edge_sequence = torch.cat((edge_sequence, one_hot))\n",
    "            edge_idx+=1\n",
    "            \n",
    "    #node_sequence[-1, 14] = 1\n",
    "    #edge_sequence[-1, 31] = 1\n",
    "    #print(node_sequence_indices, edge_sequence_indices)\n",
    "    return node_sequence, edge_sequence, node_sequence_indices, edge_sequence_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sketchgraph_node_constraint_sequence(sg, graph):\n",
    "    #y = torch.zeros((len(sg)-1, 2+31)) # 2 classes (node or edge), 14 node types (but contained in 31 positions for edge types), 31 edge types\n",
    "    #node_idx=0\n",
    "    #edge_idx=0\n",
    "    #for idx, elem in enumerate(sg):\n",
    "    #    if type(elem) == sketchgraphs.data.sequence.NodeOp and not elem.label == sketchgraphs.data._entity.EntityType.Stop:\n",
    "    #        y[idx, 0] = 1\n",
    "    #        y[idx, 2+label_dict[graph.label[node_idx]]] = 1  # 0.5 because we're marking 2 spaces in a vector of zeroes, to sum up to 1\n",
    "    #        node_idx+=1\n",
    "    #    elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "    #        y[idx, 1] = 1\n",
    "    #        y[idx, 2+edge_dict[graph.edge_label[edge_idx]]] = 1\n",
    "    #        edge_idx+=1\n",
    "    #return y\n",
    "    y = torch.zeros((len(sg)-1), dtype=torch.long)\n",
    "    node_idx=0\n",
    "    edge_idx=0\n",
    "    for idx, elem in enumerate(sg):\n",
    "        if type(elem) == sketchgraphs.data.sequence.NodeOp and not elem.label == sketchgraphs.data._entity.EntityType.Stop:\n",
    "            y[idx] = label_dict[graph.label[node_idx]]  # 0.5 because we're marking 2 spaces in a vector of zeroes, to sum up to 1\n",
    "            node_idx+=1\n",
    "        elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "            y[idx] = 15+edge_dict[graph.edge_label[edge_idx]]\n",
    "            edge_idx+=1\n",
    "    return y\n",
    "\n",
    "\n",
    "def convert_sketchgraph_to_pytorch(sg):\n",
    "    # convert first to pyGraphViz graph using sketchgraph's function\n",
    "    pgv_graph = sketchgraphs.data.sequence.pgvgraph_from_sequence(sg)\n",
    "    # then to networkx graph\n",
    "    nx_graph = nx.Graph(pgv_graph)\n",
    "    # finally to pyTorch graph\n",
    "    graph = from_networkx(nx_graph)\n",
    "    return graph\n",
    "\n",
    "def assign_attributes_to_graph(graph):\n",
    "    graph.x = get_sketch_features(graph, 30)\n",
    "    graph.y = get_sketch_attr_y(graph)\n",
    "    graph.adj_t = get_sketch_adj(graph)\n",
    "    graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "    graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "    return graph\n",
    "\n",
    "def get_sketchgraph_graph_sequence(sg):\n",
    "    generated_graph = []\n",
    "    sequence = []\n",
    "    for elem in sg:\n",
    "        generated_graph.append(elem)\n",
    "        new_graph = convert_sketchgraph_to_pytorch(generated_graph)\n",
    "        if not hasattr(new_graph, 'edge_label'):\n",
    "            new_graph.edge_label = []\n",
    "        new_graph = assign_attributes_to_graph(new_graph)\n",
    "        sequence.append(new_graph)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_sequences_into_length_groups(sg_list, factor=3):\n",
    "    lengths_dict = {}\n",
    "    for sg in sg_list:\n",
    "        length = len(sg)\n",
    "        key = str(length//3)\n",
    "        if key in lengths_dict.keys():\n",
    "            lengths_dict[key].append(sg)\n",
    "        else:\n",
    "            lengths_dict[key] = [sg]\n",
    "    return lengths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def custom_collate(batch, b):\\n    print(\"AAAAA\")\\n    edge_label_batch = []\\n    for b in batch:\\n        #print(b.edge_label)\\n        edge_label_batch.append(b.edge_label)\\n    return edge_label_batch'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom dataset class of custom attributes\n",
    "from torch_geometric.data import Dataset\n",
    "class SketchgraphDataset(Dataset):\n",
    "    def __init__(self, start_idx, end_idx, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data = []\n",
    "        seq_data = flat_array.load_dictionary_flat('datasets/sg_t16_validation.npy')\n",
    "        print(len(seq_data['sequences']))\n",
    "\n",
    "        #test_graph_seq = find_large_graph(seq_data['sequences'])\n",
    "        test_graph_seq = seq_data['sequences'][206778]\n",
    "        test_graph_seq1 = seq_data['sequences'][10]\n",
    "\n",
    "        self.sketchgraps_list = seq_data['sequences'][start_idx:end_idx]\n",
    "\n",
    "        #link_transform = T.RandomLinkSplit(is_undirected=True, key=\"edge_attr\")\n",
    "        \n",
    "        sequence_length_dict = sort_sequences_into_length_groups(self.sketchgraps_list)\n",
    "        for k in sequence_length_dict:\n",
    "            print(k, len(sequence_length_dict[k]))\n",
    "        \n",
    "        for k in sequence_length_dict:\n",
    "            for sg in sequence_length_dict[k]:\n",
    "                #print(test_graph_seq)\n",
    "                graph = convert_sketchgraph_to_pytorch(sg)\n",
    "\n",
    "                if not hasattr(graph, 'edge_label'):\n",
    "                    continue\n",
    "\n",
    "                # next we need to add required attributes: x, y, adj_t\n",
    "                graph.x = get_sketch_features(graph, 30)\n",
    "                graph.y = get_sketch_attr_y(graph)\n",
    "                graph.adj_t = get_sketch_adj(graph)\n",
    "                graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "                graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "                graph.seq = get_sketchgraph_graph_sequence(sg)\n",
    "                graph.seq_y = get_sketchgraph_node_constraint_sequence(sg, graph)\n",
    "                #graph.node_sequence, graph.edge_sequence, graph.node_sequence_indices, graph.edge_sequence_indices = get_sketch_construction_node_sequence(sg, graph)\n",
    "                #print(len(graph.edge_label))\n",
    "                #graph = link_transform(graph)\n",
    "                self.data.append(graph)\n",
    "            \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\"\"\"def custom_collate(batch, b):\n",
    "    print(\"AAAAA\")\n",
    "    edge_label_batch = []\n",
    "    for b in batch:\n",
    "        #print(b.edge_label)\n",
    "        edge_label_batch.append(b.edge_label)\n",
    "    return edge_label_batch\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "3 20\n",
      "2 51\n",
      "6 3\n",
      "10 25\n",
      "8 6\n",
      "17 5\n",
      "1 2\n",
      "18 2\n",
      "7 4\n",
      "4 18\n",
      "20 5\n",
      "27 2\n",
      "12 4\n",
      "14 3\n",
      "13 2\n",
      "32 1\n",
      "15 3\n",
      "23 1\n",
      "11 13\n",
      "16 2\n",
      "42 1\n",
      "35 1\n",
      "21 2\n",
      "19 1\n",
      "28 1\n",
      "38 1\n",
      "33 2\n",
      "9 5\n",
      "29 3\n",
      "34 1\n",
      "5 5\n",
      "22 1\n",
      "31 2\n",
      "25 1\n",
      "39 1\n",
      "315228\n",
      "2 2\n",
      "5 1\n",
      "8 1\n",
      "11 1\n",
      "10 2\n",
      "3 1\n",
      "4 1\n",
      "14 1\n"
     ]
    }
   ],
   "source": [
    "import sketchgraphs\n",
    "import networkx as nx\n",
    "from sketchgraphs.data import flat_array\n",
    "\n",
    "train_dataset = SketchgraphDataset(0, 200)\n",
    "test_dataset = SketchgraphDataset(101, 111)\n",
    "    \n",
    "#data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#next(iter(data_loader)).seq_y\n",
    "#for b in iter(data_loader):\n",
    "#    print(b)\n",
    "#print(graph)\n",
    "#print(graph.x)\n",
    "#print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "10 31\n",
      "6 6\n",
      "2 49\n",
      "4 21\n",
      "7 6\n",
      "14 5\n",
      "8 1\n",
      "16 3\n",
      "11 12\n",
      "3 10\n",
      "17 7\n",
      "24 3\n",
      "29 4\n",
      "19 3\n",
      "13 4\n",
      "12 11\n",
      "9 6\n",
      "1 1\n",
      "5 3\n",
      "36 1\n",
      "30 3\n",
      "27 1\n",
      "21 1\n",
      "20 4\n",
      "33 2\n",
      "28 1\n"
     ]
    }
   ],
   "source": [
    "#print(next(iter(data_loader)).x)\n",
    "val_dataset = SketchgraphDataset(501, 700)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    #dataset_name = 'ogbn-arxiv'\n",
    "    #dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "    #                              transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    #print(data)\n",
    "    #data = batch\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the adjacency matrix to symmetric\n",
    "    #data.adj_t = data.adj_t.to_symmetric()\n",
    "    #print(data.y)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #device = 'cpu'\n",
    "    # If you use GPU, the device should be cuda\n",
    "    print('Device: {}'.format(device))\n",
    "    #data = data.to(device)\n",
    "    #split_idx = dataset.get_idx_split()\n",
    "    #print(split_idx['train'])\n",
    "    #train_idx = torch.LongTensor(range(0, data.num_nodes)).to(device)\n",
    "    #print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class nnconvnn(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(nnconvnn, self).__init__()\n",
    "        \n",
    "        self.simpleLin = torch.nn.Linear(31, input_dim*output_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.simpleLin.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.simpleLin(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class newrnnmodel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim_second, num_nnconv_layers):\n",
    "        super(newrnnmodel, self).__init__()\n",
    "        self.layer_count = 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.input_gcn = torch.nn.ModuleList()\n",
    "        self.input_gcn.append(NNConv(30, hidden_size, nnconvnn(30, hidden_size)))\n",
    "        #self.bns = torch.nn.ModuleList()\n",
    "        for i in range(num_nnconv_layers-1):\n",
    "            self.input_gcn.append(NNConv(hidden_size, hidden_size, nnconvnn(hidden_size, hidden_size)))\n",
    "            \n",
    "        #for i in range(num_nnconv_layers-1):\n",
    "        #    self.bns.append(torch.nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "        self.rnn_class = torch.nn.GRU(hidden_size, hidden_size, self.layer_count) # batch first = True means the first dimension is batch size\n",
    "\n",
    "        self.mlp_pre_rnn = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp_class = torch.nn.Linear(hidden_size, output_dim_second)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.rnn_class.reset_parameters()\n",
    "        self.mlp_class.reset_parameters()\n",
    "        for l in range(len(self.input_gcn)):\n",
    "            self.input_gcn[l].reset_parameters()\n",
    "        \n",
    "    def forward(self, graph_sequence):\n",
    "\n",
    "        hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float).to(device)\n",
    "        F.dropout.training = self.training\n",
    "        output = torch.tensor([]).to(device)\n",
    "        for g in graph_sequence:\n",
    "            x = g.x\n",
    "            for l in range(len(self.input_gcn)):\n",
    "                x = self.input_gcn[l](x, g.edge_index, g.edge_attr)\n",
    "                x = F.relu(x)\n",
    "                #x = F.dropout(x, p=0.2)\n",
    "            x = global_mean_pool(x, torch.zeros((g.x.size(0)), dtype=torch.long).to(device))\n",
    "            \n",
    "            x = self.mlp_pre_rnn(x)\n",
    "            \n",
    "            x, hidden = self.rnn_class(x, hidden)\n",
    "            \n",
    "            x = self.mlp_class(x)\n",
    "        \n",
    "            x = self.softmax(x)\n",
    "            \n",
    "            output = torch.cat((output, x))\n",
    "        return output, hidden.detach()\n",
    "        \n",
    "    def predict_next(self, sequence):\n",
    "        pred, hidden = self.forward(sequence)\n",
    "        return torch.argmax(pred[-1], dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        #hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        #return hidden\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.231689710712912 Acc: 35.2884125000904%\n",
      "2.0253269609494424 Acc: 40.142731095190044%\n",
      "1.8841081042984622 Acc: 42.45314891560593%\n",
      "1.7687761172577365 Acc: 44.04408497588953%\n",
      "1.7298558495152536 Acc: 44.34872059901973%\n",
      "1.8115013969603495 Acc: 38.90609964460413%\n",
      "1.8309259643806286 Acc: 38.066073825903516%\n",
      "1.801803547084032 Acc: 39.03030946743408%\n",
      "1.7953264030379865 Acc: 39.21494262160241%\n",
      "1.786593852330692 Acc: 39.484401102520486%\n",
      "1.7842960314235496 Acc: 39.41701873614314%\n",
      "1.7863161322759025 Acc: 39.33649595504855%\n",
      "1.7824402426055928 Acc: 39.46724058730071%\n",
      "1.7928328275980063 Acc: 39.40741582245082%\n",
      "1.7813280194848027 Acc: 39.57435289223545%\n",
      "1.7753478722955713 Acc: 39.385073160242165%\n",
      "1.805898783943761 Acc: 38.01503560083463%\n",
      "1.8455440012653868 Acc: 37.32089943934075%\n",
      "1.9088273422801914 Acc: 36.41706502035116%\n",
      "1.85615700063993 Acc: 38.535286852928095%\n"
     ]
    }
   ],
   "source": [
    "rnn = newrnnmodel(15, 32, 31+15, 4).to(device)\n",
    "\n",
    "optimizer_class = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
    "loss_fn = F.nll_loss\n",
    "#loss_fn = F.mse_loss\n",
    "epochs = 20\n",
    "rnn.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    acc = 0\n",
    "    for batch in train_dataset.data:\n",
    "        #print(batch)\n",
    "        count+=1\n",
    "        batch.to(device)\n",
    "\n",
    "        optimizer_class.zero_grad()\n",
    "        pred_class, _ = rnn.forward(batch.seq)\n",
    "        #print(pred_type)\n",
    "        #print(pred_class)\n",
    "        #print(batch.seq_y.size())\n",
    "        loss_class = loss_fn(pred_class[:-1], batch.seq_y)\n",
    "        pred_index = torch.argmax(pred_class[:-1], dim=1)\n",
    "        acc += torch.sum(pred_index==batch.seq_y).item() / len(batch.seq_y)\n",
    "        loss_class.backward()\n",
    "        \n",
    "        loss_avg += loss_class.item()\n",
    "        \n",
    "\n",
    "        optimizer_class.step()\n",
    "        \n",
    "        \n",
    "    print(loss_avg/count, \"Acc: \" + str(acc/count * 100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0192691274363586 Acc: 70.67008369282885%\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "acc = 0\n",
    "count = 0\n",
    "loss_avg = 0\n",
    "for batch in val_dataset:\n",
    "    count+=1\n",
    "    batch.to(device)\n",
    "    pred_class,_ = rnn.forward(batch.seq)\n",
    "    loss_class = loss_fn(pred_class[:-1], batch.seq_y)\n",
    "    pred_index = torch.argmax(pred_class[:-1], dim=1)\n",
    "    acc += torch.sum(pred_index==batch.seq_y).item() / len(batch.seq_y)\n",
    "    #print(loss_class.item())\n",
    "    #print(F.softmax(pred_class, dim=-1)[:-1])\n",
    "    #print(batch.seq_y)\n",
    "    loss_avg += loss_class.item()\n",
    "print(loss_avg/count, \"Acc: \" + str(acc/count * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "10 4\n",
      "11 3\n",
      "3 1\n",
      "20 1\n",
      "2 3\n",
      "24 1\n",
      "31 1\n",
      "4 1\n",
      "SN_End\n",
      "tensor(11, device='cuda:0')\n",
      "SN_End\n",
      "tensor(11, device='cuda:0')\n",
      "SN_End\n",
      "tensor(11, device='cuda:0')\n",
      "Line\n",
      "tensor(6, device='cuda:0')\n",
      "SN_End\n",
      "tensor(11, device='cuda:0')\n",
      "Line\n",
      "tensor(1, device='cuda:0')\n",
      "Line\n",
      "tensor(1, device='cuda:0')\n",
      "SN_End\n",
      "tensor(11, device='cuda:0')\n",
      "SN_End\n",
      "tensor(11, device='cuda:0')\n",
      "Circle\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for dimension 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-3487d5a81952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mend_index\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mnode_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for dimension 0 with size 6"
     ]
    }
   ],
   "source": [
    "# testing on 3 example graphs\n",
    "example_graphs = SketchgraphDataset(2015, 2030)\n",
    "\n",
    "for g in example_graphs:\n",
    "    g.to(device)\n",
    "    node_predicted = False\n",
    "    end_index = 6\n",
    "    while not node_predicted and end_index < 100:\n",
    "        pred = rnn.predict_next(g.seq[0:end_index])\n",
    "        index = pred.item()\n",
    "        if index <= 14:\n",
    "            print(list(label_dict.keys())[index])\n",
    "            print(g.seq_y[end_index-1])\n",
    "            node_predicted = True\n",
    "        else:\n",
    "            #print(index)\n",
    "            #print(list(edge_dict.keys())[index-15])\n",
    "            end_index+=1\n",
    "    #print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class linkPredictionGAE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(linkPredictionGAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.conv1 = NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim))\n",
    "        self.conv2 = NNConv(hidden_dim, output_dim, nnconvnn(hidden_dim, output_dim))\n",
    "        \n",
    "    def encode(self, x, pos_edge_index, edge_attr):\n",
    "        x = self.conv1(x, pos_edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, pos_edge_index, edge_attr)\n",
    "    \n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "        return logits\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1\n",
    "    return link_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_link_predictor(model):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "    loss_avg = 0\n",
    "    batch_count = 0\n",
    "    for batch in iter(data_loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_count+=1\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index,\n",
    "            num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z = model.encode(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        link_logits = model.decode(z, batch.edge_index, neg_edge_index)\n",
    "        link_labels = get_link_labels(batch.edge_index, neg_edge_index)\n",
    "        loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "    return loss_avg / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    perf=0\n",
    "    batch_count = 0\n",
    "    for batch in iter(test_loader):\n",
    "        batch = batch.to(device)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index,\n",
    "            num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        z = model.encode(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        link_logits = model.decode(z, batch.edge_index, neg_edge_index)\n",
    "        link_probs = link_logits.sigmoid()\n",
    "        link_labels = get_link_labels(batch.edge_index, neg_edge_index)\n",
    "        try:\n",
    "            perf+=roc_auc_score(link_labels.cpu().detach().numpy(), link_probs.cpu().detach().numpy())\n",
    "            batch_count+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return perf/batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training link prediction model\n",
    "\n",
    "model = linkPredictionGAE(30, 128, 64).to(device)\n",
    "\n",
    "epochs = 5000\n",
    "for e in range(epochs):\n",
    "    train_loss = train_link_predictor(model)\n",
    "    perf = test(model)\n",
    "    print(train_loss, perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testGraph = next(iter(test_loader)).to(device)\n",
    "z = model.encode(testGraph.x, testGraph.edge_index, testGraph.edge_attr)\n",
    "final_edge_index = model.decode_all(z)\n",
    "print(final_edge_index)\n",
    "print(testGraph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnnModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-252-a7a8cc37c4ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# testing RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnodeModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnnModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0medgeModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnnModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnode_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodeModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0medge_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medgeModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnnModel' is not defined"
     ]
    }
   ],
   "source": [
    "# testing RNN\n",
    "nodeModel = rnnModel(15, 8, 15)\n",
    "edgeModel = rnnModel(32, 8, 32)\n",
    "node_optimizer = torch.optim.Adam(nodeModel.parameters(), lr=0.01)\n",
    "edge_optimizer = torch.optim.Adam(edgeModel.parameters(), lr=0.01)\n",
    "loss_fn = F.binary_cross_entropy\n",
    "for e in range(100):\n",
    "    for batch in iter(data_loader):\n",
    "        #batch = batch.to(device)\n",
    "        #print(batch)\n",
    "        edge_hidden_state = None\n",
    "        node_hidden_state = None\n",
    "        for idx in range(len(batch.edge_sequence_indices[0])):\n",
    "            # first run node prediction model\n",
    "            #print(\"Node predicition half\")\n",
    "            node_pair = batch.node_sequence_indices[0][idx]\n",
    "            #print(node_pair)\n",
    "            node_pred, node_hidden_state = nodeModel.forward(batch.node_sequence[node_pair[0]:node_pair[1]], batch.x, batch.edge_index, batch.edge_attr, edge_hidden_state)\n",
    "            node_target = batch.node_sequence[node_pair[0]+1:node_pair[1]+1]\n",
    "            \n",
    "            #print(\"Edge predicition half\")\n",
    "            # then run edge prediction model\n",
    "            edge_pair = batch.edge_sequence_indices[0][idx]\n",
    "            #print(edge_pair)\n",
    "            edge_pred, edge_hidden_state = edgeModel.forward(batch.edge_sequence[edge_pair[0]:edge_pair[1]], batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_target = batch.edge_sequence[edge_pair[0]+1:edge_pair[1]+1]\n",
    "            \n",
    "            loss = loss_fn(node_pred, node_target)\n",
    "            node_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            node_optimizer.step()\n",
    "            print(loss.item())\n",
    "            \n",
    "            loss = loss_fn(edge_pred, edge_target)\n",
    "            edge_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            edge_optimizer.step()\n",
    "            print(loss.item())\n",
    "            \n",
    "        #pred = nodeModel.forward(batch.node_sequence[0:-1])\n",
    "        #loss = loss_fn(pred, batch.node_sequence[1:])\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        #self.testnnconv = \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim)))\n",
    "        self.convs.append(NNConv(hidden_dim, hidden_dim, nnconvnn(hidden_dim, hidden_dim)))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        F.dropout.training = self.training\n",
    "        #x = self.testnnconv(x, edge_index, edge_attr)\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        x = self.linear(x)\n",
    "        if self.return_embeds == False:\n",
    "            x = self.softmax(x)\n",
    "        out = x\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loader, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    for batch in iter(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        # o = o[train_idx] # we train on the whole graphs now\n",
    "        #print(o)\n",
    "        #print(batch.y.squeeze())\n",
    "        loss = loss_fn(o, batch.y.squeeze(1))\n",
    "        #########################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, loader, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        batch = batch.to(device)\n",
    "        ############# Your code here ############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. No index slicing here\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        #########################################\n",
    "\n",
    "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        #if count == 1:\n",
    "            #print(y_pred, batch.y)\n",
    "        \n",
    "        train_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        valid_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        test_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "\n",
    "    train_acc /= count\n",
    "    valid_acc /= count\n",
    "    test_acc /= count\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "    }\n",
    "    args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    model = GCN(30, args['hidden_dim'],\n",
    "              14, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    # reset the parameters to initial random value\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    loss_fn = F.nll_loss\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "        loss = train(model, data_loader, [], optimizer, loss_fn)\n",
    "        result = test(model, test_loader, [], evaluator)\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_acc:.2f}%, '\n",
    "              f'Valid: {100 * valid_acc:.2f}% '\n",
    "              f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(model, data_loader, [], evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
