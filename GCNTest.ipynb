{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "3.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the modified to_networkx function that doesn't throw exceptions\n",
    "\n",
    "def from_networkx(\n",
    "    G: Any,\n",
    "    group_node_attrs: Optional[Union[List[str], all]] = None,\n",
    "    group_edge_attrs: Optional[Union[List[str], all]] = None,\n",
    ") -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "        group_node_attrs (List[str] or all, optional): The node attributes to\n",
    "            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)\n",
    "        group_edge_attrs (List[str] or all, optional): The edge attributes to\n",
    "            be concatenated and added to :obj:`data.edge_attr`.\n",
    "            (default: :obj:`None`)\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must\n",
    "        be numeric.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> edge_index = torch.tensor([\n",
    "        ...     [0, 1, 1, 2, 2, 3],\n",
    "        ...     [1, 0, 2, 1, 3, 2],\n",
    "        ... ])\n",
    "        >>> data = Data(edge_index=edge_index, num_nodes=4)\n",
    "        >>> g = to_networkx(data)\n",
    "        >>> # A `Data` object is returned\n",
    "        >>> from_networkx(g)\n",
    "        Data(edge_index=[2, 6], num_nodes=4)\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        edges = list(G.edges(keys=False))\n",
    "    else:\n",
    "        edges = list(G.edges)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    if G.number_of_nodes() > 0:\n",
    "        node_attrs = list(next(iter(G.nodes(data=True)))[-1].keys())\n",
    "    else:\n",
    "        node_attrs = {}\n",
    "\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_attrs = list(next(iter(G.edges(data=True)))[-1].keys())\n",
    "    else:\n",
    "        edge_attrs = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        if set(feat_dict.keys()) != set(node_attrs):\n",
    "            raise ValueError('Not all nodes contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        if set(feat_dict.keys()) != set(edge_attrs):\n",
    "            raise ValueError('Not all edges contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for key, value in G.graph.items():\n",
    "        key = f'graph_{key}' if key in node_attrs else key\n",
    "        data[str(key)] = value\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (tuple, list)) and isinstance(value[0], Tensor):\n",
    "            data[key] = torch.stack(value, dim=0)\n",
    "        else:\n",
    "            try:\n",
    "                data[key] = torch.tensor(value)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = Data.from_dict(data)\n",
    "\n",
    "    if group_node_attrs is all:\n",
    "        group_node_attrs = list(node_attrs)\n",
    "    if group_node_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_node_attrs:\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if group_edge_attrs is all:\n",
    "        group_edge_attrs = list(edge_attrs)\n",
    "    if group_edge_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_edge_attrs:\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.edge_attr = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if data.x is None and data.pos is None:\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "def to_edge_index(adj: Union[Tensor, SparseTensor]) -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Converts a :class:`torch.sparse.Tensor` or a\n",
    "    :class:`torch_sparse.SparseTensor` to edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.sparse.Tensor or SparseTensor): The adjacency matrix.\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "        ...                            [1, 0, 2, 1, 3, 2]])\n",
    "        >>> adj = to_torch_coo_tensor(edge_index)\n",
    "        >>> to_edge_index(adj)\n",
    "        (tensor([[0, 1, 1, 2, 2, 3],\n",
    "                [1, 0, 2, 1, 3, 2]]),\n",
    "        tensor([1., 1., 1., 1., 1., 1.]))\n",
    "    \"\"\"\n",
    "    if isinstance(adj, SparseTensor):\n",
    "        row, col, value = adj.coo()\n",
    "        if value is None:\n",
    "            value = torch.ones(row.size(0), device=row.device)\n",
    "        return torch.stack([row, col], dim=0), value\n",
    "\n",
    "    if adj.requires_grad:\n",
    "        # Calling adj._values() will return a detached tensor.\n",
    "        # Use `adj.coalesce().values()` instead to track gradients.\n",
    "        adj = adj.coalesce()\n",
    "        return adj.indices(), adj.values()\n",
    "\n",
    "    return adj._indices(), adj._values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "        \"Point\": 0,\n",
    "        \"Line\": 1,\n",
    "        \"Circle\": 2,\n",
    "        \"Ellipse\": 3,\n",
    "        \"Spline\": 4,\n",
    "        \"Conic\": 5,\n",
    "        \"Arc\": 6,\n",
    "        \"External\": 7,\n",
    "        \"Stop\": 8,\n",
    "        \"Unknown\": 9,\n",
    "        \"SN_Start\": 11,\n",
    "        \"SN_End\": 12,\n",
    "        \"SN_Center\": 13\n",
    "    }\n",
    "\n",
    "edge_dict = {\n",
    "    \"Coincident\": 0,\n",
    "    \"Projected\": 1,\n",
    "    \"Mirror\": 2,\n",
    "    \"Distance\": 3,\n",
    "    \"Horizontal\": 4,\n",
    "    \"Parallel\": 5,\n",
    "    \"Vertical\": 6,\n",
    "    \"Tangent\": 7,\n",
    "    \"Length\": 8,\n",
    "    \"Perpendicular\": 9,\n",
    "    \"Midpoint\": 10,\n",
    "    \"Equal\": 11,\n",
    "    \"Diameter\": 12,\n",
    "    \"Offset\": 13,\n",
    "    \"Radius\": 14,\n",
    "    \"Concentric\": 15,\n",
    "    \"Fix\": 16,\n",
    "    \"Angle\": 17,\n",
    "    \"Circular_Pattern\": 18,\n",
    "    \"Pierce\": 19,\n",
    "    \"Linear_Pattern\": 20,\n",
    "    \"Centerline_Dimension\": 21,\n",
    "    \"Intersected\": 22,\n",
    "    \"Silhoutted\": 23,\n",
    "    \"Quadrant\": 24,\n",
    "    \"Normal\": 25,\n",
    "    \"Minor_Diameter\": 26,\n",
    "    \"Major_Diameter\": 27,\n",
    "    \"Rho\": 28,\n",
    "    \"Unknown\": 29,\n",
    "    \"Subnode\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "def get_sketch_features(graph, feature_dim):\n",
    "    x = torch.zeros([graph.num_nodes, feature_dim])\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx, p in enumerate(graph.parameters):\n",
    "        \n",
    "        # add one hot encoding to feature vector for node label\n",
    "        onePos = label_dict[graph.label[idx]]/7\n",
    "        for i in range(0, 14):\n",
    "            x[idx, i] = 1 if onePos==i else 0\n",
    "        \n",
    "        # convert label text into a feature value\n",
    "        x[idx, 14] = label_dict[graph.label[idx]]/7\n",
    "        \n",
    "        param_dict = json.loads(p)\n",
    "        for i, k in enumerate(param_dict.keys()):\n",
    "            \n",
    "            if i+2 == feature_dim:\n",
    "                break\n",
    "            \n",
    "            # convert each parameter value into a feature value\n",
    "            x[idx, i+15] = float(param_dict[k])\n",
    "        \n",
    "        x[idx, -1] = degree(graph.edge_index[0], graph.num_nodes)[idx]\n",
    "        #print(idx, p)\n",
    "        #print(x[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_attr_y(graph):\n",
    "    y = torch.zeros([graph.num_nodes, 1], dtype=torch.int64)\n",
    "    #rint(graph.label)\n",
    "    \n",
    "    \n",
    "    for i, l in enumerate(graph.label):\n",
    "        y[i, 0] = label_dict[l]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_adj(graph):\n",
    "    tst = T.ToSparseTensor()\n",
    "    return tst(graph).adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_large_graph(graphs):\n",
    "    bestN = 0\n",
    "    bestI = 0\n",
    "    for i, g in enumerate(graphs):\n",
    "        if len(g) > bestN:\n",
    "            bestN = len(g)\n",
    "            bestI = i\n",
    "    #print(bestI)\n",
    "    return graphs[bestI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_edge_attr(graph):\n",
    "    dim = 31\n",
    "    edge_attr = torch.zeros([len(graph.edge_label), dim])\n",
    "    for idx, l in enumerate(graph.edge_label):\n",
    "        edge_attr[idx, edge_dict[l]] = 1\n",
    "    return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for now we will do node class sequence prediction\n",
    "\n",
    "def get_sketch_construction_node_sequence(sg, graph):\n",
    "    edge_sequence_indices = []\n",
    "    node_sequence_indices = []\n",
    "    node_sequence = torch.zeros((0, 15), dtype=torch.float) # 15th element is stop sign\n",
    "    edge_sequence = torch.zeros((0, 32), dtype=torch.float) # 32nd element is stop sign\n",
    "    start_node_idx=0\n",
    "    start_edge_idx=0\n",
    "    node_idx = 0\n",
    "    edge_idx=0\n",
    "    is_edge_sequence = False\n",
    "    for elem in sg:\n",
    "        \n",
    "        if node_idx == len(graph.label):\n",
    "            break\n",
    "            \n",
    "        if type(elem) == sketchgraphs.data.sequence.NodeOp:\n",
    "            if is_edge_sequence:\n",
    "                is_edge_sequence=False\n",
    "                stop_token = torch.zeros((1, 32))\n",
    "                stop_token[0, 31] = 1\n",
    "                edge_sequence = torch.cat((edge_sequence, stop_token))\n",
    "                edge_sequence_indices.append((start_edge_idx, edge_idx))\n",
    "                start_node_idx=node_idx\n",
    "                edge_idx+=1\n",
    "            one_hot = torch.zeros((1, 15))\n",
    "            one_hot[0, label_dict[graph.label[node_idx]]] = 1\n",
    "            node_sequence = torch.cat((node_sequence, one_hot))\n",
    "            node_idx+=1\n",
    "        elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "            #print(elem)\n",
    "            if not is_edge_sequence:\n",
    "                is_edge_sequence=True\n",
    "                start_edge_idx=edge_idx\n",
    "                stop_token = torch.zeros((1, 15))\n",
    "                stop_token[0, 14] = 1\n",
    "                node_sequence = torch.cat((node_sequence, stop_token))\n",
    "                node_sequence_indices.append((start_node_idx, node_idx))\n",
    "                node_idx+=1\n",
    "            constraintNumber = edge_dict[graph.edge_label[edge_idx]]\n",
    "            one_hot = torch.zeros((1, 32))\n",
    "            one_hot[0, constraintNumber] = 1\n",
    "            edge_sequence = torch.cat((edge_sequence, one_hot))\n",
    "            edge_idx+=1\n",
    "            \n",
    "    #node_sequence[-1, 14] = 1\n",
    "    #edge_sequence[-1, 31] = 1\n",
    "    #print(node_sequence_indices, edge_sequence_indices)\n",
    "    return node_sequence, edge_sequence, node_sequence_indices, edge_sequence_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sketchgraph_node_constraint_sequence(sg, graph):\n",
    "    #y = torch.zeros((len(sg)-1, 2+31)) # 2 classes (node or edge), 14 node types (but contained in 31 positions for edge types), 31 edge types\n",
    "    #node_idx=0\n",
    "    #edge_idx=0\n",
    "    #for idx, elem in enumerate(sg):\n",
    "    #    if type(elem) == sketchgraphs.data.sequence.NodeOp and not elem.label == sketchgraphs.data._entity.EntityType.Stop:\n",
    "    #        y[idx, 0] = 1\n",
    "    #        y[idx, 2+label_dict[graph.label[node_idx]]] = 1  # 0.5 because we're marking 2 spaces in a vector of zeroes, to sum up to 1\n",
    "    #        node_idx+=1\n",
    "    #    elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "    #        y[idx, 1] = 1\n",
    "    #        y[idx, 2+edge_dict[graph.edge_label[edge_idx]]] = 1\n",
    "    #        edge_idx+=1\n",
    "    #return y\n",
    "    y = torch.zeros((len(sg)-1), dtype=torch.long)\n",
    "    node_idx=0\n",
    "    edge_idx=0\n",
    "    for idx, elem in enumerate(sg):\n",
    "        if type(elem) == sketchgraphs.data.sequence.NodeOp and not elem.label == sketchgraphs.data._entity.EntityType.Stop:\n",
    "            y[idx] = label_dict[graph.label[node_idx]]  # 0.5 because we're marking 2 spaces in a vector of zeroes, to sum up to 1\n",
    "            node_idx+=1\n",
    "        elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "            y[idx] = edge_dict[graph.edge_label[edge_idx]]\n",
    "            edge_idx+=1\n",
    "    return y\n",
    "\n",
    "\n",
    "def convert_sketchgraph_to_pytorch(sg):\n",
    "    # convert first to pyGraphViz graph using sketchgraph's function\n",
    "    pgv_graph = sketchgraphs.data.sequence.pgvgraph_from_sequence(sg)\n",
    "    # then to networkx graph\n",
    "    nx_graph = nx.Graph(pgv_graph)\n",
    "    # finally to pyTorch graph\n",
    "    graph = from_networkx(nx_graph)\n",
    "    return graph\n",
    "\n",
    "def assign_attributes_to_graph(graph):\n",
    "    graph.x = get_sketch_features(graph, 30)\n",
    "    graph.y = get_sketch_attr_y(graph)\n",
    "    graph.adj_t = get_sketch_adj(graph)\n",
    "    graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "    graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "    return graph\n",
    "\n",
    "def get_sketchgraph_graph_sequence(sg):\n",
    "    generated_graph = []\n",
    "    sequence = []\n",
    "    for elem in sg:\n",
    "        generated_graph.append(elem)\n",
    "        new_graph = convert_sketchgraph_to_pytorch(generated_graph)\n",
    "        if not hasattr(new_graph, 'edge_label'):\n",
    "            new_graph.edge_label = []\n",
    "        new_graph = assign_attributes_to_graph(new_graph)\n",
    "        sequence.append(new_graph)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def custom_collate(batch, b):\\n    print(\"AAAAA\")\\n    edge_label_batch = []\\n    for b in batch:\\n        #print(b.edge_label)\\n        edge_label_batch.append(b.edge_label)\\n    return edge_label_batch'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom dataset class of custom attributes\n",
    "from torch_geometric.data import Dataset\n",
    "class SketchgraphDataset(Dataset):\n",
    "    def __init__(self, start_idx, end_idx, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data = []\n",
    "        seq_data = flat_array.load_dictionary_flat('datasets/sg_t16_validation.npy')\n",
    "        print(len(seq_data['sequences']))\n",
    "\n",
    "        #test_graph_seq = find_large_graph(seq_data['sequences'])\n",
    "        test_graph_seq = seq_data['sequences'][206778]\n",
    "        test_graph_seq1 = seq_data['sequences'][10]\n",
    "\n",
    "        sketchgraps_list = seq_data['sequences'][start_idx:end_idx]\n",
    "\n",
    "        link_transform = T.RandomLinkSplit(is_undirected=True, key=\"edge_attr\")\n",
    "        \n",
    "        for sg in sketchgraps_list:\n",
    "            #print(test_graph_seq)\n",
    "            graph = convert_sketchgraph_to_pytorch(sg)\n",
    "            \n",
    "            if not hasattr(graph, 'edge_label'):\n",
    "                continue\n",
    "\n",
    "            # next we need to add required attributes: x, y, adj_t\n",
    "            graph.x = get_sketch_features(graph, 30)\n",
    "            graph.y = get_sketch_attr_y(graph)\n",
    "            graph.adj_t = get_sketch_adj(graph)\n",
    "            graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "            graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "            graph.seq = get_sketchgraph_graph_sequence(sg)\n",
    "            graph.seq_y = get_sketchgraph_node_constraint_sequence(sg, graph)\n",
    "            #graph.node_sequence, graph.edge_sequence, graph.node_sequence_indices, graph.edge_sequence_indices = get_sketch_construction_node_sequence(sg, graph)\n",
    "            #print(len(graph.edge_label))\n",
    "            #graph = link_transform(graph)\n",
    "            self.data.append(graph)\n",
    "            \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\"\"\"def custom_collate(batch, b):\n",
    "    print(\"AAAAA\")\n",
    "    edge_label_batch = []\n",
    "    for b in batch:\n",
    "        #print(b.edge_label)\n",
    "        edge_label_batch.append(b.edge_label)\n",
    "    return edge_label_batch\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "315228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  1, 11,  4,  0, 12, 30,  1, 30,  3,  4, 11, 30, 12,  0,  1, 30, 11,\n",
       "         0,  0, 12,  3,  4,  1, 30, 11, 30,  9, 12, 30,  0,  1, 11, 30,  0, 12,\n",
       "         9,  1, 30, 30, 11,  5, 12,  0, 30,  1,  0, 11, 30,  5, 12, 30, 30,  1,\n",
       "         0, 11, 30,  0, 12, 30, 30])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sketchgraphs\n",
    "import networkx as nx\n",
    "from sketchgraphs.data import flat_array\n",
    "\n",
    "train_dataset = SketchgraphDataset(0, 30)\n",
    "test_dataset = SketchgraphDataset(101, 111)\n",
    "    \n",
    "data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "next(iter(data_loader)).seq_y\n",
    "#for b in iter(data_loader):\n",
    "#    print(b)\n",
    "#print(graph)\n",
    "#print(graph.x)\n",
    "#print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n"
     ]
    }
   ],
   "source": [
    "val_dataset = SketchgraphDataset(101, 111)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class linkPredictionGAE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(linkPredictionGAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.conv1 = NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim))\n",
    "        self.conv2 = NNConv(hidden_dim, output_dim, nnconvnn(hidden_dim, output_dim))\n",
    "        \n",
    "    def encode(self, x, pos_edge_index, edge_attr):\n",
    "        x = self.conv1(x, pos_edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, pos_edge_index, edge_attr)\n",
    "    \n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "        return logits\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n",
      "Device: cuda\n",
      "tensor([     0,      1,      2,  ..., 169145, 169148, 169251])\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    dataset_name = 'ogbn-arxiv'\n",
    "    dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    #data = batch\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the adjacency matrix to symmetric\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    #print(data.y)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #device = 'cpu'\n",
    "    # If you use GPU, the device should be cuda\n",
    "    print('Device: {}'.format(device))\n",
    "    data = data.to(device)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    print(split_idx['train'])\n",
    "    #train_idx = torch.LongTensor(range(0, data.num_nodes)).to(device)\n",
    "    #print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1\n",
    "    return link_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_predictor(model):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "    loss_avg = 0\n",
    "    batch_count = 0\n",
    "    for batch in iter(data_loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_count+=1\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index,\n",
    "            num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z = model.encode(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        link_logits = model.decode(z, batch.edge_index, neg_edge_index)\n",
    "        link_labels = get_link_labels(batch.edge_index, neg_edge_index)\n",
    "        loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "    return loss_avg / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    perf=0\n",
    "    batch_count = 0\n",
    "    for batch in iter(test_loader):\n",
    "        batch = batch.to(device)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index,\n",
    "            num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        z = model.encode(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        link_logits = model.decode(z, batch.edge_index, neg_edge_index)\n",
    "        link_probs = link_logits.sigmoid()\n",
    "        link_labels = get_link_labels(batch.edge_index, neg_edge_index)\n",
    "        try:\n",
    "            perf+=roc_auc_score(link_labels.cpu().detach().numpy(), link_probs.cpu().detach().numpy())\n",
    "            batch_count+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return perf/batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linkPredictionGAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# training link prediction model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlinkPredictionGAE\u001b[49m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linkPredictionGAE' is not defined"
     ]
    }
   ],
   "source": [
    "# training link prediction model\n",
    "\n",
    "model = linkPredictionGAE(30, 128, 64).to(device)\n",
    "\n",
    "epochs = 5000\n",
    "for e in range(epochs):\n",
    "    train_loss = train_link_predictor(model)\n",
    "    perf = test(model)\n",
    "    print(train_loss, perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
      "         4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n",
      "        [0, 1, 2, 5, 6, 7, 0, 1, 2, 5, 6, 7, 0, 1, 2, 4, 5, 6, 7, 9, 3, 4, 8, 9,\n",
      "         2, 3, 4, 7, 8, 9, 0, 1, 2, 5, 6, 7, 0, 1, 2, 5, 6, 7, 0, 1, 2, 4, 5, 6,\n",
      "         7, 9, 3, 4, 8, 9, 2, 3, 4, 7, 8, 9]], device='cuda:0')\n",
      "tensor([[0, 1, 2, 2, 2, 3, 4, 4, 5, 6, 7, 7, 7, 8, 9, 9],\n",
      "        [2, 2, 0, 1, 4, 4, 2, 3, 7, 7, 5, 6, 9, 9, 7, 8]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "testGraph = next(iter(test_loader)).to(device)\n",
    "z = model.encode(testGraph.x, testGraph.edge_index, testGraph.edge_attr)\n",
    "final_edge_index = model.decode_all(z)\n",
    "print(final_edge_index)\n",
    "print(testGraph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rnnModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim):\n",
    "        super(rnnModel, self).__init__()\n",
    "        self.layer_count = 2\n",
    "        self.batch_size = 64\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preprocess_gcn = NNConv(30, hidden_size, nnconvnn(30, hidden_size))\n",
    "        #self.gcn_mlp = torch.nn.Linear(hidden_size, self.layer_count)\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_size, self.layer_count) # batch first = True means the first dimension is batch size\n",
    "        self.mlp = torch.nn.Linear(hidden_size, output_dim)\n",
    "        # x -> batch_size x sequence_length x input_dim if we wanted many to one RNN\n",
    "        # for many to many, we use sequence length of 1\n",
    "        self.hidden_state = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.rnn.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, graph_x, edge_index, edge_attr, prev_hid=None):\n",
    "        #print(self.rnn._flat_weights[0].dtype, x.dtype)\n",
    "        #self.hidden_state.to(device)\n",
    "        hidden=None\n",
    "        if prev_hid==None:\n",
    "            hidden = self.preprocess_gcn(graph_x, edge_index, edge_attr)\n",
    "        #hidden = self.gcn_mlp(hidden)\n",
    "            hidden = global_mean_pool(hidden, torch.zeros((graph_x.size(0)), dtype=torch.long))\n",
    "            hidden = torch.cat((hidden, hidden))\n",
    "        else:\n",
    "            hidden=prev_hid\n",
    "            \n",
    "        x, self.hidden_state = self.rnn(x, hidden)\n",
    "        x = self.mlp(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x, self.hidden_state.detach()\n",
    "    \n",
    "    def generate(self, graph_x, ):\n",
    "        pass\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class newrnnmodel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim_second):\n",
    "        super(newrnnmodel, self).__init__()\n",
    "        self.layer_count = 2\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_gcn = NNConv(30, hidden_size, nnconvnn(30, hidden_size))\n",
    "\n",
    "        self.rnn_class = torch.nn.RNN(hidden_size, hidden_size, self.layer_count) # batch first = True means the first dimension is batch size\n",
    "\n",
    "        self.mlp_class = torch.nn.Linear(hidden_size, output_dim_second)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.rnn_class.reset_parameters()\n",
    "        self.mlp_class.reset_parameters()\n",
    "        self.input_gcn.reset_parameters()\n",
    "        \n",
    "    def forward(self, graph_sequence):\n",
    "\n",
    "        hidden2 = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float).to(device)\n",
    "\n",
    "        output2 = torch.tensor([]).to(device)\n",
    "        for g in graph_sequence:\n",
    "            x = self.input_gcn(g.x, g.edge_index, g.edge_attr)\n",
    "            x = global_mean_pool(x, torch.zeros((g.x.size(0)), dtype=torch.long).to(device))\n",
    "            \n",
    "\n",
    "            x2, hidden1 = self.rnn_class(x, hidden2)\n",
    "            \n",
    "\n",
    "            x2 = self.mlp_class(x2)\n",
    "            \n",
    "\n",
    "            #x2 = F.softmax(x2, dim=1)\n",
    "            \n",
    "\n",
    "            output2 = torch.cat((output2, x2))\n",
    "        return output2\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        #hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        #return hidden\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5698639035224913\n",
      "2.1172794938087462\n",
      "1.968060032526652\n",
      "1.926110859711965\n",
      "1.7571045418580373\n",
      "1.7062567313512167\n",
      "1.7551534593105316\n",
      "1.6377834111452103\n",
      "1.6300456792116165\n",
      "1.5772558338940144\n",
      "1.5853110074996948\n",
      "1.5330227663119633\n",
      "1.5388501033186912\n",
      "1.4272451050579549\n",
      "1.432727794845899\n",
      "1.526770509282748\n",
      "1.4930452813704809\n",
      "1.3420137765506903\n",
      "1.4475139893591404\n",
      "1.391873773187399\n"
     ]
    }
   ],
   "source": [
    "rnn = newrnnmodel(15, 64, 31).to(device)\n",
    "\n",
    "optimizer_class = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
    "loss_fn = F.nll_loss\n",
    "#loss_fn = F.mse_loss\n",
    "epochs = 20\n",
    "rnn.train()\n",
    "for e in range(epochs):\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for batch in iter(data_loader):\n",
    "        count+=1\n",
    "        batch.to(device)\n",
    "\n",
    "        optimizer_class.zero_grad()\n",
    "        pred_class = rnn.forward(batch.seq[0])\n",
    "        #print(pred_type)\n",
    "        #print(pred_class)\n",
    "        #print(batch.seq_y.size())\n",
    "        loss_class = loss_fn(F.log_softmax(pred_class[:-1], dim=1), batch.seq_y)\n",
    "\n",
    "        loss_class.backward()\n",
    "        \n",
    "        loss_avg += loss_type.item() + loss_class.item()\n",
    "        \n",
    "\n",
    "        optimizer_class.step()\n",
    "        \n",
    "        \n",
    "    print(loss_avg/count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3704395294189453\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [2.5493e-03, 9.5571e-01, 2.7325e-05, 7.6587e-05, 1.2103e-03, 4.8872e-04,\n",
      "         9.4283e-04, 1.1813e-02, 2.3542e-08, 8.3681e-05, 8.0828e-05, 2.4851e-02,\n",
      "         1.6024e-03, 3.9309e-04, 1.1267e-07, 1.3767e-05, 3.0393e-08, 7.5257e-08,\n",
      "         1.0144e-07, 4.1464e-08, 6.2333e-08, 8.6511e-08, 4.5104e-08, 6.7198e-08,\n",
      "         1.0023e-07, 2.8288e-08, 3.1109e-08, 4.9281e-08, 1.0892e-07, 6.7393e-08,\n",
      "         1.5559e-04],\n",
      "        [2.6858e-02, 7.2164e-02, 4.0057e-06, 6.8820e-04, 1.0378e-02, 8.0667e-04,\n",
      "         2.1167e-01, 5.1930e-01, 6.2347e-07, 1.0757e-04, 4.1761e-03, 1.8332e-03,\n",
      "         4.8764e-03, 2.3057e-05, 1.5809e-06, 2.3409e-04, 7.9610e-07, 2.0968e-06,\n",
      "         2.0741e-06, 1.1802e-06, 1.1699e-06, 2.2977e-06, 9.2079e-07, 1.0300e-06,\n",
      "         1.9828e-06, 7.3936e-07, 8.0663e-07, 1.4401e-06, 1.0781e-06, 1.3193e-06,\n",
      "         1.4686e-01],\n",
      "        [6.5643e-04, 4.9008e-03, 1.2400e-06, 1.1643e-03, 2.4787e-02, 1.3542e-04,\n",
      "         1.3639e-02, 3.7000e-04, 2.6408e-07, 5.7654e-05, 5.8618e-04, 7.6238e-01,\n",
      "         1.7979e-01, 1.1035e-02, 8.6485e-07, 4.4987e-05, 5.0137e-07, 8.6136e-07,\n",
      "         6.1433e-07, 5.4283e-07, 3.9030e-07, 4.1600e-07, 5.0769e-07, 3.5837e-07,\n",
      "         8.2111e-07, 4.2049e-07, 1.9766e-07, 7.2886e-07, 5.2032e-07, 4.1799e-07,\n",
      "         4.5052e-04],\n",
      "        [6.5956e-02, 1.0645e-01, 4.7727e-06, 3.0524e-03, 2.6228e-02, 1.2724e-03,\n",
      "         2.4189e-01, 3.0992e-02, 1.6898e-06, 2.2051e-04, 7.1230e-03, 1.4585e-02,\n",
      "         3.0612e-02, 1.0994e-04, 4.5163e-06, 3.3968e-04, 2.2963e-06, 5.7184e-06,\n",
      "         3.0628e-06, 2.5801e-06, 2.2704e-06, 4.2490e-06, 4.2233e-06, 3.2400e-06,\n",
      "         3.0048e-06, 2.5766e-06, 3.2870e-06, 4.5122e-06, 3.7238e-06, 3.0474e-06,\n",
      "         4.7111e-01],\n",
      "        [4.3272e-02, 5.0316e-02, 7.1275e-06, 1.5241e-02, 2.1843e-01, 7.0759e-03,\n",
      "         1.2770e-01, 7.9745e-03, 2.3486e-05, 1.1514e-03, 5.1845e-03, 1.1045e-01,\n",
      "         5.1661e-02, 5.5632e-04, 4.3974e-05, 4.0049e-04, 3.1087e-05, 5.5418e-05,\n",
      "         3.4568e-05, 4.5566e-05, 3.1399e-05, 4.1602e-05, 3.7998e-05, 3.1630e-05,\n",
      "         3.6875e-05, 3.2933e-05, 3.8868e-05, 3.9286e-05, 4.2612e-05, 2.9158e-05,\n",
      "         3.5999e-01],\n",
      "        [4.3689e-02, 2.6660e-02, 5.6478e-06, 1.7918e-02, 2.2688e-01, 6.7273e-03,\n",
      "         2.8099e-02, 3.0287e-04, 8.2478e-06, 1.3503e-03, 2.5887e-03, 2.0998e-01,\n",
      "         3.0327e-01, 4.2928e-04, 1.6507e-05, 2.0250e-04, 1.3239e-05, 2.8145e-05,\n",
      "         1.3107e-05, 2.1166e-05, 1.2075e-05, 1.2842e-05, 1.2476e-05, 1.2774e-05,\n",
      "         1.6368e-05, 1.3216e-05, 1.4928e-05, 2.0766e-05, 1.9237e-05, 1.5307e-05,\n",
      "         1.3165e-01],\n",
      "        [1.1800e-01, 1.8260e-01, 1.5281e-05, 6.8159e-03, 7.0753e-02, 5.1159e-03,\n",
      "         5.5006e-02, 5.8856e-04, 2.8243e-06, 1.0247e-03, 3.4776e-03, 9.3318e-02,\n",
      "         7.4508e-02, 1.9020e-04, 7.1684e-06, 1.8905e-04, 4.3008e-06, 1.0497e-05,\n",
      "         4.7064e-06, 7.3404e-06, 4.2317e-06, 6.2391e-06, 5.8872e-06, 6.1082e-06,\n",
      "         5.5803e-06, 4.7440e-06, 5.6164e-06, 8.4363e-06, 6.5758e-06, 5.6037e-06,\n",
      "         3.8830e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  1,  0, 11,  0,  6, 12, 30, 30], device='cuda:0')\n",
      "0.41726043820381165\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [1.3358e-04, 2.2132e-04, 9.9278e-01, 1.7483e-05, 3.7628e-06, 3.2506e-05,\n",
      "         1.9035e-05, 4.7130e-04, 1.1558e-04, 1.4139e-04, 2.1466e-04, 1.3909e-05,\n",
      "         1.0995e-05, 2.8959e-03, 1.5251e-04, 1.2021e-03, 1.0304e-04, 9.5070e-05,\n",
      "         1.1674e-04, 9.1226e-05, 9.0088e-05, 8.0557e-05, 1.1267e-04, 1.5054e-04,\n",
      "         1.1816e-04, 1.0378e-04, 1.0133e-04, 1.7869e-04, 9.4875e-05, 1.2430e-04,\n",
      "         1.0116e-05],\n",
      "        [6.2668e-01, 2.3929e-04, 3.4203e-03, 1.7086e-04, 3.0932e-05, 5.8471e-05,\n",
      "         1.0637e-03, 4.7660e-05, 9.1689e-06, 7.6646e-05, 1.7673e-01, 2.8424e-04,\n",
      "         2.1553e-02, 3.7004e-02, 1.9439e-05, 1.1977e-01, 1.7567e-05, 2.6225e-05,\n",
      "         1.6723e-05, 2.1026e-05, 1.2001e-05, 1.5232e-05, 1.8866e-05, 2.4373e-05,\n",
      "         2.4675e-05, 1.0076e-05, 1.6658e-05, 3.1173e-05, 1.8336e-05, 1.6376e-05,\n",
      "         1.2575e-02],\n",
      "        [5.1562e-04, 4.1584e-05, 3.9966e-05, 3.4696e-05, 3.6882e-05, 4.5961e-06,\n",
      "         2.6031e-05, 1.4858e-06, 1.1319e-07, 2.4719e-06, 1.8920e-04, 3.1486e-03,\n",
      "         3.1083e-03, 9.9259e-01, 4.0238e-07, 2.0700e-04, 1.4920e-07, 2.8368e-07,\n",
      "         2.9081e-07, 3.6315e-07, 1.3860e-07, 2.1146e-07, 2.5871e-07, 2.7060e-07,\n",
      "         4.3643e-07, 2.1294e-07, 1.6350e-07, 4.6215e-07, 2.5003e-07, 2.4000e-07,\n",
      "         4.7512e-05],\n",
      "        [3.9356e-02, 2.6323e-04, 3.5131e-04, 1.2396e-04, 3.8073e-05, 1.3758e-05,\n",
      "         6.3337e-03, 6.5884e-06, 1.1148e-06, 2.2287e-05, 5.8785e-02, 2.9042e-04,\n",
      "         8.4211e-01, 2.5695e-03, 2.0541e-06, 3.1576e-03, 2.4057e-06, 2.6475e-06,\n",
      "         2.1508e-06, 1.5419e-06, 2.0308e-06, 1.7363e-06, 1.7579e-06, 1.4945e-06,\n",
      "         3.4157e-06, 1.5454e-06, 1.9775e-06, 2.8203e-06, 2.4197e-06, 1.4225e-06,\n",
      "         4.6541e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  2, 15, 13, 12, 30], device='cuda:0')\n",
      "1.5256675481796265\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [8.4473e-03, 8.4182e-01, 3.1035e-04, 2.7263e-04, 1.3014e-03, 1.3747e-03,\n",
      "         8.4049e-04, 8.4412e-03, 1.5347e-07, 3.8920e-04, 1.7652e-04, 1.2700e-01,\n",
      "         5.3318e-03, 4.1068e-03, 5.3233e-07, 8.7402e-05, 1.7441e-07, 4.6478e-07,\n",
      "         5.7049e-07, 2.6500e-07, 2.6986e-07, 3.2855e-07, 2.2437e-07, 2.8050e-07,\n",
      "         4.7138e-07, 1.3613e-07, 1.9185e-07, 3.1773e-07, 5.0784e-07, 3.3850e-07,\n",
      "         9.7663e-05],\n",
      "        [1.1351e-03, 4.6759e-03, 5.4248e-06, 2.0702e-04, 1.1553e-03, 1.3165e-04,\n",
      "         3.2118e-04, 3.8718e-05, 8.2152e-08, 5.0886e-05, 7.8261e-05, 9.4809e-01,\n",
      "         1.9767e-02, 2.4275e-02, 3.4795e-07, 3.9824e-05, 1.5632e-07, 2.3786e-07,\n",
      "         2.1114e-07, 2.5517e-07, 1.3566e-07, 1.2143e-07, 1.8054e-07, 2.1316e-07,\n",
      "         2.3641e-07, 1.5728e-07, 8.2331e-08, 2.0502e-07, 2.3885e-07, 1.8579e-07,\n",
      "         2.9525e-05],\n",
      "        [2.8689e-01, 3.1487e-01, 2.2488e-05, 5.5593e-04, 8.3535e-03, 5.5441e-03,\n",
      "         2.7085e-02, 1.9252e-03, 1.7450e-07, 8.1518e-04, 9.5783e-04, 2.8433e-02,\n",
      "         1.5888e-02, 1.2598e-05, 4.2401e-07, 8.6829e-05, 1.8262e-07, 4.1501e-07,\n",
      "         3.7872e-07, 2.1378e-07, 2.0012e-07, 4.7616e-07, 4.2094e-07, 2.6458e-07,\n",
      "         3.2508e-07, 1.8941e-07, 2.0559e-07, 3.9705e-07, 2.2231e-07, 3.0402e-07,\n",
      "         3.0856e-01],\n",
      "        [2.8820e-03, 7.9127e-03, 2.3314e-06, 2.3731e-04, 2.3867e-03, 1.9182e-04,\n",
      "         3.6273e-03, 1.4697e-05, 1.6794e-08, 4.8501e-05, 1.6802e-04, 4.3261e-01,\n",
      "         5.4793e-01, 1.0862e-03, 5.2697e-08, 1.7675e-05, 4.7840e-08, 5.4005e-08,\n",
      "         4.5824e-08, 2.4814e-08, 2.9655e-08, 3.7559e-08, 3.7677e-08, 2.6139e-08,\n",
      "         5.7197e-08, 3.6395e-08, 1.7233e-08, 5.3914e-08, 3.5861e-08, 2.9826e-08,\n",
      "         8.9081e-04],\n",
      "        [1.9329e-01, 3.6443e-01, 2.1141e-05, 7.4140e-04, 7.4432e-03, 4.2882e-03,\n",
      "         3.1836e-02, 1.4898e-03, 2.6731e-07, 6.8737e-04, 1.1854e-03, 1.9653e-02,\n",
      "         1.1477e-02, 1.4664e-05, 7.6362e-07, 7.7107e-05, 2.7502e-07, 6.3957e-07,\n",
      "         5.6385e-07, 3.5166e-07, 3.0507e-07, 7.2883e-07, 6.9561e-07, 5.0143e-07,\n",
      "         4.6069e-07, 2.9376e-07, 4.2579e-07, 7.0990e-07, 3.7887e-07, 4.6851e-07,\n",
      "         3.6336e-01],\n",
      "        [2.1546e-01, 3.6984e-01, 1.8635e-05, 6.0636e-04, 7.1267e-03, 4.3621e-03,\n",
      "         3.0766e-02, 1.5642e-03, 1.8365e-07, 6.6200e-04, 9.4395e-04, 2.4834e-02,\n",
      "         1.4303e-02, 1.4147e-05, 4.8643e-07, 7.1319e-05, 1.8371e-07, 4.2257e-07,\n",
      "         3.8037e-07, 2.2701e-07, 2.0824e-07, 4.8638e-07, 4.8146e-07, 3.1213e-07,\n",
      "         3.3918e-07, 2.0784e-07, 2.4306e-07, 4.1428e-07, 2.4291e-07, 3.4147e-07,\n",
      "         3.2943e-01],\n",
      "        [2.4057e-01, 3.0699e-01, 4.9180e-05, 1.2509e-03, 8.7434e-03, 1.3831e-02,\n",
      "         1.3866e-02, 5.3274e-04, 1.8028e-06, 2.0050e-03, 8.8460e-04, 3.4517e-02,\n",
      "         1.9859e-02, 1.7329e-05, 3.7205e-06, 1.0665e-04, 1.6709e-06, 2.9015e-06,\n",
      "         3.1468e-06, 2.5393e-06, 2.0957e-06, 4.0205e-06, 3.5711e-06, 3.2681e-06,\n",
      "         2.2791e-06, 1.9807e-06, 2.6322e-06, 3.8293e-06, 2.6043e-06, 2.1407e-06,\n",
      "         3.5673e-01],\n",
      "        [1.9018e-01, 2.6188e-01, 3.6733e-05, 3.0941e-03, 2.5955e-02, 3.3236e-02,\n",
      "         1.1923e-02, 4.9502e-04, 3.0564e-06, 3.6244e-03, 7.8196e-04, 1.2716e-01,\n",
      "         3.5052e-02, 3.1815e-05, 6.5057e-06, 1.1054e-04, 3.4144e-06, 6.6057e-06,\n",
      "         5.5128e-06, 5.0187e-06, 4.0120e-06, 6.6122e-06, 5.4325e-06, 4.8988e-06,\n",
      "         4.3933e-06, 3.7318e-06, 4.3306e-06, 5.9472e-06, 5.3215e-06, 3.8946e-06,\n",
      "         3.0636e-01],\n",
      "        [1.2386e-01, 5.8263e-02, 2.2234e-05, 3.2097e-03, 4.4673e-02, 3.2927e-02,\n",
      "         3.7461e-03, 4.9861e-05, 3.9729e-06, 4.4860e-03, 3.2524e-04, 4.6078e-01,\n",
      "         1.6841e-01, 7.5474e-05, 5.4582e-06, 7.7517e-05, 4.5780e-06, 6.7176e-06,\n",
      "         5.9172e-06, 6.5646e-06, 5.1039e-06, 5.6549e-06, 5.5258e-06, 4.6694e-06,\n",
      "         5.4653e-06, 5.7005e-06, 4.1404e-06, 5.0648e-06, 5.6057e-06, 4.5438e-06,\n",
      "         9.9003e-02],\n",
      "        [1.7261e-01, 2.3118e-01, 3.5423e-05, 3.9213e-03, 3.4156e-02, 3.8948e-02,\n",
      "         1.1206e-02, 4.5532e-04, 4.3403e-06, 4.1023e-03, 8.0497e-04, 1.5935e-01,\n",
      "         3.7959e-02, 3.6655e-05, 9.0995e-06, 1.1033e-04, 5.0558e-06, 9.6958e-06,\n",
      "         7.5611e-06, 7.5144e-06, 5.8866e-06, 8.8789e-06, 7.4792e-06, 6.8279e-06,\n",
      "         6.2110e-06, 5.5271e-06, 6.1404e-06, 8.4559e-06, 7.7179e-06, 5.5644e-06,\n",
      "         3.0502e-01],\n",
      "        [1.1988e-01, 6.5334e-02, 2.7404e-05, 4.4968e-03, 5.6457e-02, 3.7018e-02,\n",
      "         4.6833e-03, 6.4050e-05, 8.8399e-06, 5.4349e-03, 5.0851e-04, 4.2589e-01,\n",
      "         1.5608e-01, 9.3585e-05, 1.3171e-05, 1.0319e-04, 1.0123e-05, 1.5939e-05,\n",
      "         1.3103e-05, 1.5262e-05, 1.1499e-05, 1.2738e-05, 1.2552e-05, 1.1983e-05,\n",
      "         1.0917e-05, 1.2173e-05, 1.0585e-05, 1.2902e-05, 1.3176e-05, 9.2890e-06,\n",
      "         1.2374e-01],\n",
      "        [1.6719e-01, 2.2533e-01, 3.3943e-05, 4.1763e-03, 3.7548e-02, 3.8579e-02,\n",
      "         1.2243e-02, 5.5035e-04, 5.2256e-06, 4.0153e-03, 9.0029e-04, 1.5237e-01,\n",
      "         3.6511e-02, 3.7550e-05, 1.0889e-05, 1.1653e-04, 5.9956e-06, 1.1565e-05,\n",
      "         8.8492e-06, 9.0237e-06, 7.0115e-06, 1.0290e-05, 9.0653e-06, 8.0994e-06,\n",
      "         7.3755e-06, 6.6478e-06, 7.2248e-06, 1.0125e-05, 9.1191e-06, 6.6735e-06,\n",
      "         3.2027e-01],\n",
      "        [1.7409e-01, 2.1515e-01, 3.8093e-05, 3.9734e-03, 3.4165e-02, 3.9053e-02,\n",
      "         1.0238e-02, 3.4751e-04, 4.3231e-06, 4.2907e-03, 7.7639e-04, 1.7578e-01,\n",
      "         4.2173e-02, 3.8720e-05, 9.0013e-06, 1.0588e-04, 5.1468e-06, 9.8900e-06,\n",
      "         7.4847e-06, 7.7891e-06, 6.0579e-06, 8.7780e-06, 7.3395e-06, 6.9704e-06,\n",
      "         6.1291e-06, 5.7135e-06, 6.3529e-06, 8.6268e-06, 7.8164e-06, 5.5963e-06,\n",
      "         2.9966e-01],\n",
      "        [1.8783e-01, 9.5403e-02, 5.6551e-05, 5.2278e-03, 4.4739e-02, 7.4191e-02,\n",
      "         3.9879e-03, 5.2302e-05, 1.1342e-05, 1.0153e-02, 4.8432e-04, 2.8603e-01,\n",
      "         8.3050e-02, 4.4656e-05, 1.6673e-05, 9.5999e-05, 1.2296e-05, 1.7390e-05,\n",
      "         1.6520e-05, 1.9959e-05, 1.4737e-05, 1.7721e-05, 1.3933e-05, 1.4525e-05,\n",
      "         1.3177e-05, 1.4511e-05, 1.5268e-05, 1.8080e-05, 1.7858e-05, 1.1746e-05,\n",
      "         2.0841e-01],\n",
      "        [1.4179e-01, 6.6771e-02, 4.1830e-05, 5.5351e-03, 5.2944e-02, 6.1744e-02,\n",
      "         3.2514e-03, 4.0428e-05, 1.1395e-05, 8.1000e-03, 3.9592e-04, 3.9524e-01,\n",
      "         1.2322e-01, 6.4475e-05, 1.5719e-05, 9.1172e-05, 1.2871e-05, 1.7100e-05,\n",
      "         1.6402e-05, 2.0235e-05, 1.4653e-05, 1.6180e-05, 1.3949e-05, 1.4256e-05,\n",
      "         1.2997e-05, 1.5025e-05, 1.4907e-05, 1.7425e-05, 1.7230e-05, 1.0911e-05,\n",
      "         1.4052e-01],\n",
      "        [2.8906e-01, 1.5023e-01, 1.0434e-04, 2.1998e-03, 1.5496e-02, 4.6934e-02,\n",
      "         4.8208e-03, 5.8212e-05, 9.2526e-06, 1.0014e-02, 7.7249e-04, 7.5521e-02,\n",
      "         3.5311e-02, 2.0936e-05, 1.4393e-05, 1.0175e-04, 8.9966e-06, 1.2916e-05,\n",
      "         1.2795e-05, 1.4489e-05, 1.2186e-05, 1.5775e-05, 1.1403e-05, 1.3543e-05,\n",
      "         9.9254e-06, 1.0219e-05, 1.2873e-05, 1.4676e-05, 1.3823e-05, 1.0553e-05,\n",
      "         3.6915e-01],\n",
      "        [3.2369e-01, 7.3324e-02, 1.4271e-04, 2.6141e-03, 2.5042e-02, 8.2644e-02,\n",
      "         3.2550e-03, 2.1247e-05, 3.7996e-05, 2.8243e-02, 1.0054e-03, 7.8565e-02,\n",
      "         4.9965e-02, 2.7307e-05, 4.8727e-05, 1.3011e-04, 3.1200e-05, 3.8129e-05,\n",
      "         4.4752e-05, 4.8992e-05, 4.1862e-05, 4.9356e-05, 3.5422e-05, 4.2083e-05,\n",
      "         3.5288e-05, 3.3728e-05, 4.3993e-05, 4.2811e-05, 4.3226e-05, 3.9503e-05,\n",
      "         3.3068e-01],\n",
      "        [2.0015e-01, 5.8434e-02, 6.9832e-05, 4.9984e-03, 5.0518e-02, 8.8014e-02,\n",
      "         3.1061e-03, 2.3451e-05, 2.5441e-05, 1.7575e-02, 6.0443e-04, 2.6933e-01,\n",
      "         1.2403e-01, 5.7700e-05, 3.1751e-05, 1.1896e-04, 2.3902e-05, 3.1647e-05,\n",
      "         3.1192e-05, 3.7190e-05, 2.8929e-05, 3.2807e-05, 2.6251e-05, 2.8118e-05,\n",
      "         2.6267e-05, 2.6556e-05, 2.9545e-05, 3.0034e-05, 3.4504e-05, 2.3677e-05,\n",
      "         1.8250e-01],\n",
      "        [3.3455e-01, 9.5834e-02, 1.5190e-04, 1.8085e-03, 1.3772e-02, 6.1871e-02,\n",
      "         3.2404e-03, 2.3078e-05, 2.0350e-05, 2.0521e-02, 9.4423e-04, 5.1614e-02,\n",
      "         3.3010e-02, 1.6995e-05, 2.8268e-05, 1.1000e-04, 1.7338e-05, 2.3814e-05,\n",
      "         2.4605e-05, 2.8141e-05, 2.4972e-05, 3.0113e-05, 2.0022e-05, 2.6065e-05,\n",
      "         1.9472e-05, 1.8849e-05, 2.5894e-05, 2.5010e-05, 2.6840e-05, 2.2794e-05,\n",
      "         3.8215e-01],\n",
      "        [3.2037e-01, 6.4891e-02, 1.3774e-04, 2.7722e-03, 2.9042e-02, 8.4273e-02,\n",
      "         3.3420e-03, 2.0047e-05, 5.1757e-05, 3.1544e-02, 1.1090e-03, 8.0954e-02,\n",
      "         5.5629e-02, 3.0445e-05, 6.3097e-05, 1.4158e-04, 4.1443e-05, 4.8572e-05,\n",
      "         5.9128e-05, 6.3456e-05, 5.4609e-05, 6.3483e-05, 4.7523e-05, 5.4797e-05,\n",
      "         4.6806e-05, 4.4894e-05, 5.6241e-05, 5.5324e-05, 5.5121e-05, 5.2472e-05,\n",
      "         3.2489e-01],\n",
      "        [3.1957e-01, 6.4462e-02, 1.3754e-04, 2.7923e-03, 2.9535e-02, 8.3489e-02,\n",
      "         3.3567e-03, 1.9765e-05, 5.2019e-05, 3.1419e-02, 1.1167e-03, 8.3423e-02,\n",
      "         5.8169e-02, 3.1842e-05, 6.3223e-05, 1.4321e-04, 4.1656e-05, 4.8884e-05,\n",
      "         5.9213e-05, 6.3764e-05, 5.4893e-05, 6.3536e-05, 4.7855e-05, 5.5302e-05,\n",
      "         4.6950e-05, 4.5180e-05, 5.6392e-05, 5.5451e-05, 5.5455e-05, 5.2505e-05,\n",
      "         3.2147e-01],\n",
      "        [2.7578e-01, 5.1019e-02, 1.2229e-04, 3.8588e-03, 4.1545e-02, 1.1684e-01,\n",
      "         2.8334e-03, 1.5788e-05, 6.1910e-05, 3.6671e-02, 9.3253e-04, 1.4037e-01,\n",
      "         9.9820e-02, 4.8637e-05, 7.1187e-05, 1.5604e-04, 4.8478e-05, 5.8549e-05,\n",
      "         6.6648e-05, 7.1099e-05, 6.0403e-05, 7.1137e-05, 5.2385e-05, 5.8250e-05,\n",
      "         5.6712e-05, 4.8138e-05, 6.5049e-05, 5.5300e-05, 6.7759e-05, 5.6057e-05,\n",
      "         2.2902e-01],\n",
      "        [2.5511e-01, 4.6581e-02, 1.1331e-04, 4.0659e-03, 4.4498e-02, 1.1462e-01,\n",
      "         2.7119e-03, 1.4033e-05, 6.1419e-05, 3.5089e-02, 8.9878e-04, 1.6773e-01,\n",
      "         1.2768e-01, 5.9329e-05, 6.9112e-05, 1.6137e-04, 4.8107e-05, 5.9646e-05,\n",
      "         6.4299e-05, 6.9600e-05, 5.9368e-05, 6.9000e-05, 5.1725e-05, 5.7964e-05,\n",
      "         5.6334e-05, 4.6872e-05, 6.3457e-05, 5.2071e-05, 6.9105e-05, 5.3479e-05,\n",
      "         1.9971e-01],\n",
      "        [2.1364e-01, 4.5888e-02, 8.5481e-05, 4.3837e-03, 4.9742e-02, 9.4473e-02,\n",
      "         2.8764e-03, 1.4701e-05, 4.7705e-05, 2.6231e-02, 8.0057e-04, 2.2901e-01,\n",
      "         1.5850e-01, 6.9906e-05, 5.3682e-05, 1.4968e-04, 3.8967e-05, 5.2379e-05,\n",
      "         5.0396e-05, 5.7587e-05, 4.8652e-05, 5.5107e-05, 4.3647e-05, 4.8607e-05,\n",
      "         4.5309e-05, 4.0956e-05, 4.8850e-05, 4.3306e-05, 5.9374e-05, 4.1770e-05,\n",
      "         1.7336e-01],\n",
      "        [2.7048e-01, 4.9489e-02, 1.1950e-04, 3.8637e-03, 4.2177e-02, 1.1065e-01,\n",
      "         2.8667e-03, 1.4985e-05, 6.3998e-05, 3.5682e-02, 9.6043e-04, 1.4807e-01,\n",
      "         1.1240e-01, 5.3954e-05, 7.2523e-05, 1.6177e-04, 5.0309e-05, 6.1090e-05,\n",
      "         6.7730e-05, 7.3044e-05, 6.2441e-05, 7.2641e-05, 5.4667e-05, 6.1177e-05,\n",
      "         5.8291e-05, 4.9752e-05, 6.6336e-05, 5.6024e-05, 7.0389e-05, 5.7101e-05,\n",
      "         2.2201e-01],\n",
      "        [2.6608e-01, 4.8308e-02, 1.1669e-04, 3.5917e-03, 4.1601e-02, 8.6213e-02,\n",
      "         3.1379e-03, 1.3867e-05, 6.8523e-05, 3.1724e-02, 1.1178e-03, 1.5424e-01,\n",
      "         1.4249e-01, 6.8757e-05, 7.5090e-05, 1.7956e-04, 5.4369e-05, 6.6413e-05,\n",
      "         6.9795e-05, 7.7252e-05, 6.8025e-05, 7.5780e-05, 6.1132e-05, 7.0748e-05,\n",
      "         6.0807e-05, 5.4507e-05, 6.8476e-05, 5.9226e-05, 7.5617e-05, 6.0062e-05,\n",
      "         2.2006e-01],\n",
      "        [2.4161e-01, 4.3574e-02, 1.0576e-04, 3.7592e-03, 4.4003e-02, 8.5139e-02,\n",
      "         2.9225e-03, 1.2275e-05, 6.5218e-05, 2.9953e-02, 1.0360e-03, 1.8260e-01,\n",
      "         1.7596e-01, 8.0466e-05, 7.0333e-05, 1.7986e-04, 5.1775e-05, 6.5039e-05,\n",
      "         6.4995e-05, 7.2896e-05, 6.4177e-05, 7.0780e-05, 5.7741e-05, 6.7090e-05,\n",
      "         5.8249e-05, 5.0910e-05, 6.4497e-05, 5.3807e-05, 7.4342e-05, 5.5024e-05,\n",
      "         1.8806e-01],\n",
      "        [2.7635e-01, 5.0651e-02, 1.2090e-04, 3.5297e-03, 4.0420e-02, 8.8230e-02,\n",
      "         3.1993e-03, 1.4794e-05, 6.8357e-05, 3.2547e-02, 1.1272e-03, 1.4201e-01,\n",
      "         1.2504e-01, 6.1730e-05, 7.5864e-05, 1.7555e-04, 5.4246e-05, 6.5498e-05,\n",
      "         7.0759e-05, 7.7746e-05, 6.8155e-05, 7.6586e-05, 6.1080e-05, 7.0265e-05,\n",
      "         6.0749e-05, 5.4950e-05, 6.8980e-05, 6.0727e-05, 7.4498e-05, 6.1170e-05,\n",
      "         2.3545e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  1, 11,  0, 12, 30,  1, 30,  5, 11, 30, 12,  0,  1, 30, 11,  0,  5,\n",
      "        12,  4, 30,  1, 30,  9, 11, 30,  0, 12, 30,  0], device='cuda:0')\n",
      "2.1829044818878174\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [2.4547e-03, 9.4002e-01, 3.3599e-05, 9.8809e-05, 1.3867e-03, 5.2878e-04,\n",
      "         1.0537e-03, 2.7415e-02, 4.3069e-08, 9.1372e-05, 9.9270e-05, 2.4869e-02,\n",
      "         1.1679e-03, 6.4271e-04, 2.0907e-07, 1.9185e-05, 5.1691e-08, 1.3454e-07,\n",
      "         1.8481e-07, 7.9795e-08, 1.0870e-07, 1.5246e-07, 7.5327e-08, 1.2554e-07,\n",
      "         1.7551e-07, 4.6661e-08, 5.5839e-08, 9.2176e-08, 1.9659e-07, 1.2766e-07,\n",
      "         1.2040e-04],\n",
      "        [2.5866e-02, 6.7022e-02, 3.9054e-06, 6.8777e-04, 9.1918e-03, 7.3870e-04,\n",
      "         2.0114e-01, 5.4162e-01, 7.8470e-07, 1.0989e-04, 4.7152e-03, 1.5065e-03,\n",
      "         4.6204e-03, 1.9088e-05, 1.9332e-06, 2.4344e-04, 9.9537e-07, 2.5808e-06,\n",
      "         2.5712e-06, 1.4596e-06, 1.4333e-06, 2.7103e-06, 1.0862e-06, 1.2999e-06,\n",
      "         2.4442e-06, 8.9137e-07, 1.0028e-06, 1.7215e-06, 1.3316e-06, 1.6336e-06,\n",
      "         1.4249e-01],\n",
      "        [6.5704e-04, 5.1672e-03, 1.3873e-06, 1.2162e-03, 2.6006e-02, 1.4057e-04,\n",
      "         1.4255e-02, 4.9081e-04, 3.1768e-07, 6.1022e-05, 6.1899e-04, 7.8020e-01,\n",
      "         1.5715e-01, 1.3559e-02, 1.0509e-06, 5.1987e-05, 5.8156e-07, 1.0268e-06,\n",
      "         7.5830e-07, 6.7484e-07, 4.7586e-07, 5.1100e-07, 6.1581e-07, 4.4257e-07,\n",
      "         9.8682e-07, 4.8803e-07, 2.3858e-07, 8.5630e-07, 6.3210e-07, 5.0568e-07,\n",
      "         4.1677e-04],\n",
      "        [6.5007e-02, 1.0392e-01, 4.7687e-06, 3.0331e-03, 2.5791e-02, 1.2809e-03,\n",
      "         2.4376e-01, 3.2681e-02, 1.7500e-06, 2.1977e-04, 7.1572e-03, 1.3958e-02,\n",
      "         2.9007e-02, 1.0787e-04, 4.6327e-06, 3.4292e-04, 2.3509e-06, 5.7999e-06,\n",
      "         3.1594e-06, 2.6640e-06, 2.3457e-06, 4.3845e-06, 4.3259e-06, 3.3151e-06,\n",
      "         3.0787e-06, 2.6375e-06, 3.4019e-06, 4.6182e-06, 3.8109e-06, 3.0973e-06,\n",
      "         4.7367e-01],\n",
      "        [4.2119e-02, 5.0868e-02, 7.0078e-06, 1.4812e-02, 2.1107e-01, 6.8656e-03,\n",
      "         1.3484e-01, 9.1165e-03, 2.4025e-05, 1.1109e-03, 5.2661e-03, 1.0325e-01,\n",
      "         4.6721e-02, 5.4441e-04, 4.4701e-05, 4.0228e-04, 3.1511e-05, 5.5739e-05,\n",
      "         3.5334e-05, 4.6290e-05, 3.2272e-05, 4.2960e-05, 3.9047e-05, 3.2292e-05,\n",
      "         3.7652e-05, 3.3564e-05, 3.9517e-05, 3.9748e-05, 4.3304e-05, 2.9676e-05,\n",
      "         3.7240e-01],\n",
      "        [4.2978e-02, 2.6435e-02, 5.6543e-06, 1.8300e-02, 2.3939e-01, 6.6835e-03,\n",
      "         2.9194e-02, 3.1924e-04, 8.8648e-06, 1.3714e-03, 2.6698e-03, 2.0879e-01,\n",
      "         2.8820e-01, 4.4257e-04, 1.7708e-05, 2.0633e-04, 1.4262e-05, 3.0036e-05,\n",
      "         1.4019e-05, 2.2995e-05, 1.3145e-05, 1.3938e-05, 1.3315e-05, 1.3954e-05,\n",
      "         1.7530e-05, 1.4067e-05, 1.6097e-05, 2.2338e-05, 2.0729e-05, 1.6385e-05,\n",
      "         1.3475e-01],\n",
      "        [1.1297e-01, 1.8549e-01, 1.4223e-05, 6.7625e-03, 7.2854e-02, 4.7007e-03,\n",
      "         6.0970e-02, 7.4545e-04, 2.9936e-06, 9.4150e-04, 3.6953e-03, 8.3496e-02,\n",
      "         6.4237e-02, 1.8606e-04, 7.6174e-06, 1.9321e-04, 4.5919e-06, 1.0868e-05,\n",
      "         4.9071e-06, 7.6382e-06, 4.4598e-06, 6.6310e-06, 6.3124e-06, 6.5148e-06,\n",
      "         5.9244e-06, 4.9457e-06, 5.8123e-06, 8.9363e-06, 6.8758e-06, 5.9189e-06,\n",
      "         4.0264e-01],\n",
      "        [1.2824e-01, 1.7247e-01, 1.7614e-05, 6.8124e-03, 6.8015e-02, 6.1021e-03,\n",
      "         4.4844e-02, 3.7748e-04, 2.6153e-06, 1.2230e-03, 3.0887e-03, 1.1483e-01,\n",
      "         9.5772e-02, 1.9814e-04, 6.4766e-06, 1.8467e-04, 3.8411e-06, 9.9252e-06,\n",
      "         4.3790e-06, 6.9211e-06, 3.8818e-06, 5.6155e-06, 5.2369e-06, 5.4521e-06,\n",
      "         5.0004e-06, 4.4134e-06, 5.3367e-06, 7.4610e-06, 6.1270e-06, 4.9959e-06,\n",
      "         3.5773e-01],\n",
      "        [1.2529e-01, 4.6023e-02, 3.0259e-05, 5.6954e-03, 6.9801e-02, 3.5667e-02,\n",
      "         5.1276e-03, 2.4097e-05, 2.4907e-05, 8.1136e-03, 8.3964e-04, 3.3100e-01,\n",
      "         2.3933e-01, 1.2862e-04, 2.9672e-05, 1.4832e-04, 2.3432e-05, 3.9579e-05,\n",
      "         2.8259e-05, 4.0018e-05, 2.7780e-05, 3.1134e-05, 3.1051e-05, 3.1248e-05,\n",
      "         2.7109e-05, 3.0687e-05, 2.6513e-05, 2.9183e-05, 3.9045e-05, 2.2827e-05,\n",
      "         1.3230e-01],\n",
      "        [1.1711e-01, 5.4745e-02, 2.6073e-05, 5.4367e-03, 6.0713e-02, 1.8773e-02,\n",
      "         6.0396e-03, 2.7293e-05, 7.2379e-06, 3.7354e-03, 7.4187e-04, 3.4747e-01,\n",
      "         2.5760e-01, 1.6167e-04, 9.6764e-06, 1.1509e-04, 8.5454e-06, 1.3270e-05,\n",
      "         9.1808e-06, 1.3158e-05, 8.7787e-06, 9.5160e-06, 1.0015e-05, 9.9392e-06,\n",
      "         8.8296e-06, 9.7234e-06, 9.8510e-06, 1.1143e-05, 1.1586e-05, 7.3167e-06,\n",
      "         1.2714e-01],\n",
      "        [1.2958e-01, 4.7053e-02, 3.5720e-05, 4.6606e-03, 5.2907e-02, 3.4731e-02,\n",
      "         3.8994e-03, 1.7671e-05, 1.9676e-05, 8.0536e-03, 6.5108e-04, 3.5331e-01,\n",
      "         2.3843e-01, 1.1114e-04, 2.2936e-05, 1.2413e-04, 1.9971e-05, 2.9970e-05,\n",
      "         2.2772e-05, 3.1027e-05, 2.3596e-05, 2.5326e-05, 2.4431e-05, 2.6506e-05,\n",
      "         2.0833e-05, 2.4495e-05, 2.1726e-05, 2.4155e-05, 3.0779e-05, 1.7710e-05,\n",
      "         1.2605e-01],\n",
      "        [1.6184e-01, 5.7401e-02, 4.0302e-05, 4.9074e-03, 5.1414e-02, 2.0267e-02,\n",
      "         6.1296e-03, 1.4549e-05, 9.9492e-06, 5.7979e-03, 9.9961e-04, 2.4606e-01,\n",
      "         2.9199e-01, 1.7896e-04, 1.2014e-05, 1.4980e-04, 1.0847e-05, 1.5892e-05,\n",
      "         1.1639e-05, 1.6061e-05, 1.1044e-05, 1.3128e-05, 1.2143e-05, 1.3080e-05,\n",
      "         1.1428e-05, 1.1477e-05, 1.2986e-05, 1.3056e-05, 1.5295e-05, 9.7859e-06,\n",
      "         1.5260e-01],\n",
      "        [1.3675e-01, 6.4354e-02, 2.6212e-05, 5.2662e-03, 5.1804e-02, 1.0051e-02,\n",
      "         8.3977e-03, 1.5432e-05, 2.5940e-06, 2.4915e-03, 9.0313e-04, 2.6160e-01,\n",
      "         3.1731e-01, 2.2363e-04, 3.4926e-06, 1.1428e-04, 3.4316e-06, 5.3288e-06,\n",
      "         3.3450e-06, 4.8621e-06, 3.0349e-06, 3.7949e-06, 3.6176e-06, 3.6277e-06,\n",
      "         3.5409e-06, 3.4291e-06, 4.1553e-06, 4.4271e-06, 4.4676e-06, 2.9484e-06,\n",
      "         1.4063e-01],\n",
      "        [1.9968e-01, 5.2950e-02, 7.2939e-05, 3.9570e-03, 4.4080e-02, 3.8440e-02,\n",
      "         4.1151e-03, 1.5157e-05, 3.2071e-05, 1.2548e-02, 1.0638e-03, 2.2275e-01,\n",
      "         2.4723e-01, 1.3654e-04, 3.5962e-05, 1.7923e-04, 3.0655e-05, 3.8452e-05,\n",
      "         3.4659e-05, 4.3229e-05, 3.5082e-05, 3.7223e-05, 3.4449e-05, 4.0713e-05,\n",
      "         3.0602e-05, 3.1367e-05, 3.5954e-05, 3.3975e-05, 4.0831e-05, 2.7802e-05,\n",
      "         1.7221e-01],\n",
      "        [1.9959e-01, 5.4102e-02, 7.2079e-05, 3.9773e-03, 4.3770e-02, 3.5441e-02,\n",
      "         4.2856e-03, 1.5276e-05, 2.7970e-05, 1.1483e-02, 1.0759e-03, 2.2044e-01,\n",
      "         2.5232e-01, 1.4428e-04, 3.1688e-05, 1.7785e-04, 2.7402e-05, 3.4149e-05,\n",
      "         3.0526e-05, 3.8137e-05, 3.0677e-05, 3.2701e-05, 3.0329e-05, 3.5752e-05,\n",
      "         2.7063e-05, 2.7519e-05, 3.2226e-05, 3.0399e-05, 3.5657e-05, 2.4578e-05,\n",
      "         1.7261e-01],\n",
      "        [2.3178e-01, 5.6417e-02, 9.1066e-05, 3.5740e-03, 4.0166e-02, 3.7233e-02,\n",
      "         4.4127e-03, 1.4980e-05, 3.5148e-05, 1.3978e-02, 1.2876e-03, 1.7960e-01,\n",
      "         2.2914e-01, 1.3616e-04, 3.9379e-05, 1.9483e-04, 3.3186e-05, 3.9930e-05,\n",
      "         3.7332e-05, 4.5135e-05, 3.7854e-05, 4.0197e-05, 3.6803e-05, 4.4512e-05,\n",
      "         3.2690e-05, 3.2598e-05, 3.9631e-05, 3.6637e-05, 4.1891e-05, 3.1300e-05,\n",
      "         2.0137e-01],\n",
      "        [1.3494e-01, 6.8580e-02, 2.3617e-05, 5.4822e-03, 5.2874e-02, 8.5188e-03,\n",
      "         9.3477e-03, 1.5966e-05, 2.1070e-06, 2.1253e-03, 9.3774e-04, 2.5310e-01,\n",
      "         3.2371e-01, 2.3899e-04, 2.9370e-06, 1.1466e-04, 2.8319e-06, 4.4726e-06,\n",
      "         2.8118e-06, 4.1957e-06, 2.4574e-06, 3.1253e-06, 2.9971e-06, 2.9726e-06,\n",
      "         2.9773e-06, 2.8399e-06, 3.5847e-06, 3.8092e-06, 3.7470e-06, 2.4102e-06,\n",
      "         1.3994e-01],\n",
      "        [1.3179e-01, 6.9851e-02, 2.2311e-05, 5.5563e-03, 5.4091e-02, 8.0554e-03,\n",
      "         9.8108e-03, 1.7570e-05, 1.8857e-06, 1.9784e-03, 9.5173e-04, 2.5655e-01,\n",
      "         3.2155e-01, 2.4262e-04, 2.7243e-06, 1.1285e-04, 2.5720e-06, 4.1431e-06,\n",
      "         2.5850e-06, 3.8398e-06, 2.2166e-06, 2.8167e-06, 2.7317e-06, 2.6753e-06,\n",
      "         2.7265e-06, 2.5736e-06, 3.3083e-06, 3.5365e-06, 3.3887e-06, 2.1985e-06,\n",
      "         1.3938e-01],\n",
      "        [1.3458e-01, 6.9832e-02, 2.3640e-05, 5.4202e-03, 5.2378e-02, 8.5868e-03,\n",
      "         9.2190e-03, 1.6717e-05, 2.1001e-06, 2.1158e-03, 9.1831e-04, 2.5911e-01,\n",
      "         3.1715e-01, 2.3216e-04, 2.9613e-06, 1.1267e-04, 2.8385e-06, 4.4622e-06,\n",
      "         2.8352e-06, 4.2216e-06, 2.4829e-06, 3.1196e-06, 3.0110e-06, 2.9985e-06,\n",
      "         2.9643e-06, 2.8557e-06, 3.6019e-06, 3.8608e-06, 3.7377e-06, 2.3973e-06,\n",
      "         1.4026e-01],\n",
      "        [1.3918e-01, 6.8653e-02, 2.5256e-05, 5.5223e-03, 5.2145e-02, 8.1385e-03,\n",
      "         9.9426e-03, 1.5161e-05, 2.0509e-06, 2.0782e-03, 1.0129e-03, 2.3304e-01,\n",
      "         3.3531e-01, 2.6925e-04, 2.7944e-06, 1.2226e-04, 2.8035e-06, 4.3873e-06,\n",
      "         2.6941e-06, 3.9670e-06, 2.3477e-06, 3.0876e-06, 2.8949e-06, 2.8859e-06,\n",
      "         2.8884e-06, 2.7360e-06, 3.4635e-06, 3.6375e-06, 3.5800e-06, 2.4021e-06,\n",
      "         1.4450e-01],\n",
      "        [1.3477e-01, 6.9076e-02, 2.3614e-05, 5.5702e-03, 5.2686e-02, 7.8730e-03,\n",
      "         9.9230e-03, 1.5665e-05, 1.8844e-06, 1.9806e-03, 9.8001e-04, 2.4176e-01,\n",
      "         3.3438e-01, 2.6204e-04, 2.6342e-06, 1.1765e-04, 2.5865e-06, 4.1036e-06,\n",
      "         2.5277e-06, 3.7423e-06, 2.1744e-06, 2.8373e-06, 2.6906e-06, 2.6577e-06,\n",
      "         2.6993e-06, 2.5429e-06, 3.2594e-06, 3.4369e-06, 3.3564e-06, 2.2037e-06,\n",
      "         1.4055e-01],\n",
      "        [1.4359e-01, 7.1628e-02, 2.6049e-05, 5.4545e-03, 5.1472e-02, 8.0742e-03,\n",
      "         1.0196e-02, 1.5750e-05, 2.0903e-06, 2.0995e-03, 1.0434e-03, 2.2838e-01,\n",
      "         3.2680e-01, 2.6420e-04, 2.8668e-06, 1.2358e-04, 2.8565e-06, 4.4421e-06,\n",
      "         2.7627e-06, 4.0757e-06, 2.4113e-06, 3.1531e-06, 2.9662e-06, 2.9737e-06,\n",
      "         2.9340e-06, 2.8073e-06, 3.5542e-06, 3.7652e-06, 3.6366e-06, 2.4568e-06,\n",
      "         1.5079e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  1,  0, 11,  5,  0, 12,  6,  1, 30, 11, 30,  3, 12,  0,  1, 30, 30,\n",
      "        11,  0,  5, 12, 30, 30], device='cuda:0')\n",
      "1.8501349687576294\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [9.0013e-04, 4.4205e-01, 6.7229e-05, 1.7846e-04, 2.5375e-03, 6.9167e-04,\n",
      "         1.6774e-03, 5.3897e-01, 5.1714e-07, 1.0658e-04, 1.0668e-04, 1.0945e-02,\n",
      "         3.4268e-04, 1.3160e-03, 1.6047e-06, 3.6686e-05, 3.5494e-07, 8.4906e-07,\n",
      "         1.4017e-06, 7.0850e-07, 7.9480e-07, 1.0208e-06, 5.7886e-07, 1.1001e-06,\n",
      "         1.4076e-06, 3.9345e-07, 4.8089e-07, 6.6530e-07, 1.2837e-06, 1.0013e-06,\n",
      "         6.1283e-05],\n",
      "        [5.9250e-04, 3.6379e-03, 3.4999e-06, 1.9498e-04, 1.2866e-03, 1.2993e-04,\n",
      "         2.2942e-04, 3.9711e-05, 9.7170e-08, 5.2562e-05, 5.6330e-05, 9.6628e-01,\n",
      "         1.2281e-02, 1.5169e-02, 4.0017e-07, 2.3213e-05, 1.9454e-07, 2.9911e-07,\n",
      "         2.5508e-07, 3.0269e-07, 1.7882e-07, 1.4233e-07, 1.9442e-07, 2.3658e-07,\n",
      "         2.6236e-07, 1.8795e-07, 9.2634e-08, 2.3191e-07, 3.0211e-07, 2.2653e-07,\n",
      "         2.3871e-05],\n",
      "        [1.9694e-01, 3.5901e-01, 1.6129e-05, 7.3818e-04, 8.8757e-03, 3.7094e-03,\n",
      "         4.1566e-02, 2.2220e-03, 2.4000e-07, 5.9198e-04, 1.3273e-03, 2.1231e-02,\n",
      "         1.4162e-02, 1.8321e-05, 6.6776e-07, 9.0659e-05, 2.4170e-07, 5.8612e-07,\n",
      "         5.0279e-07, 3.1338e-07, 2.6735e-07, 6.4504e-07, 6.3383e-07, 4.2032e-07,\n",
      "         4.5138e-07, 2.7068e-07, 3.3177e-07, 5.6554e-07, 3.3138e-07, 4.4855e-07,\n",
      "         3.4949e-01],\n",
      "        [4.6498e-02, 2.9503e-02, 3.2512e-04, 7.9645e-03, 5.6235e-02, 1.0152e-02,\n",
      "         1.4021e-02, 1.7163e-05, 2.7838e-05, 4.0700e-03, 1.7155e-03, 7.1655e-02,\n",
      "         8.5220e-02, 1.3208e-03, 2.1520e-05, 4.5751e-04, 2.1692e-05, 5.9659e-05,\n",
      "         2.5554e-05, 3.2195e-05, 2.1546e-05, 2.8212e-05, 3.4677e-05, 2.4181e-05,\n",
      "         2.8046e-05, 3.8441e-05, 2.6241e-05, 2.5081e-05, 4.5152e-05, 3.6591e-05,\n",
      "         6.7035e-01],\n",
      "        [7.9087e-03, 6.2313e-03, 2.0456e-05, 5.2263e-03, 2.3685e-02, 1.5485e-03,\n",
      "         8.2323e-03, 5.9176e-06, 1.2380e-06, 5.3467e-04, 6.7205e-04, 1.5380e-01,\n",
      "         7.4933e-01, 1.7510e-03, 1.5735e-06, 8.9602e-05, 2.5345e-06, 3.6936e-06,\n",
      "         2.0957e-06, 2.0744e-06, 1.6467e-06, 1.8389e-06, 1.8004e-06, 1.2592e-06,\n",
      "         2.5541e-06, 2.4792e-06, 1.6791e-06, 2.5779e-06, 2.9154e-06, 2.1165e-06,\n",
      "         4.0930e-02],\n",
      "        [2.2713e-01, 1.8134e-01, 5.3509e-05, 2.6168e-03, 1.7291e-02, 4.9476e-03,\n",
      "         1.4621e-02, 2.3012e-05, 9.7610e-07, 1.6841e-03, 1.5149e-03, 9.0843e-02,\n",
      "         1.2226e-01, 1.2489e-04, 1.8210e-06, 1.1161e-04, 1.4364e-06, 2.8369e-06,\n",
      "         1.5085e-06, 2.4095e-06, 1.4835e-06, 2.0313e-06, 1.6291e-06, 2.1040e-06,\n",
      "         1.4501e-06, 1.6257e-06, 2.2295e-06, 2.5228e-06, 2.1008e-06, 1.5727e-06,\n",
      "         3.3541e-01],\n",
      "        [2.1443e-01, 1.7763e-01, 4.7795e-05, 2.8680e-03, 1.8664e-02, 5.0192e-03,\n",
      "         1.4188e-02, 2.1960e-05, 8.4638e-07, 1.5616e-03, 1.3202e-03, 1.1208e-01,\n",
      "         1.3663e-01, 1.2541e-04, 1.5632e-06, 1.0244e-04, 1.2723e-06, 2.5630e-06,\n",
      "         1.3106e-06, 2.1803e-06, 1.3177e-06, 1.7665e-06, 1.4432e-06, 1.8250e-06,\n",
      "         1.2831e-06, 1.4947e-06, 1.9436e-06, 2.2628e-06, 1.9162e-06, 1.3548e-06,\n",
      "         3.1528e-01],\n",
      "        [2.4031e-01, 4.3475e-02, 1.0628e-04, 3.5633e-03, 4.2841e-02, 7.3821e-02,\n",
      "         3.1206e-03, 1.1886e-05, 7.1898e-05, 2.8385e-02, 1.1455e-03, 1.7836e-01,\n",
      "         1.9246e-01, 9.0167e-05, 7.5785e-05, 1.9169e-04, 5.7321e-05, 7.1648e-05,\n",
      "         6.9915e-05, 7.9101e-05, 7.1219e-05, 7.7086e-05, 6.5179e-05, 7.7009e-05,\n",
      "         6.3002e-05, 5.6790e-05, 6.9493e-05, 5.8826e-05, 8.1027e-05, 6.0474e-05,\n",
      "         1.9102e-01],\n",
      "        [1.3037e-01, 3.9940e-02, 3.7716e-05, 4.3747e-03, 5.3021e-02, 4.3187e-02,\n",
      "         3.4198e-03, 1.4061e-05, 2.4978e-05, 1.0712e-02, 6.3097e-04, 3.5737e-01,\n",
      "         2.3086e-01, 9.1862e-05, 2.7590e-05, 1.1731e-04, 2.3566e-05, 3.7360e-05,\n",
      "         2.6683e-05, 3.5693e-05, 2.9325e-05, 3.1439e-05, 2.9583e-05, 3.1843e-05,\n",
      "         2.5810e-05, 2.9654e-05, 2.4863e-05, 2.7317e-05, 3.8686e-05, 2.2846e-05,\n",
      "         1.2539e-01],\n",
      "        [1.2205e-01, 4.1476e-02, 3.5541e-05, 4.1439e-03, 4.9977e-02, 3.4511e-02,\n",
      "         3.5106e-03, 1.5535e-05, 1.5279e-05, 7.9350e-03, 5.9898e-04, 3.8039e-01,\n",
      "         2.3482e-01, 1.0192e-04, 1.7892e-05, 1.0587e-04, 1.5988e-05, 2.4592e-05,\n",
      "         1.7377e-05, 2.2941e-05, 1.8698e-05, 1.9800e-05, 1.9095e-05, 2.0133e-05,\n",
      "         1.6775e-05, 1.9381e-05, 1.6636e-05, 1.8841e-05, 2.4331e-05, 1.4754e-05,\n",
      "         1.2003e-01],\n",
      "        [1.2291e-01, 4.2402e-02, 3.5957e-05, 4.3497e-03, 5.0445e-02, 3.8417e-02,\n",
      "         3.3014e-03, 1.6148e-05, 2.0514e-05, 8.6297e-03, 5.6503e-04, 3.8036e-01,\n",
      "         2.2976e-01, 9.6437e-05, 2.3392e-05, 1.1080e-04, 2.0632e-05, 3.0862e-05,\n",
      "         2.3095e-05, 3.1136e-05, 2.5039e-05, 2.5954e-05, 2.5161e-05, 2.7408e-05,\n",
      "         2.1325e-05, 2.5463e-05, 2.1755e-05, 2.4419e-05, 3.1823e-05, 1.8394e-05,\n",
      "         1.1820e-01],\n",
      "        [1.1577e-01, 4.5039e-02, 3.3414e-05, 4.3049e-03, 4.8401e-02, 3.1109e-02,\n",
      "         3.4245e-03, 1.8165e-05, 1.3951e-05, 6.4327e-03, 5.2725e-04, 3.9828e-01,\n",
      "         2.3293e-01, 1.0587e-04, 1.6632e-05, 1.0197e-04, 1.5251e-05, 2.2001e-05,\n",
      "         1.6529e-05, 2.2364e-05, 1.7636e-05, 1.7943e-05, 1.7902e-05, 1.9390e-05,\n",
      "         1.5027e-05, 1.8173e-05, 1.6063e-05, 1.8428e-05, 2.1841e-05, 1.2796e-05,\n",
      "         1.1324e-01],\n",
      "        [1.2264e-01, 4.2831e-02, 3.5823e-05, 4.3609e-03, 5.0255e-02, 3.8016e-02,\n",
      "         3.3043e-03, 1.6469e-05, 2.0245e-05, 8.4531e-03, 5.6112e-04, 3.8140e-01,\n",
      "         2.2952e-01, 9.7040e-05, 2.3170e-05, 1.1077e-04, 2.0504e-05, 3.0441e-05,\n",
      "         2.2938e-05, 3.0964e-05, 2.4806e-05, 2.5643e-05, 2.4937e-05, 2.7198e-05,\n",
      "         2.1054e-05, 2.5227e-05, 2.1626e-05, 2.4313e-05, 3.1389e-05, 1.8093e-05,\n",
      "         1.1801e-01],\n",
      "        [1.2119e-01, 4.3188e-02, 3.5476e-05, 4.3532e-03, 4.9787e-02, 3.7090e-02,\n",
      "         3.2886e-03, 1.6826e-05, 1.9360e-05, 8.0961e-03, 5.4995e-04, 3.8556e-01,\n",
      "         2.2950e-01, 9.8041e-05, 2.2275e-05, 1.0933e-04, 1.9867e-05, 2.9209e-05,\n",
      "         2.2122e-05, 2.9896e-05, 2.3891e-05, 2.4564e-05, 2.4009e-05, 2.6215e-05,\n",
      "         2.0182e-05, 2.4330e-05, 2.0919e-05, 2.3612e-05, 3.0028e-05, 1.7286e-05,\n",
      "         1.1676e-01],\n",
      "        [1.2401e-01, 4.1980e-02, 3.7231e-05, 4.2949e-03, 5.0412e-02, 4.1004e-02,\n",
      "         3.1920e-03, 1.6137e-05, 2.1839e-05, 9.2624e-03, 5.6131e-04, 3.8237e-01,\n",
      "         2.2364e-01, 9.2405e-05, 2.4848e-05, 1.1010e-04, 2.1606e-05, 3.2393e-05,\n",
      "         2.4442e-05, 3.2610e-05, 2.6539e-05, 2.7426e-05, 2.6420e-05, 2.8799e-05,\n",
      "         2.2534e-05, 2.6837e-05, 2.2852e-05, 2.5611e-05, 3.3648e-05, 1.9571e-05,\n",
      "         1.1861e-01],\n",
      "        [1.1465e-01, 4.3352e-02, 3.4977e-05, 4.2429e-03, 4.8470e-02, 3.6087e-02,\n",
      "         3.1144e-03, 1.8023e-05, 1.7786e-05, 7.4738e-03, 5.1041e-04, 4.0645e-01,\n",
      "         2.2466e-01, 9.8907e-05, 2.0773e-05, 1.0383e-04, 1.8648e-05, 2.6934e-05,\n",
      "         2.0771e-05, 2.7822e-05, 2.2306e-05, 2.2511e-05, 2.2248e-05, 2.4366e-05,\n",
      "         1.8572e-05, 2.2935e-05, 1.9557e-05, 2.2447e-05, 2.7602e-05, 1.5874e-05,\n",
      "         1.1039e-01],\n",
      "        [1.3692e-01, 4.1376e-02, 4.2463e-05, 4.2514e-03, 5.0746e-02, 4.5928e-02,\n",
      "         3.2144e-03, 1.4851e-05, 2.6721e-05, 1.1341e-02, 6.2988e-04, 3.5176e-01,\n",
      "         2.2553e-01, 9.1439e-05, 2.9832e-05, 1.2014e-04, 2.5305e-05, 3.7882e-05,\n",
      "         2.8953e-05, 3.7950e-05, 3.1418e-05, 3.2886e-05, 3.0993e-05, 3.3991e-05,\n",
      "         2.7031e-05, 3.0791e-05, 2.7167e-05, 2.9240e-05, 3.9915e-05, 2.3791e-05,\n",
      "         1.2754e-01],\n",
      "        [1.6144e-01, 4.0604e-02, 5.5946e-05, 4.0434e-03, 4.8878e-02, 5.1513e-02,\n",
      "         3.2016e-03, 1.3010e-05, 3.7078e-05, 1.4915e-02, 7.8108e-04, 2.9464e-01,\n",
      "         2.3850e-01, 1.0021e-04, 4.0074e-05, 1.4444e-04, 3.3282e-05, 4.7452e-05,\n",
      "         3.8212e-05, 4.8152e-05, 4.1139e-05, 4.3346e-05, 3.9838e-05, 4.5025e-05,\n",
      "         3.5718e-05, 3.7660e-05, 3.6685e-05, 3.6159e-05, 5.1015e-05, 3.2051e-05,\n",
      "         1.4052e-01],\n",
      "        [1.2647e-01, 4.1996e-02, 3.8483e-05, 4.2708e-03, 4.9600e-02, 3.9333e-02,\n",
      "         3.2587e-03, 1.5733e-05, 2.1451e-05, 9.0369e-03, 5.8810e-04, 3.7085e-01,\n",
      "         2.3412e-01, 1.0000e-04, 2.4338e-05, 1.1498e-04, 2.1587e-05, 3.1734e-05,\n",
      "         2.4034e-05, 3.1986e-05, 2.5953e-05, 2.6941e-05, 2.5958e-05, 2.8386e-05,\n",
      "         2.2168e-05, 2.6160e-05, 2.2722e-05, 2.5091e-05, 3.2817e-05, 1.9206e-05,\n",
      "         1.1979e-01],\n",
      "        [2.1812e-01, 4.0924e-02, 9.3170e-05, 3.5298e-03, 4.3411e-02, 6.3007e-02,\n",
      "         3.1076e-03, 1.1148e-05, 6.4085e-05, 2.4005e-02, 1.1069e-03, 2.0122e-01,\n",
      "         2.2831e-01, 1.0434e-04, 6.6745e-05, 1.8935e-04, 5.2323e-05, 6.6981e-05,\n",
      "         6.1782e-05, 7.1882e-05, 6.4848e-05, 6.8546e-05, 6.0177e-05, 7.1819e-05,\n",
      "         5.6422e-05, 5.2546e-05, 6.1504e-05, 5.2905e-05, 7.5033e-05, 5.3100e-05,\n",
      "         1.7186e-01],\n",
      "        [2.3162e-01, 4.0918e-02, 1.0532e-04, 3.3783e-03, 4.1764e-02, 6.1262e-02,\n",
      "         3.2731e-03, 1.0849e-05, 7.3485e-05, 2.5267e-02, 1.2692e-03, 1.7650e-01,\n",
      "         2.2947e-01, 1.1438e-04, 7.4560e-05, 2.1001e-04, 5.9885e-05, 7.4738e-05,\n",
      "         6.9353e-05, 7.9074e-05, 7.3060e-05, 7.8370e-05, 6.8189e-05, 8.2646e-05,\n",
      "         6.3080e-05, 5.9038e-05, 6.8929e-05, 5.8751e-05, 8.2510e-05, 6.1667e-05,\n",
      "         1.8372e-01],\n",
      "        [2.2900e-01, 4.0547e-02, 1.0398e-04, 3.4019e-03, 4.1982e-02, 6.1202e-02,\n",
      "         3.2501e-03, 1.0759e-05, 7.2217e-05, 2.4979e-02, 1.2534e-03, 1.7919e-01,\n",
      "         2.3289e-01, 1.1582e-04, 7.3260e-05, 2.0925e-04, 5.8987e-05, 7.3808e-05,\n",
      "         6.8168e-05, 7.7816e-05, 7.1758e-05, 7.7049e-05, 6.7007e-05, 8.1133e-05,\n",
      "         6.2152e-05, 5.8024e-05, 6.7815e-05, 5.7644e-05, 8.1434e-05, 6.0475e-05,\n",
      "         1.8076e-01],\n",
      "        [1.6110e-01, 4.1316e-02, 5.5526e-05, 4.1324e-03, 4.9844e-02, 5.5055e-02,\n",
      "         3.1506e-03, 1.3573e-05, 3.6538e-05, 1.5424e-02, 7.4896e-04, 3.0217e-01,\n",
      "         2.2513e-01, 9.3214e-05, 3.9943e-05, 1.3909e-04, 3.2540e-05, 4.6871e-05,\n",
      "         3.8043e-05, 4.7746e-05, 4.0559e-05, 4.2963e-05, 3.9027e-05, 4.3674e-05,\n",
      "         3.5479e-05, 3.7327e-05, 3.6309e-05, 3.6017e-05, 5.0654e-05, 3.1819e-05,\n",
      "         1.4099e-01],\n",
      "        [1.3341e-01, 4.1361e-02, 4.1758e-05, 4.2521e-03, 5.0765e-02, 4.5818e-02,\n",
      "         3.1703e-03, 1.5175e-05, 2.5894e-05, 1.1074e-02, 6.1278e-04, 3.6087e-01,\n",
      "         2.2320e-01, 9.1443e-05, 2.9061e-05, 1.1758e-04, 2.4659e-05, 3.7005e-05,\n",
      "         2.8281e-05, 3.6994e-05, 3.0545e-05, 3.1967e-05, 3.0123e-05, 3.2908e-05,\n",
      "         2.6325e-05, 3.0280e-05, 2.6447e-05, 2.8688e-05, 3.8956e-05, 2.3164e-05,\n",
      "         1.2472e-01],\n",
      "        [2.0766e-01, 4.1096e-02, 8.5892e-05, 3.7774e-03, 4.5607e-02, 6.8505e-02,\n",
      "         2.9834e-03, 1.1744e-05, 5.7102e-05, 2.3523e-02, 9.7608e-04, 2.2234e-01,\n",
      "         2.1866e-01, 9.6485e-05, 6.0693e-05, 1.7423e-04, 4.6924e-05, 6.1458e-05,\n",
      "         5.6351e-05, 6.5869e-05, 5.8170e-05, 6.2152e-05, 5.3691e-05, 6.2707e-05,\n",
      "         5.1619e-05, 4.7828e-05, 5.5790e-05, 4.8340e-05, 6.9071e-05, 4.7560e-05,\n",
      "         1.6359e-01],\n",
      "        [2.2854e-01, 4.1408e-02, 9.9842e-05, 3.5515e-03, 4.3204e-02, 6.9756e-02,\n",
      "         3.0319e-03, 1.1221e-05, 6.7769e-05, 2.6595e-02, 1.1122e-03, 1.9113e-01,\n",
      "         2.1239e-01, 9.7134e-05, 7.0996e-05, 1.9010e-04, 5.4408e-05, 6.8821e-05,\n",
      "         6.5479e-05, 7.4968e-05, 6.7507e-05, 7.2178e-05, 6.1895e-05, 7.3557e-05,\n",
      "         5.9495e-05, 5.3841e-05, 6.5344e-05, 5.5022e-05, 7.7853e-05, 5.6178e-05,\n",
      "         1.7784e-01],\n",
      "        [1.7429e-01, 4.0268e-02, 6.4197e-05, 3.9339e-03, 4.7885e-02, 5.6473e-02,\n",
      "         3.1122e-03, 1.2357e-05, 4.3442e-05, 1.7453e-02, 8.4452e-04, 2.7189e-01,\n",
      "         2.3644e-01, 1.0012e-04, 4.6553e-05, 1.5424e-04, 3.7754e-05, 5.2578e-05,\n",
      "         4.3923e-05, 5.4018e-05, 4.6816e-05, 4.9437e-05, 4.4637e-05, 5.1016e-05,\n",
      "         4.0945e-05, 4.1363e-05, 4.2586e-05, 4.0152e-05, 5.7439e-05, 3.7025e-05,\n",
      "         1.4634e-01],\n",
      "        [2.4686e-01, 4.4467e-02, 1.0860e-04, 3.4961e-03, 4.2167e-02, 7.2982e-02,\n",
      "         3.1605e-03, 1.2183e-05, 7.0813e-05, 2.8486e-02, 1.1709e-03, 1.7227e-01,\n",
      "         1.8542e-01, 8.7578e-05, 7.5050e-05, 1.9145e-04, 5.6536e-05, 7.0189e-05,\n",
      "         6.9374e-05, 7.8434e-05, 7.0577e-05, 7.6189e-05, 6.4427e-05, 7.6334e-05,\n",
      "         6.1915e-05, 5.6350e-05, 6.8778e-05, 5.8828e-05, 7.9471e-05, 6.0035e-05,\n",
      "         1.9804e-01],\n",
      "        [2.4706e-01, 4.4196e-02, 1.0996e-04, 3.3809e-03, 4.1367e-02, 6.7398e-02,\n",
      "         3.2784e-03, 1.1914e-05, 7.4627e-05, 2.7665e-02, 1.2474e-03, 1.6793e-01,\n",
      "         1.9438e-01, 9.4012e-05, 7.7901e-05, 1.9965e-04, 5.9853e-05, 7.3992e-05,\n",
      "         7.2121e-05, 8.1781e-05, 7.4640e-05, 7.9956e-05, 6.8730e-05, 8.2314e-05,\n",
      "         6.4458e-05, 5.9878e-05, 7.1466e-05, 6.1655e-05, 8.3062e-05, 6.3386e-05,\n",
      "         2.0053e-01],\n",
      "        [2.5096e-01, 4.5082e-02, 1.1137e-04, 3.3748e-03, 4.1183e-02, 6.8198e-02,\n",
      "         3.3042e-03, 1.2226e-05, 7.4375e-05, 2.7983e-02, 1.2515e-03, 1.6470e-01,\n",
      "         1.8700e-01, 9.0810e-05, 7.8064e-05, 1.9830e-04, 5.9605e-05, 7.3460e-05,\n",
      "         7.2310e-05, 8.1837e-05, 7.4525e-05, 7.9974e-05, 6.8523e-05, 8.1912e-05,\n",
      "         6.4310e-05, 5.9858e-05, 7.1514e-05, 6.2065e-05, 8.2549e-05, 6.3543e-05,\n",
      "         2.0540e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  1, 11,  0, 30, 12, 30,  1,  5, 10, 11,  0, 12, 30,  1,  0, 11, 30,\n",
      "         0, 12,  5,  4,  1, 30, 11, 30,  9, 12, 30,  0,  0, 30],\n",
      "       device='cuda:0')\n",
      "2.6435391902923584\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [1.3220e-04, 2.2189e-04, 9.9269e-01, 1.7614e-05, 3.7885e-06, 3.2716e-05,\n",
      "         1.9239e-05, 4.7478e-04, 1.1970e-04, 1.4226e-04, 2.1458e-04, 1.3956e-05,\n",
      "         1.0874e-05, 2.9227e-03, 1.5774e-04, 1.2038e-03, 1.0654e-04, 9.8081e-05,\n",
      "         1.2058e-04, 9.4300e-05, 9.2999e-05, 8.3308e-05, 1.1698e-04, 1.5558e-04,\n",
      "         1.2265e-04, 1.0752e-04, 1.0441e-04, 1.8451e-04, 9.8022e-05, 1.2903e-04,\n",
      "         1.0153e-05],\n",
      "        [6.2584e-01, 2.4194e-04, 3.4576e-03, 1.7205e-04, 3.1376e-05, 5.8897e-05,\n",
      "         1.0699e-03, 4.8440e-05, 9.2715e-06, 7.7267e-05, 1.7692e-01, 2.8647e-04,\n",
      "         2.1488e-02, 3.7343e-02, 1.9593e-05, 1.2016e-01, 1.7777e-05, 2.6422e-05,\n",
      "         1.6900e-05, 2.1324e-05, 1.2107e-05, 1.5433e-05, 1.8995e-05, 2.4697e-05,\n",
      "         2.4964e-05, 1.0177e-05, 1.6749e-05, 3.1463e-05, 1.8437e-05, 1.6543e-05,\n",
      "         1.2499e-02],\n",
      "        [5.0244e-04, 4.0705e-05, 4.0054e-05, 3.4379e-05, 3.6674e-05, 4.5553e-06,\n",
      "         2.5515e-05, 1.4861e-06, 1.1399e-07, 2.4604e-06, 1.8615e-04, 3.1003e-03,\n",
      "         3.0208e-03, 9.9275e-01, 4.0394e-07, 2.0555e-04, 1.4990e-07, 2.8401e-07,\n",
      "         2.9262e-07, 3.6613e-07, 1.3929e-07, 2.1264e-07, 2.6135e-07, 2.7373e-07,\n",
      "         4.3713e-07, 2.1511e-07, 1.6589e-07, 4.6358e-07, 2.5181e-07, 2.4284e-07,\n",
      "         4.5856e-05],\n",
      "        [3.9936e-02, 2.6113e-04, 3.6180e-04, 1.2604e-04, 3.9069e-05, 1.4062e-05,\n",
      "         6.4459e-03, 6.7722e-06, 1.1397e-06, 2.2811e-05, 6.0045e-02, 2.9279e-04,\n",
      "         8.4013e-01, 2.5651e-03, 2.0978e-06, 3.2184e-03, 2.4695e-06, 2.7067e-06,\n",
      "         2.2009e-06, 1.5840e-06, 2.0789e-06, 1.7837e-06, 1.8171e-06, 1.5447e-06,\n",
      "         3.4842e-06, 1.5957e-06, 2.0465e-06, 2.8778e-06, 2.4740e-06, 1.4675e-06,\n",
      "         4.6501e-02],\n",
      "        [3.3085e-02, 6.1240e-04, 2.2010e-02, 7.6412e-04, 5.0041e-04, 4.2667e-04,\n",
      "         2.2017e-03, 6.0302e-07, 6.9017e-05, 8.1094e-04, 1.3590e-02, 4.4641e-04,\n",
      "         9.3334e-03, 1.7702e-03, 6.9257e-05, 3.3171e-03, 7.0213e-05, 1.1891e-04,\n",
      "         7.8554e-05, 8.8970e-05, 9.5448e-05, 8.4387e-05, 7.8018e-05, 8.9952e-05,\n",
      "         8.9697e-05, 7.2165e-05, 1.0820e-04, 8.9972e-05, 9.5834e-05, 8.8830e-05,\n",
      "         9.0974e-01],\n",
      "        [3.6330e-02, 7.8827e-04, 2.1710e-02, 8.6566e-04, 5.5824e-04, 4.3885e-04,\n",
      "         2.4423e-03, 7.3817e-07, 6.8240e-05, 8.3779e-04, 1.4883e-02, 5.9550e-04,\n",
      "         1.0503e-02, 2.2763e-03, 7.0332e-05, 3.5724e-03, 7.4051e-05, 1.1647e-04,\n",
      "         8.1311e-05, 9.1936e-05, 1.0025e-04, 8.5996e-05, 7.8634e-05, 9.1833e-05,\n",
      "         9.0750e-05, 7.3281e-05, 1.1224e-04, 9.2495e-05, 9.9349e-05, 9.1975e-05,\n",
      "         9.0278e-01],\n",
      "        [3.7268e-02, 1.4931e-03, 9.4931e-03, 1.5336e-03, 1.5004e-03, 6.9474e-04,\n",
      "         5.0089e-03, 1.3205e-06, 3.8105e-05, 1.1368e-03, 1.2425e-02, 1.8853e-03,\n",
      "         2.2334e-02, 2.7338e-03, 4.0311e-05, 3.0958e-03, 4.8277e-05, 8.3413e-05,\n",
      "         4.6956e-05, 6.1698e-05, 6.8254e-05, 5.7389e-05, 5.1824e-05, 6.3272e-05,\n",
      "         6.2422e-05, 5.1674e-05, 7.4771e-05, 6.0238e-05, 7.2619e-05, 6.1391e-05,\n",
      "         8.9845e-01],\n",
      "        [3.7046e-03, 1.0435e-03, 1.0053e-04, 7.8473e-03, 2.4922e-02, 1.4575e-03,\n",
      "         2.6367e-01, 2.2862e-03, 2.7159e-04, 6.9954e-04, 2.8267e-02, 7.6681e-03,\n",
      "         9.1448e-02, 4.8998e-03, 3.4671e-04, 3.6266e-03, 2.8968e-04, 9.6266e-04,\n",
      "         3.2619e-04, 3.7239e-04, 5.1290e-04, 3.9182e-04, 2.4964e-04, 2.7911e-04,\n",
      "         4.5396e-04, 3.1989e-04, 2.4719e-04, 3.4859e-04, 3.6353e-04, 2.4012e-04,\n",
      "         5.5239e-01],\n",
      "        [2.5212e-03, 1.0148e-03, 2.0155e-05, 1.0063e-02, 9.8170e-02, 1.8872e-03,\n",
      "         2.5334e-01, 1.3470e-03, 8.4910e-05, 7.9849e-04, 1.0151e-02, 7.1614e-02,\n",
      "         4.3169e-01, 2.3678e-02, 1.1183e-04, 1.1746e-03, 8.5751e-05, 2.2149e-04,\n",
      "         9.8539e-05, 1.1920e-04, 1.3545e-04, 1.1933e-04, 6.6052e-05, 5.7751e-05,\n",
      "         1.3699e-04, 9.9009e-05, 6.1486e-05, 1.0283e-04, 1.0931e-04, 7.0466e-05,\n",
      "         9.0857e-02],\n",
      "        [9.0648e-03, 8.5414e-03, 1.0966e-05, 1.3104e-02, 1.0485e-01, 2.7547e-03,\n",
      "         3.0083e-01, 9.4219e-03, 4.1099e-05, 5.8033e-04, 1.0476e-02, 5.1924e-02,\n",
      "         7.8166e-02, 2.5836e-03, 8.2031e-05, 6.6428e-04, 5.8155e-05, 1.2910e-04,\n",
      "         6.1858e-05, 8.3348e-05, 7.9587e-05, 8.7652e-05, 5.8291e-05, 4.8052e-05,\n",
      "         8.6686e-05, 6.3646e-05, 4.8326e-05, 6.8551e-05, 8.4902e-05, 6.8547e-05,\n",
      "         4.0588e-01],\n",
      "        [7.2612e-03, 1.8193e-03, 3.1868e-05, 1.1378e-02, 7.5493e-02, 2.8906e-03,\n",
      "         2.1430e-01, 2.3338e-03, 2.6545e-04, 1.1232e-03, 2.0014e-02, 2.8598e-02,\n",
      "         1.3363e-01, 4.3833e-03, 3.4809e-04, 1.9030e-03, 2.7696e-04, 7.5899e-04,\n",
      "         3.2176e-04, 4.0271e-04, 4.3962e-04, 3.8613e-04, 2.7372e-04, 2.1829e-04,\n",
      "         4.1298e-04, 3.6210e-04, 2.3246e-04, 2.7873e-04, 4.4145e-04, 2.9897e-04,\n",
      "         4.8913e-01],\n",
      "        [4.9307e-03, 1.3529e-03, 2.0036e-05, 1.6772e-02, 1.1719e-01, 3.3061e-03,\n",
      "         1.9230e-01, 8.2358e-04, 1.1543e-04, 1.2258e-03, 9.0443e-03, 7.5928e-02,\n",
      "         3.9125e-01, 8.5512e-03, 1.4163e-04, 1.1474e-03, 1.1441e-04, 3.0495e-04,\n",
      "         1.2916e-04, 1.5984e-04, 1.6377e-04, 1.5414e-04, 1.1642e-04, 7.4792e-05,\n",
      "         1.7276e-04, 1.5514e-04, 1.0030e-04, 1.1999e-04, 1.9809e-04, 1.0893e-04,\n",
      "         1.7382e-01],\n",
      "        [1.6370e-02, 1.1739e-02, 7.2862e-06, 1.7141e-02, 1.8234e-01, 4.8389e-03,\n",
      "         1.5808e-01, 9.4343e-03, 1.1822e-04, 1.3088e-03, 1.0901e-02, 3.9489e-02,\n",
      "         4.9898e-02, 1.1082e-03, 1.5666e-04, 6.5306e-04, 1.5127e-04, 2.9318e-04,\n",
      "         1.6914e-04, 1.9289e-04, 1.9322e-04, 2.2655e-04, 1.5825e-04, 1.4784e-04,\n",
      "         1.9170e-04, 1.7416e-04, 1.5672e-04, 1.4918e-04, 2.3417e-04, 2.0979e-04,\n",
      "         4.9377e-01],\n",
      "        [1.7396e-02, 1.4482e-02, 6.8471e-06, 2.1950e-02, 2.3575e-01, 5.8206e-03,\n",
      "         1.3543e-01, 8.0257e-03, 7.5582e-05, 1.3292e-03, 7.2728e-03, 7.3640e-02,\n",
      "         7.0503e-02, 1.1940e-03, 1.0480e-04, 5.2147e-04, 1.0097e-04, 1.8971e-04,\n",
      "         1.1398e-04, 1.4004e-04, 1.2500e-04, 1.3830e-04, 1.0524e-04, 9.4340e-05,\n",
      "         1.2160e-04, 1.1976e-04, 1.1489e-04, 1.0893e-04, 1.5794e-04, 1.2750e-04,\n",
      "         4.0474e-01],\n",
      "        [1.6950e-02, 1.8791e-02, 5.4652e-06, 1.4999e-02, 1.5350e-01, 3.9325e-03,\n",
      "         1.6890e-01, 1.4341e-02, 6.1622e-05, 9.4047e-04, 8.5595e-03, 4.0589e-02,\n",
      "         3.4505e-02, 6.4786e-04, 9.4639e-05, 4.5468e-04, 8.3555e-05, 1.6008e-04,\n",
      "         9.6285e-05, 1.1583e-04, 1.1071e-04, 1.2678e-04, 9.3472e-05, 8.7352e-05,\n",
      "         1.0845e-04, 9.6465e-05, 9.4333e-05, 9.7008e-05, 1.3604e-04, 1.1420e-04,\n",
      "         5.2121e-01],\n",
      "        [1.4169e-02, 1.3465e-02, 4.5646e-06, 2.6966e-02, 3.0921e-01, 6.1223e-03,\n",
      "         1.1462e-01, 7.4446e-03, 5.5510e-05, 1.1947e-03, 4.8290e-03, 1.2579e-01,\n",
      "         9.0957e-02, 1.1298e-03, 8.1846e-05, 3.9155e-04, 7.4795e-05, 1.3536e-04,\n",
      "         8.8137e-05, 1.1381e-04, 9.2344e-05, 9.3921e-05, 7.8344e-05, 6.5813e-05,\n",
      "         9.5388e-05, 8.6226e-05, 9.1177e-05, 8.7163e-05, 1.2184e-04, 8.4896e-05,\n",
      "         2.8227e-01],\n",
      "        [4.1873e-02, 5.5479e-02, 5.7269e-06, 1.4096e-02, 1.0715e-01, 4.5206e-03,\n",
      "         1.5381e-01, 1.2357e-02, 1.1945e-05, 7.1037e-04, 6.5695e-03, 6.6848e-02,\n",
      "         6.9749e-02, 3.3425e-04, 2.4233e-05, 3.9184e-04, 1.9554e-05, 3.9124e-05,\n",
      "         2.0136e-05, 2.7037e-05, 2.2407e-05, 2.5838e-05, 2.0850e-05, 2.0505e-05,\n",
      "         2.2294e-05, 2.0527e-05, 2.4020e-05, 2.9693e-05, 3.1459e-05, 2.1002e-05,\n",
      "         4.6572e-01],\n",
      "        [3.4327e-02, 3.7161e-02, 9.7412e-06, 1.5795e-02, 1.7384e-01, 5.5264e-03,\n",
      "         1.0873e-01, 4.7030e-03, 2.6624e-05, 1.2551e-03, 5.7897e-03, 9.1196e-02,\n",
      "         6.0380e-02, 6.2484e-04, 4.7654e-05, 3.7437e-04, 3.8364e-05, 7.7120e-05,\n",
      "         4.2890e-05, 6.0194e-05, 4.4726e-05, 5.0656e-05, 4.2474e-05, 4.1468e-05,\n",
      "         4.2424e-05, 4.7116e-05, 4.9660e-05, 5.1760e-05, 5.9059e-05, 4.4653e-05,\n",
      "         4.5952e-01],\n",
      "        [3.3064e-02, 2.6001e-02, 9.1893e-06, 2.0463e-02, 2.9164e-01, 6.8393e-03,\n",
      "         4.9319e-02, 7.9967e-04, 2.3635e-05, 1.7611e-03, 3.7209e-03, 1.7529e-01,\n",
      "         1.3755e-01, 7.8361e-04, 4.0067e-05, 2.8114e-04, 3.3154e-05, 6.7722e-05,\n",
      "         3.6061e-05, 5.6789e-05, 3.5499e-05, 3.7551e-05, 3.4696e-05, 3.4307e-05,\n",
      "         3.9162e-05, 3.7804e-05, 4.1084e-05, 4.4912e-05, 5.1536e-05, 3.9516e-05,\n",
      "         2.5183e-01],\n",
      "        [7.9069e-02, 9.3979e-02, 1.2450e-05, 1.1298e-02, 1.1431e-01, 5.2978e-03,\n",
      "         7.6980e-02, 1.5073e-03, 6.9001e-06, 1.1319e-03, 4.9516e-03, 1.0022e-01,\n",
      "         8.5498e-02, 2.9438e-04, 1.6398e-05, 2.7781e-04, 1.1214e-05, 2.5585e-05,\n",
      "         1.1326e-05, 1.8203e-05, 1.1507e-05, 1.4299e-05, 1.2612e-05, 1.3909e-05,\n",
      "         1.2711e-05, 1.2453e-05, 1.4257e-05, 1.9579e-05, 1.7160e-05, 1.3193e-05,\n",
      "         4.2494e-01],\n",
      "        [9.1063e-02, 9.3964e-02, 1.5385e-05, 1.1924e-02, 1.1306e-01, 6.2500e-03,\n",
      "         5.0324e-02, 5.4835e-04, 5.2698e-06, 1.3851e-03, 3.9881e-03, 1.3456e-01,\n",
      "         1.3810e-01, 2.9571e-04, 1.2619e-05, 2.3966e-04, 8.4688e-06, 2.0883e-05,\n",
      "         8.7125e-06, 1.4885e-05, 8.5776e-06, 1.0314e-05, 9.4118e-06, 1.0562e-05,\n",
      "         1.0003e-05, 9.7345e-06, 1.1073e-05, 1.5532e-05, 1.3508e-05, 1.0589e-05,\n",
      "         3.5410e-01],\n",
      "        [8.9755e-02, 9.5760e-02, 1.5169e-05, 1.2019e-02, 1.0092e-01, 5.9839e-03,\n",
      "         4.8614e-02, 5.1980e-04, 4.4467e-06, 1.3035e-03, 3.8929e-03, 1.3357e-01,\n",
      "         1.5389e-01, 2.7825e-04, 1.1026e-05, 2.2930e-04, 7.4597e-06, 1.9034e-05,\n",
      "         7.6379e-06, 1.2975e-05, 7.5825e-06, 9.0077e-06, 8.1016e-06, 9.2768e-06,\n",
      "         8.8218e-06, 8.6330e-06, 9.7983e-06, 1.4169e-05, 1.2273e-05, 9.5823e-06,\n",
      "         3.5308e-01],\n",
      "        [2.8014e-02, 4.5000e-02, 3.7830e-06, 1.4062e-02, 1.0703e-01, 3.8969e-03,\n",
      "         1.9974e-01, 3.0745e-02, 1.4708e-05, 5.5006e-04, 6.6426e-03, 5.4125e-02,\n",
      "         5.1871e-02, 3.4950e-04, 2.6143e-05, 4.2127e-04, 2.2357e-05, 4.1870e-05,\n",
      "         2.3691e-05, 2.8332e-05, 2.6446e-05, 3.0566e-05, 2.5091e-05, 2.1693e-05,\n",
      "         2.6027e-05, 2.2179e-05, 2.6880e-05, 2.9709e-05, 3.8110e-05, 2.2187e-05,\n",
      "         4.5713e-01],\n",
      "        [2.7899e-02, 4.1306e-02, 3.8953e-06, 1.7674e-02, 1.4006e-01, 4.9130e-03,\n",
      "         1.8560e-01, 2.5729e-02, 1.6862e-05, 6.5030e-04, 6.2391e-03, 7.5420e-02,\n",
      "         6.9761e-02, 4.5344e-04, 3.0143e-05, 4.4054e-04, 2.5763e-05, 4.8390e-05,\n",
      "         2.7266e-05, 3.2866e-05, 2.9418e-05, 3.4260e-05, 2.7911e-05, 2.4078e-05,\n",
      "         3.0748e-05, 2.5137e-05, 3.0888e-05, 3.3095e-05, 4.3406e-05, 2.5198e-05,\n",
      "         4.0336e-01],\n",
      "        [3.0070e-02, 4.5586e-02, 3.8177e-06, 1.5353e-02, 1.1399e-01, 4.3639e-03,\n",
      "         1.9667e-01, 2.9504e-02, 1.4349e-05, 5.7952e-04, 6.6885e-03, 6.1141e-02,\n",
      "         6.3726e-02, 3.7351e-04, 2.5620e-05, 4.5065e-04, 2.2048e-05, 4.1407e-05,\n",
      "         2.2898e-05, 2.7067e-05, 2.5097e-05, 2.9422e-05, 2.4094e-05, 2.0701e-05,\n",
      "         2.5544e-05, 2.1153e-05, 2.6066e-05, 2.8582e-05, 3.7202e-05, 2.1153e-05,\n",
      "         4.3109e-01],\n",
      "        [3.1505e-02, 4.5990e-02, 4.4076e-06, 1.3793e-02, 1.1818e-01, 4.2289e-03,\n",
      "         1.8855e-01, 2.4546e-02, 1.7173e-05, 6.3860e-04, 6.6857e-03, 5.6833e-02,\n",
      "         4.8379e-02, 3.6389e-04, 3.1007e-05, 4.2228e-04, 2.5544e-05, 4.6286e-05,\n",
      "         2.7527e-05, 3.3473e-05, 2.9484e-05, 3.4703e-05, 2.9183e-05, 2.5198e-05,\n",
      "         2.9557e-05, 2.6342e-05, 3.0887e-05, 3.4603e-05, 4.0375e-05, 2.5511e-05,\n",
      "         4.5939e-01],\n",
      "        [2.7951e-02, 3.9196e-02, 4.5641e-06, 1.9682e-02, 1.7964e-01, 5.6873e-03,\n",
      "         1.5827e-01, 1.6834e-02, 2.0233e-05, 7.8749e-04, 5.5188e-03, 9.7939e-02,\n",
      "         6.9982e-02, 5.6074e-04, 3.7947e-05, 4.0151e-04, 3.0803e-05, 5.7562e-05,\n",
      "         3.3652e-05, 4.2899e-05, 3.4860e-05, 4.0947e-05, 3.3675e-05, 2.9684e-05,\n",
      "         3.8146e-05, 3.1797e-05, 3.8139e-05, 4.0877e-05, 4.9548e-05, 3.1654e-05,\n",
      "         3.7695e-01],\n",
      "        [3.6503e-02, 5.2199e-02, 5.2433e-06, 1.5440e-02, 1.2451e-01, 5.0767e-03,\n",
      "         1.6923e-01, 1.7698e-02, 1.4221e-05, 7.0699e-04, 6.1718e-03, 7.4805e-02,\n",
      "         6.2663e-02, 4.0525e-04, 2.8210e-05, 4.1553e-04, 2.2353e-05, 4.3209e-05,\n",
      "         2.3510e-05, 2.9754e-05, 2.4737e-05, 3.0109e-05, 2.4494e-05, 2.2262e-05,\n",
      "         2.6044e-05, 2.2969e-05, 2.7495e-05, 3.0932e-05, 3.5482e-05, 2.2433e-05,\n",
      "         4.3373e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  2,  0, 13, 12, 30,  1,  7,  0, 11,  7,  0, 12,  0,  1, 30, 11,  7,\n",
      "         4, 12, 30,  1, 30,  0, 11, 30, 30, 12,  0,  6], device='cuda:0')\n",
      "0.4171633720397949\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [1.3226e-04, 2.2169e-04, 9.9268e-01, 1.7629e-05, 3.7876e-06, 3.2754e-05,\n",
      "         1.9233e-05, 4.7392e-04, 1.2004e-04, 1.4251e-04, 2.1467e-04, 1.3957e-05,\n",
      "         1.0892e-05, 2.9200e-03, 1.5815e-04, 1.2045e-03, 1.0684e-04, 9.8388e-05,\n",
      "         1.2092e-04, 9.4549e-05, 9.3268e-05, 8.3531e-05, 1.1730e-04, 1.5600e-04,\n",
      "         1.2296e-04, 1.0782e-04, 1.0474e-04, 1.8502e-04, 9.8261e-05, 1.2933e-04,\n",
      "         1.0153e-05],\n",
      "        [6.2606e-01, 2.4184e-04, 3.4577e-03, 1.7178e-04, 3.1307e-05, 5.8828e-05,\n",
      "         1.0675e-03, 4.8485e-05, 9.2614e-06, 7.7161e-05, 1.7685e-01, 2.8581e-04,\n",
      "         2.1444e-02, 3.7245e-02, 1.9568e-05, 1.2017e-01, 1.7759e-05, 2.6380e-05,\n",
      "         1.6876e-05, 2.1302e-05, 1.2086e-05, 1.5413e-05, 1.8967e-05, 2.4666e-05,\n",
      "         2.4934e-05, 1.0163e-05, 1.6719e-05, 3.1420e-05, 1.8407e-05, 1.6517e-05,\n",
      "         1.2482e-02],\n",
      "        [5.0475e-04, 4.0752e-05, 4.0255e-05, 3.4526e-05, 3.6817e-05, 4.5746e-06,\n",
      "         2.5623e-05, 1.4929e-06, 1.1460e-07, 2.4709e-06, 1.8707e-04, 3.1060e-03,\n",
      "         3.0271e-03, 9.9273e-01, 4.0595e-07, 2.0674e-04, 1.5078e-07, 2.8557e-07,\n",
      "         2.9426e-07, 3.6830e-07, 1.4007e-07, 2.1381e-07, 2.6300e-07, 2.7553e-07,\n",
      "         4.3954e-07, 2.1647e-07, 1.6702e-07, 4.6622e-07, 2.5330e-07, 2.4432e-07,\n",
      "         4.5987e-05],\n",
      "        [4.0202e-02, 2.5971e-04, 3.6685e-04, 1.2648e-04, 3.9344e-05, 1.4163e-05,\n",
      "         6.4802e-03, 6.8327e-06, 1.1527e-06, 2.3057e-05, 6.0688e-02, 2.9248e-04,\n",
      "         8.3921e-01, 2.5658e-03, 2.1192e-06, 3.2494e-03, 2.4999e-06, 2.7321e-06,\n",
      "         2.2257e-06, 1.6021e-06, 2.1006e-06, 1.8047e-06, 1.8429e-06, 1.5674e-06,\n",
      "         3.5185e-06, 1.6162e-06, 2.0794e-06, 2.9043e-06, 2.4981e-06, 1.4872e-06,\n",
      "         4.6445e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  2, 15, 13, 12, 30], device='cuda:0')\n",
      "2.7261526584625244\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05,  ..., 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [1.1915e-03, 6.3924e-01, 6.7760e-05,  ..., 9.2035e-07, 6.9819e-07,\n",
      "         8.8099e-05],\n",
      "        [3.5472e-03, 1.0038e-02, 2.3517e-06,  ..., 8.2711e-05, 8.7406e-05,\n",
      "         1.3796e-02],\n",
      "        ...,\n",
      "        [3.8736e-02, 4.4819e-02, 4.0343e-06,  ..., 1.4951e-05, 9.2902e-06,\n",
      "         7.1969e-01],\n",
      "        [3.7754e-02, 4.4513e-02, 3.8609e-06,  ..., 1.5271e-05, 9.4649e-06,\n",
      "         7.1594e-01],\n",
      "        [3.8204e-02, 4.3567e-02, 3.9909e-06,  ..., 1.4179e-05, 8.9475e-06,\n",
      "         7.2044e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  1, 10, 11, 10,  4, 12, 30, 30,  6, 13, 10, 30, 11,  0, 12, 10,  6,\n",
      "        13, 30,  0, 11, 30, 12, 30,  1, 30,  7, 11,  7,  0, 12, 30, 30,  1,  0,\n",
      "        30, 11,  0, 30, 12, 30, 30], device='cuda:0')\n",
      "2.9591658115386963\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [1.3392e-04, 2.2095e-04, 9.9281e-01, 1.7454e-05, 3.7567e-06, 3.2450e-05,\n",
      "         1.8970e-05, 4.6981e-04, 1.1455e-04, 1.4125e-04, 2.1467e-04, 1.3900e-05,\n",
      "         1.1037e-05, 2.8885e-03, 1.5117e-04, 1.2014e-03, 1.0215e-04, 9.4336e-05,\n",
      "         1.1578e-04, 9.0470e-05, 8.9385e-05, 7.9860e-05, 1.1157e-04, 1.4928e-04,\n",
      "         1.1700e-04, 1.0285e-04, 1.0059e-04, 1.7725e-04, 9.4088e-05, 1.2309e-04,\n",
      "         1.0105e-05],\n",
      "        [6.2703e-01, 2.3837e-04, 3.4118e-03, 1.7039e-04, 3.0770e-05, 5.8308e-05,\n",
      "         1.0601e-03, 4.7441e-05, 9.1440e-06, 7.6436e-05, 1.7664e-01, 2.8316e-04,\n",
      "         2.1531e-02, 3.6855e-02, 1.9398e-05, 1.1967e-01, 1.7515e-05, 2.6173e-05,\n",
      "         1.6677e-05, 2.0954e-05, 1.1973e-05, 1.5179e-05, 1.8835e-05, 2.4285e-05,\n",
      "         2.4601e-05, 1.0052e-05, 1.6635e-05, 3.1098e-05, 1.8310e-05, 1.6333e-05,\n",
      "         1.2594e-02],\n",
      "        [5.2414e-04, 4.1948e-05, 4.0260e-05, 3.5039e-05, 3.7190e-05, 4.6422e-06,\n",
      "         2.6394e-05, 1.4955e-06, 1.1385e-07, 2.4925e-06, 1.9176e-04, 3.1725e-03,\n",
      "         3.1474e-03, 9.9251e-01, 4.0486e-07, 2.0947e-04, 1.5034e-07, 2.8601e-07,\n",
      "         2.9272e-07, 3.6543e-07, 1.3959e-07, 2.1291e-07, 2.6037e-07, 2.7227e-07,\n",
      "         4.3990e-07, 2.1427e-07, 1.6444e-07, 4.6573e-07, 2.5173e-07, 2.4132e-07,\n",
      "         4.8335e-05],\n",
      "        [3.9375e-02, 2.6275e-04, 3.5194e-04, 1.2380e-04, 3.8048e-05, 1.3747e-05,\n",
      "         6.3330e-03, 6.5877e-06, 1.1162e-06, 2.2303e-05, 5.8886e-02, 2.8998e-04,\n",
      "         8.4203e-01, 2.5724e-03, 2.0559e-06, 3.1626e-03, 2.4086e-06, 2.6488e-06,\n",
      "         2.1533e-06, 1.5427e-06, 2.0322e-06, 1.7377e-06, 1.7596e-06, 1.4962e-06,\n",
      "         3.4193e-06, 1.5463e-06, 1.9809e-06, 2.8224e-06, 2.4207e-06, 1.4235e-06,\n",
      "         4.6495e-02],\n",
      "        [3.3136e-02, 6.1374e-04, 2.1918e-02, 7.6641e-04, 5.0046e-04, 4.2609e-04,\n",
      "         2.2123e-03, 6.0767e-07, 6.8379e-05, 8.0529e-04, 1.3603e-02, 4.4613e-04,\n",
      "         9.4362e-03, 1.7709e-03, 6.8852e-05, 3.3152e-03, 6.9703e-05, 1.1809e-04,\n",
      "         7.8110e-05, 8.8455e-05, 9.4777e-05, 8.3832e-05, 7.7433e-05, 8.9503e-05,\n",
      "         8.9043e-05, 7.1747e-05, 1.0763e-04, 8.9533e-05, 9.5206e-05, 8.8251e-05,\n",
      "         9.0967e-01],\n",
      "        [6.7617e-02, 7.4302e-04, 5.7645e-02, 5.4206e-04, 3.6730e-04, 3.0509e-04,\n",
      "         4.2187e-03, 2.8239e-06, 5.1693e-05, 7.8396e-04, 4.6118e-02, 1.0400e-03,\n",
      "         2.6938e-02, 1.1147e-02, 6.3400e-05, 1.3278e-02, 6.4804e-05, 9.3924e-05,\n",
      "         6.9932e-05, 5.4800e-05, 5.7493e-05, 6.7377e-05, 8.2635e-05, 5.8452e-05,\n",
      "         9.1959e-05, 5.5029e-05, 1.0295e-04, 9.4798e-05, 5.9166e-05, 6.6536e-05,\n",
      "         7.6812e-01],\n",
      "        [4.2831e-02, 8.0043e-04, 2.0599e-02, 9.6098e-04, 4.7181e-04, 3.3409e-04,\n",
      "         4.1653e-03, 1.5383e-06, 6.2113e-05, 6.8189e-04, 2.4627e-02, 5.7268e-04,\n",
      "         2.1503e-02, 3.2084e-03, 6.9627e-05, 5.5461e-03, 6.6753e-05, 1.1218e-04,\n",
      "         8.0861e-05, 7.5857e-05, 9.0173e-05, 8.4373e-05, 7.9477e-05, 8.6497e-05,\n",
      "         9.1792e-05, 6.4199e-05, 1.1034e-04, 9.7757e-05, 9.2585e-05, 9.5245e-05,\n",
      "         8.7234e-01],\n",
      "        [5.7795e-02, 1.2116e-03, 1.2694e-02, 1.8152e-03, 1.1529e-03, 3.7165e-04,\n",
      "         5.3747e-03, 4.0646e-06, 2.6423e-05, 6.7546e-04, 5.8113e-02, 4.7332e-03,\n",
      "         1.1896e-01, 2.9229e-02, 4.2180e-05, 7.0662e-03, 6.0723e-05, 6.6599e-05,\n",
      "         4.9278e-05, 4.4025e-05, 6.6707e-05, 4.7219e-05, 4.4498e-05, 4.0115e-05,\n",
      "         7.0969e-05, 4.2790e-05, 8.2527e-05, 8.4196e-05, 6.1026e-05, 4.9404e-05,\n",
      "         6.9993e-01],\n",
      "        [3.0528e-01, 3.8739e-03, 4.2198e-03, 5.3543e-04, 3.9994e-04, 3.9687e-04,\n",
      "         7.9509e-03, 1.2370e-05, 7.2466e-06, 3.6768e-04, 4.1087e-02, 4.4652e-04,\n",
      "         9.4251e-02, 6.2995e-04, 1.0339e-05, 4.1915e-03, 9.0298e-06, 1.1838e-05,\n",
      "         9.9397e-06, 1.0384e-05, 9.6570e-06, 1.2532e-05, 1.0664e-05, 1.4509e-05,\n",
      "         1.0566e-05, 6.9604e-06, 1.9125e-05, 1.1627e-05, 9.5612e-06, 7.9103e-06,\n",
      "         5.3619e-01],\n",
      "        [3.0344e-02, 5.0318e-04, 2.0386e-02, 5.2777e-04, 4.6005e-04, 4.2806e-04,\n",
      "         1.4328e-03, 4.9979e-07, 3.9259e-05, 6.1179e-04, 8.0180e-03, 2.6889e-04,\n",
      "         6.0897e-03, 1.0280e-03, 3.9710e-05, 2.1489e-03, 3.6768e-05, 6.9197e-05,\n",
      "         4.4407e-05, 5.5870e-05, 4.9432e-05, 4.8225e-05, 4.4994e-05, 5.6833e-05,\n",
      "         5.0273e-05, 4.5934e-05, 6.6105e-05, 5.0481e-05, 5.4915e-05, 5.0322e-05,\n",
      "         9.2695e-01],\n",
      "        [3.3562e-02, 5.5485e-04, 2.2494e-02, 5.2550e-04, 4.3345e-04, 4.4459e-04,\n",
      "         1.3622e-03, 4.8383e-07, 4.6922e-05, 6.9473e-04, 8.7891e-03, 2.7499e-04,\n",
      "         5.7502e-03, 1.1009e-03, 4.6071e-05, 2.3830e-03, 4.3002e-05, 7.8980e-05,\n",
      "         5.2098e-05, 6.1937e-05, 5.7020e-05, 5.6498e-05, 5.2515e-05, 6.4348e-05,\n",
      "         5.7713e-05, 5.1814e-05, 7.6187e-05, 5.7419e-05, 6.3814e-05, 5.8396e-05,\n",
      "         9.2071e-01],\n",
      "        [3.1349e-02, 5.6486e-04, 2.0996e-02, 7.8414e-04, 5.0637e-04, 4.2630e-04,\n",
      "         2.1369e-03, 5.8864e-07, 6.6650e-05, 7.8126e-04, 1.2994e-02, 4.3239e-04,\n",
      "         9.7305e-03, 1.6785e-03, 6.7260e-05, 3.1562e-03, 6.8865e-05, 1.1857e-04,\n",
      "         7.6639e-05, 8.7682e-05, 9.3573e-05, 8.1914e-05, 7.5654e-05, 8.8293e-05,\n",
      "         8.8100e-05, 7.1763e-05, 1.0655e-04, 8.9641e-05, 9.4266e-05, 8.7023e-05,\n",
      "         9.1310e-01],\n",
      "        [3.4588e-02, 7.6840e-04, 1.7978e-02, 1.2080e-03, 7.2054e-04, 4.5805e-04,\n",
      "         2.8447e-03, 9.0650e-07, 8.5379e-05, 1.0054e-03, 1.8352e-02, 9.2967e-04,\n",
      "         1.5149e-02, 3.1291e-03, 8.5785e-05, 3.9865e-03, 1.0161e-04, 1.5703e-04,\n",
      "         1.0221e-04, 1.1305e-04, 1.3858e-04, 1.0927e-04, 1.0027e-04, 1.1044e-04,\n",
      "         1.2564e-04, 9.2609e-05, 1.4457e-04, 1.2607e-04, 1.3517e-04, 1.2610e-04,\n",
      "         8.9703e-01],\n",
      "        [6.2296e-02, 9.3136e-04, 1.8638e-02, 6.4091e-04, 4.5704e-04, 4.5953e-04,\n",
      "         2.6506e-03, 1.2232e-06, 2.8325e-05, 5.7668e-04, 1.4151e-02, 3.4507e-04,\n",
      "         1.6324e-02, 1.2742e-03, 3.4390e-05, 3.2384e-03, 2.8737e-05, 5.1382e-05,\n",
      "         3.6173e-05, 3.9825e-05, 3.5642e-05, 3.9442e-05, 3.6464e-05, 4.6600e-05,\n",
      "         3.8968e-05, 3.2453e-05, 5.7966e-05, 4.3027e-05, 3.7285e-05, 3.5665e-05,\n",
      "         8.7739e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  2,  0, 13,  0,  0,  2, 12, 13, 30, 11,  2,  0, 13, 30, 11],\n",
      "       device='cuda:0')\n",
      "3.1408987045288086\n",
      "tensor([[1.2044e-05, 8.4966e-04, 2.5859e-05, 2.9005e-05, 1.2388e-04, 6.4967e-05,\n",
      "         7.7731e-05, 9.9836e-01, 8.6870e-06, 2.5966e-05, 9.9966e-06, 1.5076e-04,\n",
      "         5.1225e-06, 6.8824e-05, 2.3176e-05, 1.4785e-05, 6.4244e-06, 1.0068e-05,\n",
      "         1.2476e-05, 1.6362e-05, 1.0532e-05, 1.0950e-05, 7.9849e-06, 1.3120e-05,\n",
      "         8.8862e-06, 9.5560e-06, 7.6016e-06, 1.3016e-05, 1.1622e-05, 8.6137e-06,\n",
      "         5.3211e-07],\n",
      "        [1.3315e-04, 2.2169e-04, 9.9275e-01, 1.7517e-05, 3.7708e-06, 3.2562e-05,\n",
      "         1.9107e-05, 4.7297e-04, 1.1675e-04, 1.4155e-04, 2.1461e-04, 1.3922e-05,\n",
      "         1.0946e-05, 2.9054e-03, 1.5403e-04, 1.2026e-03, 1.0404e-04, 9.5900e-05,\n",
      "         1.1782e-04, 9.2095e-05, 9.0894e-05, 8.1347e-05, 1.1392e-04, 1.5197e-04,\n",
      "         1.1947e-04, 1.0485e-04, 1.0216e-04, 1.8034e-04, 9.5777e-05, 1.2569e-04,\n",
      "         1.0128e-05],\n",
      "        [6.2628e-01, 2.4029e-04, 3.4312e-03, 1.7141e-04, 3.1119e-05, 5.8657e-05,\n",
      "         1.0675e-03, 4.7902e-05, 9.2028e-06, 7.6900e-05, 1.7683e-01, 2.8544e-04,\n",
      "         2.1569e-02, 3.7175e-02, 1.9494e-05, 1.1988e-01, 1.7636e-05, 2.6300e-05,\n",
      "         1.6785e-05, 2.1121e-05, 1.2040e-05, 1.5301e-05, 1.8912e-05, 2.4485e-05,\n",
      "         2.4773e-05, 1.0111e-05, 1.6695e-05, 3.1277e-05, 1.8376e-05, 1.6435e-05,\n",
      "         1.2556e-02],\n",
      "        [5.0795e-04, 4.1217e-05, 3.9754e-05, 3.4406e-05, 3.6629e-05, 4.5575e-06,\n",
      "         2.5708e-05, 1.4782e-06, 1.1277e-07, 2.4552e-06, 1.8699e-04, 3.1253e-03,\n",
      "         3.0697e-03, 9.9267e-01, 4.0065e-07, 2.0502e-04, 1.4842e-07, 2.8197e-07,\n",
      "         2.8954e-07, 3.6172e-07, 1.3793e-07, 2.1049e-07, 2.5772e-07, 2.6963e-07,\n",
      "         4.3387e-07, 2.1214e-07, 1.6304e-07, 4.5958e-07, 2.4892e-07, 2.3929e-07,\n",
      "         4.6734e-05],\n",
      "        [3.9367e-02, 2.6359e-04, 3.5124e-04, 1.2428e-04, 3.8177e-05, 1.3787e-05,\n",
      "         6.3433e-03, 6.6029e-06, 1.1145e-06, 2.2295e-05, 5.8751e-02, 2.9120e-04,\n",
      "         8.4209e-01, 2.5668e-03, 2.0541e-06, 3.1561e-03, 2.4061e-06, 2.6494e-06,\n",
      "         2.1506e-06, 1.5433e-06, 2.0317e-06, 1.7373e-06, 1.7594e-06, 1.4954e-06,\n",
      "         3.4153e-06, 1.5474e-06, 1.9778e-06, 2.8211e-06, 2.4213e-06, 1.4239e-06,\n",
      "         4.6579e-02],\n",
      "        [1.0573e-01, 1.7299e-03, 1.9198e-02, 2.5626e-04, 1.3664e-04, 2.5317e-04,\n",
      "         4.4405e-03, 4.7312e-06, 2.4809e-05, 3.8915e-04, 2.6674e-02, 9.6300e-05,\n",
      "         6.1473e-03, 8.2706e-04, 2.2826e-05, 8.5726e-03, 1.7234e-05, 2.6154e-05,\n",
      "         2.4167e-05, 1.6304e-05, 1.5477e-05, 2.4072e-05, 2.8016e-05, 2.8926e-05,\n",
      "         2.5760e-05, 1.6037e-05, 3.0728e-05, 2.4641e-05, 2.8087e-05, 2.1671e-05,\n",
      "         8.2517e-01],\n",
      "        [1.2305e-01, 1.4641e-03, 1.6722e-01, 3.4203e-04, 1.3927e-04, 5.3575e-04,\n",
      "         5.9930e-03, 6.5075e-06, 7.1770e-05, 1.1965e-03, 4.9214e-02, 4.8386e-04,\n",
      "         8.8865e-03, 1.3599e-02, 9.5652e-05, 7.3890e-02, 8.9759e-05, 9.2111e-05,\n",
      "         7.6526e-05, 5.4406e-05, 4.5426e-05, 5.0772e-05, 1.0296e-04, 6.9938e-05,\n",
      "         1.0713e-04, 6.2692e-05, 9.6290e-05, 1.0893e-04, 8.8300e-05, 5.2697e-05,\n",
      "         5.5271e-01],\n",
      "        [1.3148e-01, 8.8235e-04, 1.8506e-02, 4.4374e-04, 1.3158e-04, 2.3884e-04,\n",
      "         6.0685e-03, 3.6429e-06, 9.7034e-05, 7.2189e-04, 6.1826e-02, 1.1682e-04,\n",
      "         2.0358e-02, 6.8437e-04, 8.3403e-05, 1.4650e-02, 6.2636e-05, 1.0531e-04,\n",
      "         7.7014e-05, 6.7314e-05, 5.9830e-05, 7.8465e-05, 1.0238e-04, 1.0405e-04,\n",
      "         8.4212e-05, 5.8129e-05, 1.1016e-04, 9.0195e-05, 9.1158e-05, 7.4302e-05,\n",
      "         7.4254e-01],\n",
      "        [8.9005e-02, 1.1573e-03, 6.1510e-02, 4.9936e-04, 3.0013e-04, 2.5363e-04,\n",
      "         5.4382e-03, 4.8144e-06, 2.3778e-05, 4.5815e-04, 5.2323e-02, 1.1299e-03,\n",
      "         3.9616e-02, 1.6619e-02, 3.1553e-05, 1.7775e-02, 3.5850e-05, 4.3442e-05,\n",
      "         3.7122e-05, 2.4632e-05, 2.6392e-05, 3.0742e-05, 3.9458e-05, 2.8724e-05,\n",
      "         4.3476e-05, 2.5662e-05, 5.4272e-05, 5.2630e-05, 2.7061e-05, 2.4942e-05,\n",
      "         7.1336e-01],\n",
      "        [1.8426e-01, 9.7463e-04, 7.9825e-03, 2.0324e-04, 1.4493e-04, 2.8762e-04,\n",
      "         1.9425e-03, 5.7573e-06, 3.0727e-05, 3.6282e-04, 2.3227e-02, 4.6654e-05,\n",
      "         8.8700e-03, 1.9863e-04, 2.4469e-05, 5.4105e-03, 1.9359e-05, 3.7061e-05,\n",
      "         2.8112e-05, 2.9504e-05, 2.0998e-05, 3.2980e-05, 3.5097e-05, 5.1550e-05,\n",
      "         2.6298e-05, 2.3471e-05, 5.3977e-05, 3.3056e-05, 3.2787e-05, 2.8408e-05,\n",
      "         7.6557e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 7,  2, 15, 13, 15, 12,  2, 30, 13, 11, 15], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "for batch in iter(val_loader):\n",
    "    batch.to(device)\n",
    "    pred_class = rnn.forward(batch.seq[0])\n",
    "    loss_class = loss_fn(F.log_softmax(pred_class[:-1], dim=1), batch.seq_y)\n",
    "    print(loss_class.item())\n",
    "    print(F.softmax(pred_class[:-1], dim=1)[:-1])\n",
    "    print(batch.seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing RNN\n",
    "nodeModel = rnnModel(15, 8, 15)\n",
    "edgeModel = rnnModel(32, 8, 32)\n",
    "node_optimizer = torch.optim.Adam(nodeModel.parameters(), lr=0.01)\n",
    "edge_optimizer = torch.optim.Adam(edgeModel.parameters(), lr=0.01)\n",
    "loss_fn = F.binary_cross_entropy\n",
    "for e in range(100):\n",
    "    for batch in iter(data_loader):\n",
    "        #batch = batch.to(device)\n",
    "        #print(batch)\n",
    "        edge_hidden_state = None\n",
    "        node_hidden_state = None\n",
    "        for idx in range(len(batch.edge_sequence_indices[0])):\n",
    "            # first run node prediction model\n",
    "            #print(\"Node predicition half\")\n",
    "            node_pair = batch.node_sequence_indices[0][idx]\n",
    "            #print(node_pair)\n",
    "            node_pred, node_hidden_state = nodeModel.forward(batch.node_sequence[node_pair[0]:node_pair[1]], batch.x, batch.edge_index, batch.edge_attr, edge_hidden_state)\n",
    "            node_target = batch.node_sequence[node_pair[0]+1:node_pair[1]+1]\n",
    "            \n",
    "            #print(\"Edge predicition half\")\n",
    "            # then run edge prediction model\n",
    "            edge_pair = batch.edge_sequence_indices[0][idx]\n",
    "            #print(edge_pair)\n",
    "            edge_pred, edge_hidden_state = edgeModel.forward(batch.edge_sequence[edge_pair[0]:edge_pair[1]], batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_target = batch.edge_sequence[edge_pair[0]+1:edge_pair[1]+1]\n",
    "            \n",
    "            loss = loss_fn(node_pred, node_target)\n",
    "            node_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            node_optimizer.step()\n",
    "            print(loss.item())\n",
    "            \n",
    "            loss = loss_fn(edge_pred, edge_target)\n",
    "            edge_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            edge_optimizer.step()\n",
    "            print(loss.item())\n",
    "            \n",
    "        #pred = nodeModel.forward(batch.node_sequence[0:-1])\n",
    "        #loss = loss_fn(pred, batch.node_sequence[1:])\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class nnconvnn(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(nnconvnn, self).__init__()\n",
    "        \n",
    "        self.simpleLin = torch.nn.Linear(31, input_dim*output_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.simpleLin.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.simpleLin(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        #self.testnnconv = \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim)))\n",
    "        self.convs.append(NNConv(hidden_dim, hidden_dim, nnconvnn(hidden_dim, hidden_dim)))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        F.dropout.training = self.training\n",
    "        #x = self.testnnconv(x, edge_index, edge_attr)\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        x = self.linear(x)\n",
    "        if self.return_embeds == False:\n",
    "            x = self.softmax(x)\n",
    "        out = x\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loader, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    for batch in iter(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        # o = o[train_idx] # we train on the whole graphs now\n",
    "        #print(o)\n",
    "        #print(batch.y.squeeze())\n",
    "        loss = loss_fn(o, batch.y.squeeze(1))\n",
    "        #########################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, loader, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        batch = batch.to(device)\n",
    "        ############# Your code here ############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. No index slicing here\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        #########################################\n",
    "\n",
    "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        #if count == 1:\n",
    "            #print(y_pred, batch.y)\n",
    "        \n",
    "        train_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        valid_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        test_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "\n",
    "    train_acc /= count\n",
    "    valid_acc /= count\n",
    "    test_acc /= count\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "    }\n",
    "    args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    model = GCN(30, args['hidden_dim'],\n",
    "              14, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.1944, Train: 66.89%, Valid: 66.89% Test: 66.89%\n",
      "Epoch: 02, Loss: 0.7170, Train: 72.64%, Valid: 72.64% Test: 72.64%\n",
      "Epoch: 03, Loss: 0.6501, Train: 74.53%, Valid: 74.53% Test: 74.53%\n",
      "Epoch: 04, Loss: 0.4657, Train: 75.57%, Valid: 75.57% Test: 75.57%\n",
      "Epoch: 05, Loss: 0.4036, Train: 75.54%, Valid: 75.54% Test: 75.54%\n",
      "Epoch: 06, Loss: 0.4142, Train: 75.61%, Valid: 75.61% Test: 75.61%\n",
      "Epoch: 07, Loss: 0.3661, Train: 77.97%, Valid: 77.97% Test: 77.97%\n",
      "Epoch: 08, Loss: 0.3721, Train: 79.85%, Valid: 79.85% Test: 79.85%\n",
      "Epoch: 09, Loss: 0.3096, Train: 80.93%, Valid: 80.93% Test: 80.93%\n",
      "Epoch: 10, Loss: 0.3449, Train: 80.03%, Valid: 80.03% Test: 80.03%\n",
      "Epoch: 11, Loss: 0.3756, Train: 81.84%, Valid: 81.84% Test: 81.84%\n",
      "Epoch: 12, Loss: 0.2886, Train: 79.02%, Valid: 79.02% Test: 79.02%\n",
      "Epoch: 13, Loss: 0.3435, Train: 83.23%, Valid: 83.23% Test: 83.23%\n",
      "Epoch: 14, Loss: 0.3259, Train: 85.24%, Valid: 85.24% Test: 85.24%\n",
      "Epoch: 15, Loss: 0.2378, Train: 86.44%, Valid: 86.44% Test: 86.44%\n",
      "Epoch: 16, Loss: 0.2188, Train: 79.59%, Valid: 79.59% Test: 79.59%\n",
      "Epoch: 17, Loss: 0.3246, Train: 82.01%, Valid: 82.01% Test: 82.01%\n",
      "Epoch: 18, Loss: 0.2081, Train: 85.60%, Valid: 85.60% Test: 85.60%\n",
      "Epoch: 19, Loss: 0.2428, Train: 87.90%, Valid: 87.90% Test: 87.90%\n",
      "Epoch: 20, Loss: 0.2182, Train: 80.85%, Valid: 80.85% Test: 80.85%\n",
      "Epoch: 21, Loss: 0.1706, Train: 87.97%, Valid: 87.97% Test: 87.97%\n",
      "Epoch: 22, Loss: 0.2968, Train: 86.19%, Valid: 86.19% Test: 86.19%\n",
      "Epoch: 23, Loss: 0.2417, Train: 90.28%, Valid: 90.28% Test: 90.28%\n",
      "Epoch: 24, Loss: 0.1411, Train: 89.91%, Valid: 89.91% Test: 89.91%\n",
      "Epoch: 25, Loss: 0.2349, Train: 90.41%, Valid: 90.41% Test: 90.41%\n",
      "Epoch: 26, Loss: 0.2296, Train: 88.76%, Valid: 88.76% Test: 88.76%\n",
      "Epoch: 27, Loss: 0.2621, Train: 91.15%, Valid: 91.15% Test: 91.15%\n",
      "Epoch: 28, Loss: 0.1569, Train: 90.74%, Valid: 90.74% Test: 90.74%\n",
      "Epoch: 29, Loss: 0.1318, Train: 92.58%, Valid: 92.58% Test: 92.58%\n",
      "Epoch: 30, Loss: 0.0976, Train: 91.43%, Valid: 91.43% Test: 91.43%\n",
      "Epoch: 31, Loss: 0.1681, Train: 91.70%, Valid: 91.70% Test: 91.70%\n",
      "Epoch: 32, Loss: 0.3574, Train: 89.84%, Valid: 89.84% Test: 89.84%\n",
      "Epoch: 33, Loss: 0.1642, Train: 83.23%, Valid: 83.23% Test: 83.23%\n",
      "Epoch: 34, Loss: 0.1369, Train: 88.86%, Valid: 88.86% Test: 88.86%\n",
      "Epoch: 35, Loss: 0.1345, Train: 93.12%, Valid: 93.12% Test: 93.12%\n",
      "Epoch: 36, Loss: 0.2281, Train: 92.39%, Valid: 92.39% Test: 92.39%\n",
      "Epoch: 37, Loss: 0.0567, Train: 94.27%, Valid: 94.27% Test: 94.27%\n",
      "Epoch: 38, Loss: 0.0652, Train: 93.86%, Valid: 93.86% Test: 93.86%\n",
      "Epoch: 39, Loss: 0.0877, Train: 94.25%, Valid: 94.25% Test: 94.25%\n",
      "Epoch: 40, Loss: 0.0418, Train: 94.55%, Valid: 94.55% Test: 94.55%\n",
      "Epoch: 41, Loss: 0.0472, Train: 93.71%, Valid: 93.71% Test: 93.71%\n",
      "Epoch: 42, Loss: 0.0649, Train: 94.46%, Valid: 94.46% Test: 94.46%\n",
      "Epoch: 43, Loss: 0.0807, Train: 94.79%, Valid: 94.79% Test: 94.79%\n",
      "Epoch: 44, Loss: 0.2919, Train: 93.40%, Valid: 93.40% Test: 93.40%\n",
      "Epoch: 45, Loss: 0.0402, Train: 94.38%, Valid: 94.38% Test: 94.38%\n",
      "Epoch: 46, Loss: 0.1923, Train: 90.43%, Valid: 90.43% Test: 90.43%\n",
      "Epoch: 47, Loss: 0.0497, Train: 94.54%, Valid: 94.54% Test: 94.54%\n",
      "Epoch: 48, Loss: 0.0750, Train: 93.70%, Valid: 93.70% Test: 93.70%\n",
      "Epoch: 49, Loss: 0.0650, Train: 94.19%, Valid: 94.19% Test: 94.19%\n",
      "Epoch: 50, Loss: 0.0899, Train: 94.96%, Valid: 94.96% Test: 94.96%\n",
      "Epoch: 51, Loss: 0.1130, Train: 95.36%, Valid: 95.36% Test: 95.36%\n",
      "Epoch: 52, Loss: 0.1710, Train: 92.79%, Valid: 92.79% Test: 92.79%\n",
      "Epoch: 53, Loss: 0.1214, Train: 94.55%, Valid: 94.55% Test: 94.55%\n",
      "Epoch: 54, Loss: 0.0303, Train: 95.58%, Valid: 95.58% Test: 95.58%\n",
      "Epoch: 55, Loss: 0.0599, Train: 95.72%, Valid: 95.72% Test: 95.72%\n",
      "Epoch: 56, Loss: 0.0282, Train: 94.79%, Valid: 94.79% Test: 94.79%\n",
      "Epoch: 57, Loss: 0.0231, Train: 94.54%, Valid: 94.54% Test: 94.54%\n",
      "Epoch: 58, Loss: 0.0583, Train: 94.07%, Valid: 94.07% Test: 94.07%\n",
      "Epoch: 59, Loss: 0.0671, Train: 95.25%, Valid: 95.25% Test: 95.25%\n",
      "Epoch: 60, Loss: 0.0661, Train: 94.95%, Valid: 94.95% Test: 94.95%\n",
      "Epoch: 61, Loss: 0.0949, Train: 95.04%, Valid: 95.04% Test: 95.04%\n",
      "Epoch: 62, Loss: 0.0665, Train: 94.68%, Valid: 94.68% Test: 94.68%\n",
      "Epoch: 63, Loss: 0.0248, Train: 95.82%, Valid: 95.82% Test: 95.82%\n",
      "Epoch: 64, Loss: 0.1172, Train: 94.49%, Valid: 94.49% Test: 94.49%\n",
      "Epoch: 65, Loss: 0.0229, Train: 95.17%, Valid: 95.17% Test: 95.17%\n",
      "Epoch: 66, Loss: 0.2187, Train: 95.48%, Valid: 95.48% Test: 95.48%\n",
      "Epoch: 67, Loss: 0.0234, Train: 96.01%, Valid: 96.01% Test: 96.01%\n",
      "Epoch: 68, Loss: 0.0167, Train: 96.47%, Valid: 96.47% Test: 96.47%\n",
      "Epoch: 69, Loss: 0.0147, Train: 95.39%, Valid: 95.39% Test: 95.39%\n",
      "Epoch: 70, Loss: 0.0852, Train: 95.82%, Valid: 95.82% Test: 95.82%\n",
      "Epoch: 71, Loss: 0.0816, Train: 95.37%, Valid: 95.37% Test: 95.37%\n",
      "Epoch: 72, Loss: 0.2787, Train: 95.82%, Valid: 95.82% Test: 95.82%\n",
      "Epoch: 73, Loss: 0.1199, Train: 94.46%, Valid: 94.46% Test: 94.46%\n",
      "Epoch: 74, Loss: 0.0509, Train: 96.31%, Valid: 96.31% Test: 96.31%\n",
      "Epoch: 75, Loss: 0.0259, Train: 96.11%, Valid: 96.11% Test: 96.11%\n",
      "Epoch: 76, Loss: 0.0321, Train: 95.65%, Valid: 95.65% Test: 95.65%\n",
      "Epoch: 77, Loss: 0.0306, Train: 96.44%, Valid: 96.44% Test: 96.44%\n",
      "Epoch: 78, Loss: 0.0297, Train: 96.29%, Valid: 96.29% Test: 96.29%\n",
      "Epoch: 79, Loss: 0.0074, Train: 94.78%, Valid: 94.78% Test: 94.78%\n",
      "Epoch: 80, Loss: 0.0536, Train: 96.21%, Valid: 96.21% Test: 96.21%\n",
      "Epoch: 81, Loss: 0.0623, Train: 96.84%, Valid: 96.84% Test: 96.84%\n",
      "Epoch: 82, Loss: 0.0457, Train: 96.28%, Valid: 96.28% Test: 96.28%\n",
      "Epoch: 83, Loss: 0.0513, Train: 96.18%, Valid: 96.18% Test: 96.18%\n",
      "Epoch: 84, Loss: 0.0163, Train: 96.04%, Valid: 96.04% Test: 96.04%\n",
      "Epoch: 85, Loss: 0.0297, Train: 95.40%, Valid: 95.40% Test: 95.40%\n",
      "Epoch: 86, Loss: 0.0700, Train: 96.06%, Valid: 96.06% Test: 96.06%\n",
      "Epoch: 87, Loss: 0.0582, Train: 95.29%, Valid: 95.29% Test: 95.29%\n",
      "Epoch: 88, Loss: 0.0113, Train: 95.17%, Valid: 95.17% Test: 95.17%\n",
      "Epoch: 89, Loss: 0.0494, Train: 95.24%, Valid: 95.24% Test: 95.24%\n",
      "Epoch: 90, Loss: 0.0276, Train: 95.55%, Valid: 95.55% Test: 95.55%\n",
      "Epoch: 91, Loss: 0.0085, Train: 95.31%, Valid: 95.31% Test: 95.31%\n",
      "Epoch: 92, Loss: 0.0747, Train: 96.43%, Valid: 96.43% Test: 96.43%\n",
      "Epoch: 93, Loss: 0.0152, Train: 96.54%, Valid: 96.54% Test: 96.54%\n",
      "Epoch: 94, Loss: 0.0521, Train: 96.91%, Valid: 96.91% Test: 96.91%\n",
      "Epoch: 95, Loss: 0.0156, Train: 97.09%, Valid: 97.09% Test: 97.09%\n",
      "Epoch: 96, Loss: 0.0873, Train: 96.98%, Valid: 96.98% Test: 96.98%\n",
      "Epoch: 97, Loss: 0.0253, Train: 96.73%, Valid: 96.73% Test: 96.73%\n",
      "Epoch: 98, Loss: 0.1190, Train: 96.53%, Valid: 96.53% Test: 96.53%\n",
      "Epoch: 99, Loss: 0.0233, Train: 96.99%, Valid: 96.99% Test: 96.99%\n",
      "Epoch: 100, Loss: 0.0133, Train: 96.25%, Valid: 96.25% Test: 96.25%\n"
     ]
    }
   ],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    # reset the parameters to initial random value\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    loss_fn = F.nll_loss\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "        loss = train(model, data_loader, [], optimizer, loss_fn)\n",
    "        result = test(model, test_loader, [], evaluator)\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_acc:.2f}%, '\n",
    "              f'Valid: {100 * valid_acc:.2f}% '\n",
    "              f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(model, data_loader, [], evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
