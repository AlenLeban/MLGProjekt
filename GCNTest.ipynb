{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the modified to_networkx function that doesn't throw exceptions\n",
    "\n",
    "def from_networkx(\n",
    "    G: Any,\n",
    "    group_node_attrs: Optional[Union[List[str], all]] = None,\n",
    "    group_edge_attrs: Optional[Union[List[str], all]] = None,\n",
    ") -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "        group_node_attrs (List[str] or all, optional): The node attributes to\n",
    "            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)\n",
    "        group_edge_attrs (List[str] or all, optional): The edge attributes to\n",
    "            be concatenated and added to :obj:`data.edge_attr`.\n",
    "            (default: :obj:`None`)\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must\n",
    "        be numeric.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> edge_index = torch.tensor([\n",
    "        ...     [0, 1, 1, 2, 2, 3],\n",
    "        ...     [1, 0, 2, 1, 3, 2],\n",
    "        ... ])\n",
    "        >>> data = Data(edge_index=edge_index, num_nodes=4)\n",
    "        >>> g = to_networkx(data)\n",
    "        >>> # A `Data` object is returned\n",
    "        >>> from_networkx(g)\n",
    "        Data(edge_index=[2, 6], num_nodes=4)\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        edges = list(G.edges(keys=False))\n",
    "    else:\n",
    "        edges = list(G.edges)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    if G.number_of_nodes() > 0:\n",
    "        node_attrs = list(next(iter(G.nodes(data=True)))[-1].keys())\n",
    "    else:\n",
    "        node_attrs = {}\n",
    "\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_attrs = list(next(iter(G.edges(data=True)))[-1].keys())\n",
    "    else:\n",
    "        edge_attrs = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        if set(feat_dict.keys()) != set(node_attrs):\n",
    "            raise ValueError('Not all nodes contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        if set(feat_dict.keys()) != set(edge_attrs):\n",
    "            raise ValueError('Not all edges contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for key, value in G.graph.items():\n",
    "        key = f'graph_{key}' if key in node_attrs else key\n",
    "        data[str(key)] = value\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (tuple, list)) and isinstance(value[0], Tensor):\n",
    "            data[key] = torch.stack(value, dim=0)\n",
    "        else:\n",
    "            try:\n",
    "                data[key] = torch.tensor(value)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = Data.from_dict(data)\n",
    "\n",
    "    if group_node_attrs is all:\n",
    "        group_node_attrs = list(node_attrs)\n",
    "    if group_node_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_node_attrs:\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if group_edge_attrs is all:\n",
    "        group_edge_attrs = list(edge_attrs)\n",
    "    if group_edge_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_edge_attrs:\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.edge_attr = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if data.x is None and data.pos is None:\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "def to_edge_index(adj: Union[Tensor, SparseTensor]) -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Converts a :class:`torch.sparse.Tensor` or a\n",
    "    :class:`torch_sparse.SparseTensor` to edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.sparse.Tensor or SparseTensor): The adjacency matrix.\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "        ...                            [1, 0, 2, 1, 3, 2]])\n",
    "        >>> adj = to_torch_coo_tensor(edge_index)\n",
    "        >>> to_edge_index(adj)\n",
    "        (tensor([[0, 1, 1, 2, 2, 3],\n",
    "                [1, 0, 2, 1, 3, 2]]),\n",
    "        tensor([1., 1., 1., 1., 1., 1.]))\n",
    "    \"\"\"\n",
    "    if isinstance(adj, SparseTensor):\n",
    "        row, col, value = adj.coo()\n",
    "        if value is None:\n",
    "            value = torch.ones(row.size(0), device=row.device)\n",
    "        return torch.stack([row, col], dim=0), value\n",
    "\n",
    "    if adj.requires_grad:\n",
    "        # Calling adj._values() will return a detached tensor.\n",
    "        # Use `adj.coalesce().values()` instead to track gradients.\n",
    "        adj = adj.coalesce()\n",
    "        return adj.indices(), adj.values()\n",
    "\n",
    "    return adj._indices(), adj._values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "        \"Point\": 0,\n",
    "        \"Line\": 1,\n",
    "        \"Circle\": 2,\n",
    "        \"Ellipse\": 3,\n",
    "        \"Spline\": 4,\n",
    "        \"Conic\": 5,\n",
    "        \"Arc\": 6,\n",
    "        \"External\": 7,\n",
    "        \"Stop\": 8,\n",
    "        \"Unknown\": 9,\n",
    "        \"SN_Start\": 11,\n",
    "        \"SN_End\": 12,\n",
    "        \"SN_Center\": 13\n",
    "    }\n",
    "\n",
    "edge_dict = {\n",
    "    \"Coincident\": 0,\n",
    "    \"Projected\": 1,\n",
    "    \"Mirror\": 2,\n",
    "    \"Distance\": 3,\n",
    "    \"Horizontal\": 4,\n",
    "    \"Parallel\": 5,\n",
    "    \"Vertical\": 6,\n",
    "    \"Tangent\": 7,\n",
    "    \"Length\": 8,\n",
    "    \"Perpendicular\": 9,\n",
    "    \"Midpoint\": 10,\n",
    "    \"Equal\": 11,\n",
    "    \"Diameter\": 12,\n",
    "    \"Offset\": 13,\n",
    "    \"Radius\": 14,\n",
    "    \"Concentric\": 15,\n",
    "    \"Fix\": 16,\n",
    "    \"Angle\": 17,\n",
    "    \"Circular_Pattern\": 18,\n",
    "    \"Pierce\": 19,\n",
    "    \"Linear_Pattern\": 20,\n",
    "    \"Centerline_Dimension\": 21,\n",
    "    \"Intersected\": 22,\n",
    "    \"Silhoutted\": 23,\n",
    "    \"Quadrant\": 24,\n",
    "    \"Normal\": 25,\n",
    "    \"Minor_Diameter\": 26,\n",
    "    \"Major_Diameter\": 27,\n",
    "    \"Rho\": 28,\n",
    "    \"Unknown\": 29,\n",
    "    \"Subnode\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "def get_sketch_features(graph, feature_dim):\n",
    "    x = torch.zeros([graph.num_nodes, feature_dim])\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx, p in enumerate(graph.parameters):\n",
    "        \n",
    "        # add one hot encoding to feature vector for node label\n",
    "        #onePos = label_dict[graph.label[idx]]/7\n",
    "        #for i in range(0, 14):\n",
    "        #    x[idx, i] = 1 if onePos==i else 0\n",
    "        x[idx, label_dict[graph.label[idx]]] = 1\n",
    "        # convert label text into a feature value\n",
    "        #x[idx, 14] = label_dict[graph.label[idx]]/7\n",
    "        \n",
    "        param_dict = json.loads(p)\n",
    "        for i, k in enumerate(param_dict.keys()):\n",
    "            \n",
    "            if i+2 == feature_dim:\n",
    "                break\n",
    "            \n",
    "            # convert each parameter value into a feature value\n",
    "            x[idx, i+15] = float(param_dict[k])\n",
    "        \n",
    "        x[idx, -1] = degree(graph.edge_index[0], graph.num_nodes)[idx]\n",
    "        #print(idx, p)\n",
    "        #print(x[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_attr_y(graph):\n",
    "    y = torch.zeros([graph.num_nodes, 1], dtype=torch.int64)\n",
    "    #rint(graph.label)\n",
    "    \n",
    "    \n",
    "    for i, l in enumerate(graph.label):\n",
    "        y[i, 0] = label_dict[l]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_adj(graph):\n",
    "    tst = T.ToSparseTensor()\n",
    "    return tst(graph).adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_large_graph(graphs):\n",
    "    bestN = 0\n",
    "    bestI = 0\n",
    "    for i, g in enumerate(graphs):\n",
    "        if len(g) > bestN:\n",
    "            bestN = len(g)\n",
    "            bestI = i\n",
    "    #print(bestI)\n",
    "    return graphs[bestI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_edge_attr(graph):\n",
    "    dim = 31\n",
    "    edge_attr = torch.zeros([len(graph.edge_label), dim])\n",
    "    for idx, l in enumerate(graph.edge_label):\n",
    "        edge_attr[idx, edge_dict[l]] = 1\n",
    "    return edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for now we will do node class sequence prediction\n",
    "\n",
    "def get_sketch_construction_node_sequence(sg, graph):\n",
    "    edge_sequence_indices = []\n",
    "    node_sequence_indices = []\n",
    "    node_sequence = torch.zeros((0, 15), dtype=torch.float) # 15th element is stop sign\n",
    "    edge_sequence = torch.zeros((0, 32), dtype=torch.float) # 32nd element is stop sign\n",
    "    start_node_idx=0\n",
    "    start_edge_idx=0\n",
    "    node_idx = 0\n",
    "    edge_idx=0\n",
    "    is_edge_sequence = False\n",
    "    for elem in sg:\n",
    "        \n",
    "        if node_idx == len(graph.label):\n",
    "            break\n",
    "            \n",
    "        if type(elem) == sketchgraphs.data.sequence.NodeOp:\n",
    "            if is_edge_sequence:\n",
    "                is_edge_sequence=False\n",
    "                stop_token = torch.zeros((1, 32))\n",
    "                stop_token[0, 31] = 1\n",
    "                edge_sequence = torch.cat((edge_sequence, stop_token))\n",
    "                edge_sequence_indices.append((start_edge_idx, edge_idx))\n",
    "                start_node_idx=node_idx\n",
    "                edge_idx+=1\n",
    "            one_hot = torch.zeros((1, 15))\n",
    "            one_hot[0, label_dict[graph.label[node_idx]]] = 1\n",
    "            node_sequence = torch.cat((node_sequence, one_hot))\n",
    "            node_idx+=1\n",
    "        elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "            #print(elem)\n",
    "            if not is_edge_sequence:\n",
    "                is_edge_sequence=True\n",
    "                start_edge_idx=edge_idx\n",
    "                stop_token = torch.zeros((1, 15))\n",
    "                stop_token[0, 14] = 1\n",
    "                node_sequence = torch.cat((node_sequence, stop_token))\n",
    "                node_sequence_indices.append((start_node_idx, node_idx))\n",
    "                node_idx+=1\n",
    "            constraintNumber = edge_dict[graph.edge_label[edge_idx]]\n",
    "            one_hot = torch.zeros((1, 32))\n",
    "            one_hot[0, constraintNumber] = 1\n",
    "            edge_sequence = torch.cat((edge_sequence, one_hot))\n",
    "            edge_idx+=1\n",
    "            \n",
    "    #node_sequence[-1, 14] = 1\n",
    "    #edge_sequence[-1, 31] = 1\n",
    "    #print(node_sequence_indices, edge_sequence_indices)\n",
    "    return node_sequence, edge_sequence, node_sequence_indices, edge_sequence_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sketchgraph_node_constraint_sequence(sg, graph):\n",
    "    #y = torch.zeros((len(sg)-1, 2+31)) # 2 classes (node or edge), 14 node types (but contained in 31 positions for edge types), 31 edge types\n",
    "    #node_idx=0\n",
    "    #edge_idx=0\n",
    "    #for idx, elem in enumerate(sg):\n",
    "    #    if type(elem) == sketchgraphs.data.sequence.NodeOp and not elem.label == sketchgraphs.data._entity.EntityType.Stop:\n",
    "    #        y[idx, 0] = 1\n",
    "    #        y[idx, 2+label_dict[graph.label[node_idx]]] = 1  # 0.5 because we're marking 2 spaces in a vector of zeroes, to sum up to 1\n",
    "    #        node_idx+=1\n",
    "    #    elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "    #        y[idx, 1] = 1\n",
    "    #        y[idx, 2+edge_dict[graph.edge_label[edge_idx]]] = 1\n",
    "    #        edge_idx+=1\n",
    "    #return y\n",
    "    y = torch.zeros((len(sg)-1), dtype=torch.long)\n",
    "    node_idx=0\n",
    "    edge_idx=0\n",
    "    for idx, elem in enumerate(sg):\n",
    "        if type(elem) == sketchgraphs.data.sequence.NodeOp and not elem.label == sketchgraphs.data._entity.EntityType.Stop:\n",
    "            y[idx] = label_dict[graph.label[node_idx]]  # 0.5 because we're marking 2 spaces in a vector of zeroes, to sum up to 1\n",
    "            node_idx+=1\n",
    "        elif type(elem) == sketchgraphs.data.sequence.EdgeOp:\n",
    "            y[idx] = 15+edge_dict[graph.edge_label[edge_idx]]\n",
    "            edge_idx+=1\n",
    "    return y\n",
    "\n",
    "\n",
    "def convert_sketchgraph_to_pytorch(sg):\n",
    "    # convert first to pyGraphViz graph using sketchgraph's function\n",
    "    pgv_graph = sketchgraphs.data.sequence.pgvgraph_from_sequence(sg)\n",
    "    # then to networkx graph\n",
    "    nx_graph = nx.Graph(pgv_graph)\n",
    "    # finally to pyTorch graph\n",
    "    graph = from_networkx(nx_graph)\n",
    "    return graph\n",
    "\n",
    "def assign_attributes_to_graph(graph):\n",
    "    graph.x = get_sketch_features(graph, 30)\n",
    "    graph.y = get_sketch_attr_y(graph)\n",
    "    graph.adj_t = get_sketch_adj(graph)\n",
    "    graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "    graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "    return graph\n",
    "\n",
    "def get_sketchgraph_graph_sequence(sg):\n",
    "    generated_graph = []\n",
    "    sequence = []\n",
    "    for elem in sg:\n",
    "        generated_graph.append(elem)\n",
    "        new_graph = convert_sketchgraph_to_pytorch(generated_graph)\n",
    "        if not hasattr(new_graph, 'edge_label'):\n",
    "            new_graph.edge_label = []\n",
    "        new_graph = assign_attributes_to_graph(new_graph)\n",
    "        sequence.append(new_graph)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def custom_collate(batch, b):\\n    print(\"AAAAA\")\\n    edge_label_batch = []\\n    for b in batch:\\n        #print(b.edge_label)\\n        edge_label_batch.append(b.edge_label)\\n    return edge_label_batch'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom dataset class of custom attributes\n",
    "from torch_geometric.data import Dataset\n",
    "class SketchgraphDataset(Dataset):\n",
    "    def __init__(self, start_idx, end_idx, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data = []\n",
    "        seq_data = flat_array.load_dictionary_flat('datasets/sg_t16_validation.npy')\n",
    "        print(len(seq_data['sequences']))\n",
    "\n",
    "        #test_graph_seq = find_large_graph(seq_data['sequences'])\n",
    "        test_graph_seq = seq_data['sequences'][206778]\n",
    "        test_graph_seq1 = seq_data['sequences'][10]\n",
    "\n",
    "        sketchgraps_list = seq_data['sequences'][start_idx:end_idx]\n",
    "\n",
    "        #link_transform = T.RandomLinkSplit(is_undirected=True, key=\"edge_attr\")\n",
    "        \n",
    "        for sg in sketchgraps_list:\n",
    "            #print(test_graph_seq)\n",
    "            graph = convert_sketchgraph_to_pytorch(sg)\n",
    "            \n",
    "            if not hasattr(graph, 'edge_label'):\n",
    "                continue\n",
    "\n",
    "            # next we need to add required attributes: x, y, adj_t\n",
    "            graph.x = get_sketch_features(graph, 30)\n",
    "            graph.y = get_sketch_attr_y(graph)\n",
    "            graph.adj_t = get_sketch_adj(graph)\n",
    "            graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "            graph.edge_attr = get_sketch_edge_attr(graph)\n",
    "            graph.seq = get_sketchgraph_graph_sequence(sg)\n",
    "            graph.seq_y = get_sketchgraph_node_constraint_sequence(sg, graph)\n",
    "            #graph.node_sequence, graph.edge_sequence, graph.node_sequence_indices, graph.edge_sequence_indices = get_sketch_construction_node_sequence(sg, graph)\n",
    "            #print(len(graph.edge_label))\n",
    "            #graph = link_transform(graph)\n",
    "            self.data.append(graph)\n",
    "            \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\"\"\"def custom_collate(batch, b):\n",
    "    print(\"AAAAA\")\n",
    "    edge_label_batch = []\n",
    "    for b in batch:\n",
    "        #print(b.edge_label)\n",
    "        edge_label_batch.append(b.edge_label)\n",
    "    return edge_label_batch\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "315228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  1, 11, 15, 12, 45,  1, 45, 20, 11, 15, 12, 45,  0, 15,  1, 11, 45,\n",
       "        15, 12, 20,  1, 19, 45, 11, 45, 12, 15,  1, 45, 11, 15, 45, 12, 15, 15,\n",
       "         1, 45, 11, 45, 20, 15, 12, 45, 15,  1, 11, 45, 15, 12, 20,  1, 19, 45,\n",
       "        11, 45, 12, 24,  1, 45, 11, 15, 45, 12, 15, 24,  1, 45, 11, 45, 20, 15,\n",
       "        12, 45, 15,  0, 45, 20,  0, 45, 45])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sketchgraphs\n",
    "import networkx as nx\n",
    "from sketchgraphs.data import flat_array\n",
    "\n",
    "train_dataset = SketchgraphDataset(0, 300)\n",
    "test_dataset = SketchgraphDataset(101, 111)\n",
    "    \n",
    "data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "next(iter(data_loader)).seq_y\n",
    "#for b in iter(data_loader):\n",
    "#    print(b)\n",
    "#print(graph)\n",
    "#print(graph.x)\n",
    "#print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n"
     ]
    }
   ],
   "source": [
    "#print(next(iter(data_loader)).x)\n",
    "val_dataset = SketchgraphDataset(500, 700)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class linkPredictionGAE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(linkPredictionGAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.conv1 = NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim))\n",
    "        self.conv2 = NNConv(hidden_dim, output_dim, nnconvnn(hidden_dim, output_dim))\n",
    "        \n",
    "    def encode(self, x, pos_edge_index, edge_attr):\n",
    "        x = self.conv1(x, pos_edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, pos_edge_index, edge_attr)\n",
    "    \n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "        return logits\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n",
      "Device: cuda\n",
      "tensor([     0,      1,      2,  ..., 169145, 169148, 169251])\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    dataset_name = 'ogbn-arxiv'\n",
    "    dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    #data = batch\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the adjacency matrix to symmetric\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    #print(data.y)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #device = 'cpu'\n",
    "    # If you use GPU, the device should be cuda\n",
    "    print('Device: {}'.format(device))\n",
    "    data = data.to(device)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    print(split_idx['train'])\n",
    "    #train_idx = torch.LongTensor(range(0, data.num_nodes)).to(device)\n",
    "    #print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1\n",
    "    return link_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_link_predictor(model):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "    loss_avg = 0\n",
    "    batch_count = 0\n",
    "    for batch in iter(data_loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_count+=1\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index,\n",
    "            num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z = model.encode(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        link_logits = model.decode(z, batch.edge_index, neg_edge_index)\n",
    "        link_labels = get_link_labels(batch.edge_index, neg_edge_index)\n",
    "        loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "    return loss_avg / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    perf=0\n",
    "    batch_count = 0\n",
    "    for batch in iter(test_loader):\n",
    "        batch = batch.to(device)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index,\n",
    "            num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        z = model.encode(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        link_logits = model.decode(z, batch.edge_index, neg_edge_index)\n",
    "        link_probs = link_logits.sigmoid()\n",
    "        link_labels = get_link_labels(batch.edge_index, neg_edge_index)\n",
    "        try:\n",
    "            perf+=roc_auc_score(link_labels.cpu().detach().numpy(), link_probs.cpu().detach().numpy())\n",
    "            batch_count+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return perf/batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linkPredictionGAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# training link prediction model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlinkPredictionGAE\u001b[49m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linkPredictionGAE' is not defined"
     ]
    }
   ],
   "source": [
    "# training link prediction model\n",
    "\n",
    "model = linkPredictionGAE(30, 128, 64).to(device)\n",
    "\n",
    "epochs = 5000\n",
    "for e in range(epochs):\n",
    "    train_loss = train_link_predictor(model)\n",
    "    perf = test(model)\n",
    "    print(train_loss, perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
      "         4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
      "         7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n",
      "        [0, 1, 2, 5, 6, 7, 0, 1, 2, 5, 6, 7, 0, 1, 2, 4, 5, 6, 7, 9, 3, 4, 8, 9,\n",
      "         2, 3, 4, 7, 8, 9, 0, 1, 2, 5, 6, 7, 0, 1, 2, 5, 6, 7, 0, 1, 2, 4, 5, 6,\n",
      "         7, 9, 3, 4, 8, 9, 2, 3, 4, 7, 8, 9]], device='cuda:0')\n",
      "tensor([[0, 1, 2, 2, 2, 3, 4, 4, 5, 6, 7, 7, 7, 8, 9, 9],\n",
      "        [2, 2, 0, 1, 4, 4, 2, 3, 7, 7, 5, 6, 9, 9, 7, 8]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "testGraph = next(iter(test_loader)).to(device)\n",
    "z = model.encode(testGraph.x, testGraph.edge_index, testGraph.edge_attr)\n",
    "final_edge_index = model.decode_all(z)\n",
    "print(final_edge_index)\n",
    "print(testGraph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rnnModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim):\n",
    "        super(rnnModel, self).__init__()\n",
    "        self.layer_count = 2\n",
    "        self.batch_size = 64\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preprocess_gcn = NNConv(30, hidden_size, nnconvnn(30, hidden_size))\n",
    "        #self.gcn_mlp = torch.nn.Linear(hidden_size, self.layer_count)\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_size, self.layer_count) # batch first = True means the first dimension is batch size\n",
    "        self.mlp = torch.nn.Linear(hidden_size, output_dim)\n",
    "        # x -> batch_size x sequence_length x input_dim if we wanted many to one RNN\n",
    "        # for many to many, we use sequence length of 1\n",
    "        self.hidden_state = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.rnn.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, graph_x, edge_index, edge_attr, prev_hid=None):\n",
    "        #print(self.rnn._flat_weights[0].dtype, x.dtype)\n",
    "        #self.hidden_state.to(device)\n",
    "        hidden=None\n",
    "        if prev_hid==None:\n",
    "            hidden = self.preprocess_gcn(graph_x, edge_index, edge_attr)\n",
    "        #hidden = self.gcn_mlp(hidden)\n",
    "            hidden = global_mean_pool(hidden, torch.zeros((graph_x.size(0)), dtype=torch.long))\n",
    "            hidden = torch.cat((hidden, hidden))\n",
    "        else:\n",
    "            hidden=prev_hid\n",
    "            \n",
    "        x, self.hidden_state = self.rnn(x, hidden)\n",
    "        x = self.mlp(x)\n",
    "        torch.nn.LogSoftmax(dim=-1)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x, self.hidden_state.detach()\n",
    "    \n",
    "    def generate(self, graph_x, ):\n",
    "        pass\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class nnconvnn(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(nnconvnn, self).__init__()\n",
    "        \n",
    "        self.simpleLin = torch.nn.Linear(31, input_dim*output_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.simpleLin.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.simpleLin(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class newrnnmodel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim_second, num_nnconv_layers):\n",
    "        super(newrnnmodel, self).__init__()\n",
    "        self.layer_count = 2\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.input_gcn = torch.nn.ModuleList()\n",
    "        self.input_gcn.append(NNConv(30, hidden_size, nnconvnn(30, hidden_size)))\n",
    "        #self.bns = torch.nn.ModuleList()\n",
    "        for i in range(num_nnconv_layers-1):\n",
    "            self.input_gcn.append(NNConv(hidden_size, hidden_size, nnconvnn(hidden_size, hidden_size)))\n",
    "            \n",
    "        #for i in range(num_nnconv_layers-1):\n",
    "        #    self.bns.append(torch.nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "        self.rnn_class = torch.nn.RNN(hidden_size, hidden_size, self.layer_count) # batch first = True means the first dimension is batch size\n",
    "\n",
    "        self.mlp_pre_rnn = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.mlp_class = torch.nn.Linear(hidden_size, output_dim_second)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.rnn_class.reset_parameters()\n",
    "        self.mlp_class.reset_parameters()\n",
    "        for l in range(len(self.input_gcn)):\n",
    "            self.input_gcn[l].reset_parameters()\n",
    "        \n",
    "    def forward(self, graph_sequence):\n",
    "\n",
    "        hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float).to(device)\n",
    "        F.dropout.training = self.training\n",
    "        output = torch.tensor([]).to(device)\n",
    "        for g in graph_sequence:\n",
    "            x = g.x\n",
    "            for l in range(len(self.input_gcn)):\n",
    "                x = self.input_gcn[l](x, g.edge_index, g.edge_attr)\n",
    "                x = F.relu(x)\n",
    "                #x = F.dropout(x, p=0.2)\n",
    "            x = global_mean_pool(x, torch.zeros((g.x.size(0)), dtype=torch.long).to(device))\n",
    "            \n",
    "            x = self.mlp_pre_rnn(x)\n",
    "            \n",
    "            x, hidden = self.rnn_class(x, hidden)\n",
    "            \n",
    "            x = self.mlp_class(x)\n",
    "        \n",
    "            x = self.softmax(x)\n",
    "            \n",
    "            output = torch.cat((output, x))\n",
    "        return output, hidden.detach()\n",
    "        \n",
    "    def predict_next(self, sequence):\n",
    "        pred, hidden = self.forward(sequence)\n",
    "        return torch.argmax(pred[-1], dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        #hidden = torch.zeros((self.layer_count, self.hidden_size), dtype=torch.float)\n",
    "        #return hidden\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.085012123138211 Acc: 39.17942178718779%\n",
      "1.666133288157026 Acc: 48.20912219759601%\n",
      "1.5573732196088619 Acc: 50.58895868659086%\n",
      "1.48229676375421 Acc: 54.07953633750664%\n",
      "1.1791081705121291 Acc: 64.15743618389408%\n",
      "1.08879535682624 Acc: 66.72323824474009%\n",
      "1.0097746913847716 Acc: 67.50647380471196%\n",
      "0.9837427906766784 Acc: 69.00742232387098%\n",
      "0.9742474609294464 Acc: 69.13851806216064%\n",
      "0.9834740185069799 Acc: 68.35246355409387%\n",
      "0.9798067321247081 Acc: 68.92727099616673%\n",
      "0.974554391858171 Acc: 68.23420336620252%\n",
      "0.9364402917117179 Acc: 69.64798486470889%\n",
      "0.9377042466033263 Acc: 69.65832404277386%\n",
      "1.0009042897252334 Acc: 67.95296438326463%\n",
      "0.9970688350423921 Acc: 68.05789546986028%\n",
      "0.9077694527181893 Acc: 69.67256946287162%\n",
      "0.9400986725321183 Acc: 69.12506865858643%\n",
      "0.8980874640736293 Acc: 70.45778186129382%\n",
      "0.8798653557647431 Acc: 70.83856483605963%\n"
     ]
    }
   ],
   "source": [
    "rnn = newrnnmodel(15, 16, 31+15, 2).to(device)\n",
    "\n",
    "optimizer_class = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
    "loss_fn = F.nll_loss\n",
    "#loss_fn = F.mse_loss\n",
    "epochs = 20\n",
    "rnn.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    acc = 0\n",
    "    for batch in iter(data_loader):\n",
    "        count+=1\n",
    "        batch.to(device)\n",
    "\n",
    "        optimizer_class.zero_grad()\n",
    "        pred_class, _ = rnn.forward(batch.seq[0])\n",
    "        #print(pred_type)\n",
    "        #print(pred_class)\n",
    "        #print(batch.seq_y.size())\n",
    "        loss_class = loss_fn(pred_class[:-1], batch.seq_y)\n",
    "        pred_index = torch.argmax(pred_class[:-1], dim=1)\n",
    "        acc += torch.sum(pred_index==batch.seq_y).item() / len(batch.seq_y)\n",
    "        loss_class.backward()\n",
    "        \n",
    "        loss_avg += loss_class.item()\n",
    "        \n",
    "\n",
    "        optimizer_class.step()\n",
    "        \n",
    "        \n",
    "    print(loss_avg/count, \"Acc: \" + str(acc/count * 100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765283055754021 Acc: 73.83273358869177%\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "acc = 0\n",
    "count = 0\n",
    "loss_avg = 0\n",
    "for batch in iter(val_loader):\n",
    "    count+=1\n",
    "    batch.to(device)\n",
    "    pred_class,_ = rnn.forward(batch.seq[0])\n",
    "    loss_class = loss_fn(pred_class[:-1], batch.seq_y)\n",
    "    pred_index = torch.argmax(pred_class[:-1], dim=1)\n",
    "    acc += torch.sum(pred_index==batch.seq_y).item() / len(batch.seq_y)\n",
    "    #print(loss_class.item())\n",
    "    #print(F.softmax(pred_class, dim=-1)[:-1])\n",
    "    #print(batch.seq_y)\n",
    "    loss_avg += loss_class.item()\n",
    "print(loss_avg/count, \"Acc: \" + str(acc/count * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing RNN\n",
    "nodeModel = rnnModel(15, 8, 15)\n",
    "edgeModel = rnnModel(32, 8, 32)\n",
    "node_optimizer = torch.optim.Adam(nodeModel.parameters(), lr=0.01)\n",
    "edge_optimizer = torch.optim.Adam(edgeModel.parameters(), lr=0.01)\n",
    "loss_fn = F.binary_cross_entropy\n",
    "for e in range(100):\n",
    "    for batch in iter(data_loader):\n",
    "        #batch = batch.to(device)\n",
    "        #print(batch)\n",
    "        edge_hidden_state = None\n",
    "        node_hidden_state = None\n",
    "        for idx in range(len(batch.edge_sequence_indices[0])):\n",
    "            # first run node prediction model\n",
    "            #print(\"Node predicition half\")\n",
    "            node_pair = batch.node_sequence_indices[0][idx]\n",
    "            #print(node_pair)\n",
    "            node_pred, node_hidden_state = nodeModel.forward(batch.node_sequence[node_pair[0]:node_pair[1]], batch.x, batch.edge_index, batch.edge_attr, edge_hidden_state)\n",
    "            node_target = batch.node_sequence[node_pair[0]+1:node_pair[1]+1]\n",
    "            \n",
    "            #print(\"Edge predicition half\")\n",
    "            # then run edge prediction model\n",
    "            edge_pair = batch.edge_sequence_indices[0][idx]\n",
    "            #print(edge_pair)\n",
    "            edge_pred, edge_hidden_state = edgeModel.forward(batch.edge_sequence[edge_pair[0]:edge_pair[1]], batch.x, batch.edge_index, batch.edge_attr)\n",
    "            edge_target = batch.edge_sequence[edge_pair[0]+1:edge_pair[1]+1]\n",
    "            \n",
    "            loss = loss_fn(node_pred, node_target)\n",
    "            node_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            node_optimizer.step()\n",
    "            print(loss.item())\n",
    "            \n",
    "            loss = loss_fn(edge_pred, edge_target)\n",
    "            edge_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            edge_optimizer.step()\n",
    "            print(loss.item())\n",
    "            \n",
    "        #pred = nodeModel.forward(batch.node_sequence[0:-1])\n",
    "        #loss = loss_fn(pred, batch.node_sequence[1:])\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        #self.testnnconv = \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(NNConv(input_dim, hidden_dim, nnconvnn(input_dim, hidden_dim)))\n",
    "        self.convs.append(NNConv(hidden_dim, hidden_dim, nnconvnn(hidden_dim, hidden_dim)))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        F.dropout.training = self.training\n",
    "        #x = self.testnnconv(x, edge_index, edge_attr)\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        x = self.linear(x)\n",
    "        if self.return_embeds == False:\n",
    "            x = self.softmax(x)\n",
    "        out = x\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loader, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    for batch in iter(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        # o = o[train_idx] # we train on the whole graphs now\n",
    "        #print(o)\n",
    "        #print(batch.y.squeeze())\n",
    "        loss = loss_fn(o, batch.y.squeeze(1))\n",
    "        #########################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, loader, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        batch = batch.to(device)\n",
    "        ############# Your code here ############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. No index slicing here\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        #########################################\n",
    "\n",
    "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        #if count == 1:\n",
    "            #print(y_pred, batch.y)\n",
    "        \n",
    "        train_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        valid_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        test_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "\n",
    "    train_acc /= count\n",
    "    valid_acc /= count\n",
    "    test_acc /= count\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "    }\n",
    "    args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    model = GCN(30, args['hidden_dim'],\n",
    "              14, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.1944, Train: 66.89%, Valid: 66.89% Test: 66.89%\n",
      "Epoch: 02, Loss: 0.7170, Train: 72.64%, Valid: 72.64% Test: 72.64%\n",
      "Epoch: 03, Loss: 0.6501, Train: 74.53%, Valid: 74.53% Test: 74.53%\n",
      "Epoch: 04, Loss: 0.4657, Train: 75.57%, Valid: 75.57% Test: 75.57%\n",
      "Epoch: 05, Loss: 0.4036, Train: 75.54%, Valid: 75.54% Test: 75.54%\n",
      "Epoch: 06, Loss: 0.4142, Train: 75.61%, Valid: 75.61% Test: 75.61%\n",
      "Epoch: 07, Loss: 0.3661, Train: 77.97%, Valid: 77.97% Test: 77.97%\n",
      "Epoch: 08, Loss: 0.3721, Train: 79.85%, Valid: 79.85% Test: 79.85%\n",
      "Epoch: 09, Loss: 0.3096, Train: 80.93%, Valid: 80.93% Test: 80.93%\n",
      "Epoch: 10, Loss: 0.3449, Train: 80.03%, Valid: 80.03% Test: 80.03%\n",
      "Epoch: 11, Loss: 0.3756, Train: 81.84%, Valid: 81.84% Test: 81.84%\n",
      "Epoch: 12, Loss: 0.2886, Train: 79.02%, Valid: 79.02% Test: 79.02%\n",
      "Epoch: 13, Loss: 0.3435, Train: 83.23%, Valid: 83.23% Test: 83.23%\n",
      "Epoch: 14, Loss: 0.3259, Train: 85.24%, Valid: 85.24% Test: 85.24%\n",
      "Epoch: 15, Loss: 0.2378, Train: 86.44%, Valid: 86.44% Test: 86.44%\n",
      "Epoch: 16, Loss: 0.2188, Train: 79.59%, Valid: 79.59% Test: 79.59%\n",
      "Epoch: 17, Loss: 0.3246, Train: 82.01%, Valid: 82.01% Test: 82.01%\n",
      "Epoch: 18, Loss: 0.2081, Train: 85.60%, Valid: 85.60% Test: 85.60%\n",
      "Epoch: 19, Loss: 0.2428, Train: 87.90%, Valid: 87.90% Test: 87.90%\n",
      "Epoch: 20, Loss: 0.2182, Train: 80.85%, Valid: 80.85% Test: 80.85%\n",
      "Epoch: 21, Loss: 0.1706, Train: 87.97%, Valid: 87.97% Test: 87.97%\n",
      "Epoch: 22, Loss: 0.2968, Train: 86.19%, Valid: 86.19% Test: 86.19%\n",
      "Epoch: 23, Loss: 0.2417, Train: 90.28%, Valid: 90.28% Test: 90.28%\n",
      "Epoch: 24, Loss: 0.1411, Train: 89.91%, Valid: 89.91% Test: 89.91%\n",
      "Epoch: 25, Loss: 0.2349, Train: 90.41%, Valid: 90.41% Test: 90.41%\n",
      "Epoch: 26, Loss: 0.2296, Train: 88.76%, Valid: 88.76% Test: 88.76%\n",
      "Epoch: 27, Loss: 0.2621, Train: 91.15%, Valid: 91.15% Test: 91.15%\n",
      "Epoch: 28, Loss: 0.1569, Train: 90.74%, Valid: 90.74% Test: 90.74%\n",
      "Epoch: 29, Loss: 0.1318, Train: 92.58%, Valid: 92.58% Test: 92.58%\n",
      "Epoch: 30, Loss: 0.0976, Train: 91.43%, Valid: 91.43% Test: 91.43%\n",
      "Epoch: 31, Loss: 0.1681, Train: 91.70%, Valid: 91.70% Test: 91.70%\n",
      "Epoch: 32, Loss: 0.3574, Train: 89.84%, Valid: 89.84% Test: 89.84%\n",
      "Epoch: 33, Loss: 0.1642, Train: 83.23%, Valid: 83.23% Test: 83.23%\n",
      "Epoch: 34, Loss: 0.1369, Train: 88.86%, Valid: 88.86% Test: 88.86%\n",
      "Epoch: 35, Loss: 0.1345, Train: 93.12%, Valid: 93.12% Test: 93.12%\n",
      "Epoch: 36, Loss: 0.2281, Train: 92.39%, Valid: 92.39% Test: 92.39%\n",
      "Epoch: 37, Loss: 0.0567, Train: 94.27%, Valid: 94.27% Test: 94.27%\n",
      "Epoch: 38, Loss: 0.0652, Train: 93.86%, Valid: 93.86% Test: 93.86%\n",
      "Epoch: 39, Loss: 0.0877, Train: 94.25%, Valid: 94.25% Test: 94.25%\n",
      "Epoch: 40, Loss: 0.0418, Train: 94.55%, Valid: 94.55% Test: 94.55%\n",
      "Epoch: 41, Loss: 0.0472, Train: 93.71%, Valid: 93.71% Test: 93.71%\n",
      "Epoch: 42, Loss: 0.0649, Train: 94.46%, Valid: 94.46% Test: 94.46%\n",
      "Epoch: 43, Loss: 0.0807, Train: 94.79%, Valid: 94.79% Test: 94.79%\n",
      "Epoch: 44, Loss: 0.2919, Train: 93.40%, Valid: 93.40% Test: 93.40%\n",
      "Epoch: 45, Loss: 0.0402, Train: 94.38%, Valid: 94.38% Test: 94.38%\n",
      "Epoch: 46, Loss: 0.1923, Train: 90.43%, Valid: 90.43% Test: 90.43%\n",
      "Epoch: 47, Loss: 0.0497, Train: 94.54%, Valid: 94.54% Test: 94.54%\n",
      "Epoch: 48, Loss: 0.0750, Train: 93.70%, Valid: 93.70% Test: 93.70%\n",
      "Epoch: 49, Loss: 0.0650, Train: 94.19%, Valid: 94.19% Test: 94.19%\n",
      "Epoch: 50, Loss: 0.0899, Train: 94.96%, Valid: 94.96% Test: 94.96%\n",
      "Epoch: 51, Loss: 0.1130, Train: 95.36%, Valid: 95.36% Test: 95.36%\n",
      "Epoch: 52, Loss: 0.1710, Train: 92.79%, Valid: 92.79% Test: 92.79%\n",
      "Epoch: 53, Loss: 0.1214, Train: 94.55%, Valid: 94.55% Test: 94.55%\n",
      "Epoch: 54, Loss: 0.0303, Train: 95.58%, Valid: 95.58% Test: 95.58%\n",
      "Epoch: 55, Loss: 0.0599, Train: 95.72%, Valid: 95.72% Test: 95.72%\n",
      "Epoch: 56, Loss: 0.0282, Train: 94.79%, Valid: 94.79% Test: 94.79%\n",
      "Epoch: 57, Loss: 0.0231, Train: 94.54%, Valid: 94.54% Test: 94.54%\n",
      "Epoch: 58, Loss: 0.0583, Train: 94.07%, Valid: 94.07% Test: 94.07%\n",
      "Epoch: 59, Loss: 0.0671, Train: 95.25%, Valid: 95.25% Test: 95.25%\n",
      "Epoch: 60, Loss: 0.0661, Train: 94.95%, Valid: 94.95% Test: 94.95%\n",
      "Epoch: 61, Loss: 0.0949, Train: 95.04%, Valid: 95.04% Test: 95.04%\n",
      "Epoch: 62, Loss: 0.0665, Train: 94.68%, Valid: 94.68% Test: 94.68%\n",
      "Epoch: 63, Loss: 0.0248, Train: 95.82%, Valid: 95.82% Test: 95.82%\n",
      "Epoch: 64, Loss: 0.1172, Train: 94.49%, Valid: 94.49% Test: 94.49%\n",
      "Epoch: 65, Loss: 0.0229, Train: 95.17%, Valid: 95.17% Test: 95.17%\n",
      "Epoch: 66, Loss: 0.2187, Train: 95.48%, Valid: 95.48% Test: 95.48%\n",
      "Epoch: 67, Loss: 0.0234, Train: 96.01%, Valid: 96.01% Test: 96.01%\n",
      "Epoch: 68, Loss: 0.0167, Train: 96.47%, Valid: 96.47% Test: 96.47%\n",
      "Epoch: 69, Loss: 0.0147, Train: 95.39%, Valid: 95.39% Test: 95.39%\n",
      "Epoch: 70, Loss: 0.0852, Train: 95.82%, Valid: 95.82% Test: 95.82%\n",
      "Epoch: 71, Loss: 0.0816, Train: 95.37%, Valid: 95.37% Test: 95.37%\n",
      "Epoch: 72, Loss: 0.2787, Train: 95.82%, Valid: 95.82% Test: 95.82%\n",
      "Epoch: 73, Loss: 0.1199, Train: 94.46%, Valid: 94.46% Test: 94.46%\n",
      "Epoch: 74, Loss: 0.0509, Train: 96.31%, Valid: 96.31% Test: 96.31%\n",
      "Epoch: 75, Loss: 0.0259, Train: 96.11%, Valid: 96.11% Test: 96.11%\n",
      "Epoch: 76, Loss: 0.0321, Train: 95.65%, Valid: 95.65% Test: 95.65%\n",
      "Epoch: 77, Loss: 0.0306, Train: 96.44%, Valid: 96.44% Test: 96.44%\n",
      "Epoch: 78, Loss: 0.0297, Train: 96.29%, Valid: 96.29% Test: 96.29%\n",
      "Epoch: 79, Loss: 0.0074, Train: 94.78%, Valid: 94.78% Test: 94.78%\n",
      "Epoch: 80, Loss: 0.0536, Train: 96.21%, Valid: 96.21% Test: 96.21%\n",
      "Epoch: 81, Loss: 0.0623, Train: 96.84%, Valid: 96.84% Test: 96.84%\n",
      "Epoch: 82, Loss: 0.0457, Train: 96.28%, Valid: 96.28% Test: 96.28%\n",
      "Epoch: 83, Loss: 0.0513, Train: 96.18%, Valid: 96.18% Test: 96.18%\n",
      "Epoch: 84, Loss: 0.0163, Train: 96.04%, Valid: 96.04% Test: 96.04%\n",
      "Epoch: 85, Loss: 0.0297, Train: 95.40%, Valid: 95.40% Test: 95.40%\n",
      "Epoch: 86, Loss: 0.0700, Train: 96.06%, Valid: 96.06% Test: 96.06%\n",
      "Epoch: 87, Loss: 0.0582, Train: 95.29%, Valid: 95.29% Test: 95.29%\n",
      "Epoch: 88, Loss: 0.0113, Train: 95.17%, Valid: 95.17% Test: 95.17%\n",
      "Epoch: 89, Loss: 0.0494, Train: 95.24%, Valid: 95.24% Test: 95.24%\n",
      "Epoch: 90, Loss: 0.0276, Train: 95.55%, Valid: 95.55% Test: 95.55%\n",
      "Epoch: 91, Loss: 0.0085, Train: 95.31%, Valid: 95.31% Test: 95.31%\n",
      "Epoch: 92, Loss: 0.0747, Train: 96.43%, Valid: 96.43% Test: 96.43%\n",
      "Epoch: 93, Loss: 0.0152, Train: 96.54%, Valid: 96.54% Test: 96.54%\n",
      "Epoch: 94, Loss: 0.0521, Train: 96.91%, Valid: 96.91% Test: 96.91%\n",
      "Epoch: 95, Loss: 0.0156, Train: 97.09%, Valid: 97.09% Test: 97.09%\n",
      "Epoch: 96, Loss: 0.0873, Train: 96.98%, Valid: 96.98% Test: 96.98%\n",
      "Epoch: 97, Loss: 0.0253, Train: 96.73%, Valid: 96.73% Test: 96.73%\n",
      "Epoch: 98, Loss: 0.1190, Train: 96.53%, Valid: 96.53% Test: 96.53%\n",
      "Epoch: 99, Loss: 0.0233, Train: 96.99%, Valid: 96.99% Test: 96.99%\n",
      "Epoch: 100, Loss: 0.0133, Train: 96.25%, Valid: 96.25% Test: 96.25%\n"
     ]
    }
   ],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    # reset the parameters to initial random value\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    loss_fn = F.nll_loss\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "        loss = train(model, data_loader, [], optimizer, loss_fn)\n",
    "        result = test(model, test_loader, [], evaluator)\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_acc:.2f}%, '\n",
    "              f'Valid: {100 * valid_acc:.2f}% '\n",
    "              f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(model, data_loader, [], evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
