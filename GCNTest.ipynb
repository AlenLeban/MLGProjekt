{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "from collections import defaultdict\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the modified to_networkx function that doesn't throw exceptions\n",
    "\n",
    "def from_networkx(\n",
    "    G: Any,\n",
    "    group_node_attrs: Optional[Union[List[str], all]] = None,\n",
    "    group_edge_attrs: Optional[Union[List[str], all]] = None,\n",
    ") -> 'torch_geometric.data.Data':\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "        group_node_attrs (List[str] or all, optional): The node attributes to\n",
    "            be concatenated and added to :obj:`data.x`. (default: :obj:`None`)\n",
    "        group_edge_attrs (List[str] or all, optional): The edge attributes to\n",
    "            be concatenated and added to :obj:`data.edge_attr`.\n",
    "            (default: :obj:`None`)\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        All :attr:`group_node_attrs` and :attr:`group_edge_attrs` values must\n",
    "        be numeric.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> edge_index = torch.tensor([\n",
    "        ...     [0, 1, 1, 2, 2, 3],\n",
    "        ...     [1, 0, 2, 1, 3, 2],\n",
    "        ... ])\n",
    "        >>> data = Data(edge_index=edge_index, num_nodes=4)\n",
    "        >>> g = to_networkx(data)\n",
    "        >>> # A `Data` object is returned\n",
    "        >>> from_networkx(g)\n",
    "        Data(edge_index=[2, 6], num_nodes=4)\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "\n",
    "    if isinstance(G, (nx.MultiGraph, nx.MultiDiGraph)):\n",
    "        edges = list(G.edges(keys=False))\n",
    "    else:\n",
    "        edges = list(G.edges)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    if G.number_of_nodes() > 0:\n",
    "        node_attrs = list(next(iter(G.nodes(data=True)))[-1].keys())\n",
    "    else:\n",
    "        node_attrs = {}\n",
    "\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_attrs = list(next(iter(G.edges(data=True)))[-1].keys())\n",
    "    else:\n",
    "        edge_attrs = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        if set(feat_dict.keys()) != set(node_attrs):\n",
    "            raise ValueError('Not all nodes contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        if set(feat_dict.keys()) != set(edge_attrs):\n",
    "            raise ValueError('Not all edges contain the same attributes')\n",
    "        for key, value in feat_dict.items():\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            data[str(key)].append(value)\n",
    "\n",
    "    for key, value in G.graph.items():\n",
    "        key = f'graph_{key}' if key in node_attrs else key\n",
    "        data[str(key)] = value\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (tuple, list)) and isinstance(value[0], Tensor):\n",
    "            data[key] = torch.stack(value, dim=0)\n",
    "        else:\n",
    "            try:\n",
    "                data[key] = torch.tensor(value)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data = Data.from_dict(data)\n",
    "\n",
    "    if group_node_attrs is all:\n",
    "        group_node_attrs = list(node_attrs)\n",
    "    if group_node_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_node_attrs:\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if group_edge_attrs is all:\n",
    "        group_edge_attrs = list(edge_attrs)\n",
    "    if group_edge_attrs is not None:\n",
    "        xs = []\n",
    "        for key in group_edge_attrs:\n",
    "            key = f'edge_{key}' if key in node_attrs else key\n",
    "            x = data[key]\n",
    "            x = x.view(-1, 1) if x.dim() <= 1 else x\n",
    "            xs.append(x)\n",
    "            del data[key]\n",
    "        data.edge_attr = torch.cat(xs, dim=-1)\n",
    "\n",
    "    if data.x is None and data.pos is None:\n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "def to_edge_index(adj: Union[Tensor, SparseTensor]) -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"Converts a :class:`torch.sparse.Tensor` or a\n",
    "    :class:`torch_sparse.SparseTensor` to edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.sparse.Tensor or SparseTensor): The adjacency matrix.\n",
    "\n",
    "    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "        ...                            [1, 0, 2, 1, 3, 2]])\n",
    "        >>> adj = to_torch_coo_tensor(edge_index)\n",
    "        >>> to_edge_index(adj)\n",
    "        (tensor([[0, 1, 1, 2, 2, 3],\n",
    "                [1, 0, 2, 1, 3, 2]]),\n",
    "        tensor([1., 1., 1., 1., 1., 1.]))\n",
    "    \"\"\"\n",
    "    if isinstance(adj, SparseTensor):\n",
    "        row, col, value = adj.coo()\n",
    "        if value is None:\n",
    "            value = torch.ones(row.size(0), device=row.device)\n",
    "        return torch.stack([row, col], dim=0), value\n",
    "\n",
    "    if adj.requires_grad:\n",
    "        # Calling adj._values() will return a detached tensor.\n",
    "        # Use `adj.coalesce().values()` instead to track gradients.\n",
    "        adj = adj.coalesce()\n",
    "        return adj.indices(), adj.values()\n",
    "\n",
    "    return adj._indices(), adj._values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "        \"Point\": 0,\n",
    "        \"Line\": 1,\n",
    "        \"Circle\": 2,\n",
    "        \"Ellipse\": 3,\n",
    "        \"Spline\": 4,\n",
    "        \"Conic\": 5,\n",
    "        \"Arc\": 6,\n",
    "        \"External\": 7,\n",
    "        \"Stop\": 8,\n",
    "        \"Unknown\": 9,\n",
    "        \"SN_Start\": 11,\n",
    "        \"SN_End\": 12,\n",
    "        \"SN_Center\": 13\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "def get_sketch_features(graph, feature_dim):\n",
    "    x = torch.zeros([graph.num_nodes, feature_dim])\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx, p in enumerate(graph.parameters):\n",
    "        \n",
    "        # add one hot encoding to feature vector for node label\n",
    "        onePos = label_dict[graph.label[idx]]/7\n",
    "        for i in range(0, 14):\n",
    "            x[idx, i] = 1 if onePos==i else 0\n",
    "        \n",
    "        # convert label text into a feature value\n",
    "        x[idx, 14] = label_dict[graph.label[idx]]/7\n",
    "        \n",
    "        param_dict = json.loads(p)\n",
    "        for i, k in enumerate(param_dict.keys()):\n",
    "            \n",
    "            if i+2 == feature_dim:\n",
    "                break\n",
    "            \n",
    "            # convert each parameter value into a feature value\n",
    "            x[idx, i+15] = float(param_dict[k])\n",
    "        \n",
    "        x[idx, -1] = degree(graph.edge_index[0], graph.num_nodes)[idx]\n",
    "        #print(idx, p)\n",
    "        #print(x[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_attr_y(graph):\n",
    "    y = torch.zeros([graph.num_nodes, 1], dtype=torch.int64)\n",
    "    #rint(graph.label)\n",
    "    \n",
    "    \n",
    "    for i, l in enumerate(graph.label):\n",
    "        y[i, 0] = label_dict[l]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sketch_adj(graph):\n",
    "    tst = T.ToSparseTensor()\n",
    "    return tst(graph).adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_large_graph(graphs):\n",
    "    bestN = 0\n",
    "    bestI = 0\n",
    "    for i, g in enumerate(graphs):\n",
    "        if len(g) > bestN:\n",
    "            bestN = len(g)\n",
    "            bestI = i\n",
    "    #print(bestI)\n",
    "    return graphs[bestI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def custom_collate(batch, b):\\n    print(\"AAAAA\")\\n    edge_label_batch = []\\n    for b in batch:\\n        #print(b.edge_label)\\n        edge_label_batch.append(b.edge_label)\\n    return edge_label_batch'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom dataset class of custom attributes\n",
    "from torch_geometric.data import Dataset\n",
    "class SketchgraphDataset(Dataset):\n",
    "    def __init__(self, start_idx, end_idx, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.data = []\n",
    "        seq_data = flat_array.load_dictionary_flat('datasets/sg_t16_validation.npy')\n",
    "        print(len(seq_data['sequences']))\n",
    "\n",
    "        #test_graph_seq = find_large_graph(seq_data['sequences'])\n",
    "        test_graph_seq = seq_data['sequences'][206778]\n",
    "        test_graph_seq1 = seq_data['sequences'][10]\n",
    "\n",
    "        sketchgraps_list = seq_data['sequences'][start_idx:end_idx]\n",
    "\n",
    "        for sg in sketchgraps_list:\n",
    "            #print(test_graph_seq)\n",
    "\n",
    "            # convert first to pyGraphViz graph using sketchgraph's function\n",
    "            pgv_graph = sketchgraphs.data.sequence.pgvgraph_from_sequence(sg)\n",
    "            #print(pgv_graph)\n",
    "\n",
    "            # then to networkx graph\n",
    "            nx_graph = nx.Graph(pgv_graph)\n",
    "            #print(nx_graph)\n",
    "\n",
    "            # finally to pyTorch graph\n",
    "            graph = from_networkx(nx_graph)\n",
    "\n",
    "            # next we need to add required attributes: x, y, adj_t\n",
    "            graph.x = get_sketch_features(graph, 30)\n",
    "            graph.y = get_sketch_attr_y(graph)\n",
    "            graph.adj_t = get_sketch_adj(graph)\n",
    "            #print(graph.x)\n",
    "            graph.edge_index = to_edge_index(graph.adj_t)[0]\n",
    "            if not hasattr(graph, 'edge_label'):\n",
    "                continue\n",
    "            #print(len(graph.edge_label))\n",
    "            self.data.append(graph)\n",
    "            \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\"\"\"def custom_collate(batch, b):\n",
    "    print(\"AAAAA\")\n",
    "    edge_label_batch = []\n",
    "    for b in batch:\n",
    "        #print(b.edge_label)\n",
    "        edge_label_batch.append(b.edge_label)\n",
    "    return edge_label_batch\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315228\n",
      "315228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 3.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 3.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 2.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sketchgraphs\n",
    "import networkx as nx\n",
    "from sketchgraphs.data import flat_array\n",
    "\n",
    "train_dataset = SketchgraphDataset(0, 1023)\n",
    "test_dataset = SketchgraphDataset(1024, 2047)\n",
    "    \n",
    "data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "next(iter(data_loader)).x\n",
    "#for b in iter(data_loader):\n",
    "#    print(b)\n",
    "#print(graph)\n",
    "#print(graph.x)\n",
    "#print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n",
      "Device: cuda\n",
      "tensor([     0,      1,      2,  ..., 169145, 169148, 169251])\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    dataset_name = 'ogbn-arxiv'\n",
    "    dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    #data = batch\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the adjacency matrix to symmetric\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    #print(data.y)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # If you use GPU, the device should be cuda\n",
    "    print('Device: {}'.format(device))\n",
    "    data = data.to(device)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    print(split_idx['train'])\n",
    "    #train_idx = torch.LongTensor(range(0, data.num_nodes)).to(device)\n",
    "    #print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## For more information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        for i in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        F.dropout.training = self.training\n",
    "        \n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = self.linear(x)\n",
    "        if self.return_embeds == False:\n",
    "            x = self.softmax(x)\n",
    "        out = x\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    for batch in iter(loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o = model(batch.x, batch.edge_index)\n",
    "        # o = o[train_idx] # we train on the whole graphs now\n",
    "        #print(o)\n",
    "        #print(batch.y.squeeze())\n",
    "        loss = loss_fn(o, batch.y.squeeze(1))\n",
    "        #########################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, loader, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "    train_acc = 0\n",
    "    valid_acc = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        count+=1\n",
    "        batch = batch.to(device)\n",
    "        ############# Your code here ############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. No index slicing here\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        #########################################\n",
    "\n",
    "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        #if count == 1:\n",
    "            #print(y_pred, batch.y)\n",
    "        \n",
    "        train_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        valid_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "        test_acc += evaluator.eval({\n",
    "            'y_true': batch.y,\n",
    "            'y_pred': y_pred,\n",
    "        })['acc']\n",
    "\n",
    "    train_acc /= count\n",
    "    valid_acc /= count\n",
    "    test_acc /= count\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "    }\n",
    "    args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    model = GCN(30, args['hidden_dim'],\n",
    "              14, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.3842, Train: 31.00%, Valid: 31.00% Test: 31.00%\n",
      "Epoch: 02, Loss: 1.2447, Train: 34.00%, Valid: 34.00% Test: 34.00%\n",
      "Epoch: 03, Loss: 1.2221, Train: 37.71%, Valid: 37.71% Test: 37.71%\n",
      "Epoch: 04, Loss: 1.1066, Train: 38.21%, Valid: 38.21% Test: 38.21%\n",
      "Epoch: 05, Loss: 1.0437, Train: 37.86%, Valid: 37.86% Test: 37.86%\n",
      "Epoch: 06, Loss: 1.1269, Train: 41.01%, Valid: 41.01% Test: 41.01%\n",
      "Epoch: 07, Loss: 1.0657, Train: 40.77%, Valid: 40.77% Test: 40.77%\n",
      "Epoch: 08, Loss: 1.0630, Train: 44.11%, Valid: 44.11% Test: 44.11%\n",
      "Epoch: 09, Loss: 1.0695, Train: 40.54%, Valid: 40.54% Test: 40.54%\n",
      "Epoch: 10, Loss: 1.0215, Train: 42.80%, Valid: 42.80% Test: 42.80%\n",
      "Epoch: 11, Loss: 0.9664, Train: 45.92%, Valid: 45.92% Test: 45.92%\n",
      "Epoch: 12, Loss: 1.0064, Train: 42.78%, Valid: 42.78% Test: 42.78%\n",
      "Epoch: 13, Loss: 1.0510, Train: 42.29%, Valid: 42.29% Test: 42.29%\n",
      "Epoch: 14, Loss: 1.1001, Train: 44.24%, Valid: 44.24% Test: 44.24%\n",
      "Epoch: 15, Loss: 1.1003, Train: 41.72%, Valid: 41.72% Test: 41.72%\n",
      "Epoch: 16, Loss: 1.0292, Train: 48.76%, Valid: 48.76% Test: 48.76%\n",
      "Epoch: 17, Loss: 1.0135, Train: 48.74%, Valid: 48.74% Test: 48.74%\n",
      "Epoch: 18, Loss: 0.9168, Train: 45.72%, Valid: 45.72% Test: 45.72%\n",
      "Epoch: 19, Loss: 1.0644, Train: 45.54%, Valid: 45.54% Test: 45.54%\n",
      "Epoch: 20, Loss: 1.0664, Train: 45.52%, Valid: 45.52% Test: 45.52%\n",
      "Epoch: 21, Loss: 0.9789, Train: 51.90%, Valid: 51.90% Test: 51.90%\n",
      "Epoch: 22, Loss: 1.0181, Train: 45.49%, Valid: 45.49% Test: 45.49%\n",
      "Epoch: 23, Loss: 0.9667, Train: 52.98%, Valid: 52.98% Test: 52.98%\n",
      "Epoch: 24, Loss: 0.9160, Train: 48.13%, Valid: 48.13% Test: 48.13%\n",
      "Epoch: 25, Loss: 0.9053, Train: 52.79%, Valid: 52.79% Test: 52.79%\n",
      "Epoch: 26, Loss: 0.8734, Train: 47.34%, Valid: 47.34% Test: 47.34%\n",
      "Epoch: 27, Loss: 0.9281, Train: 49.46%, Valid: 49.46% Test: 49.46%\n",
      "Epoch: 28, Loss: 0.9631, Train: 50.59%, Valid: 50.59% Test: 50.59%\n",
      "Epoch: 29, Loss: 0.9138, Train: 50.46%, Valid: 50.46% Test: 50.46%\n",
      "Epoch: 30, Loss: 0.9900, Train: 51.52%, Valid: 51.52% Test: 51.52%\n",
      "Epoch: 31, Loss: 0.9097, Train: 45.28%, Valid: 45.28% Test: 45.28%\n",
      "Epoch: 32, Loss: 0.8562, Train: 53.10%, Valid: 53.10% Test: 53.10%\n",
      "Epoch: 33, Loss: 0.8788, Train: 50.65%, Valid: 50.65% Test: 50.65%\n",
      "Epoch: 34, Loss: 0.8356, Train: 52.71%, Valid: 52.71% Test: 52.71%\n",
      "Epoch: 35, Loss: 0.9960, Train: 52.59%, Valid: 52.59% Test: 52.59%\n",
      "Epoch: 36, Loss: 0.9436, Train: 52.27%, Valid: 52.27% Test: 52.27%\n",
      "Epoch: 37, Loss: 0.9189, Train: 55.48%, Valid: 55.48% Test: 55.48%\n",
      "Epoch: 38, Loss: 0.8735, Train: 52.53%, Valid: 52.53% Test: 52.53%\n",
      "Epoch: 39, Loss: 1.1633, Train: 55.65%, Valid: 55.65% Test: 55.65%\n",
      "Epoch: 40, Loss: 1.0019, Train: 55.45%, Valid: 55.45% Test: 55.45%\n",
      "Epoch: 41, Loss: 0.9013, Train: 54.98%, Valid: 54.98% Test: 54.98%\n",
      "Epoch: 42, Loss: 0.8107, Train: 55.40%, Valid: 55.40% Test: 55.40%\n",
      "Epoch: 43, Loss: 0.8947, Train: 54.14%, Valid: 54.14% Test: 54.14%\n",
      "Epoch: 44, Loss: 0.9365, Train: 52.03%, Valid: 52.03% Test: 52.03%\n",
      "Epoch: 45, Loss: 0.9875, Train: 54.71%, Valid: 54.71% Test: 54.71%\n",
      "Epoch: 46, Loss: 0.8519, Train: 53.57%, Valid: 53.57% Test: 53.57%\n",
      "Epoch: 47, Loss: 1.0125, Train: 53.81%, Valid: 53.81% Test: 53.81%\n",
      "Epoch: 48, Loss: 0.8335, Train: 49.96%, Valid: 49.96% Test: 49.96%\n",
      "Epoch: 49, Loss: 0.8315, Train: 56.16%, Valid: 56.16% Test: 56.16%\n",
      "Epoch: 50, Loss: 0.7513, Train: 52.23%, Valid: 52.23% Test: 52.23%\n",
      "Epoch: 51, Loss: 0.8908, Train: 53.53%, Valid: 53.53% Test: 53.53%\n",
      "Epoch: 52, Loss: 0.9046, Train: 51.69%, Valid: 51.69% Test: 51.69%\n",
      "Epoch: 53, Loss: 0.8870, Train: 50.88%, Valid: 50.88% Test: 50.88%\n",
      "Epoch: 54, Loss: 0.8592, Train: 55.74%, Valid: 55.74% Test: 55.74%\n",
      "Epoch: 55, Loss: 0.8192, Train: 54.74%, Valid: 54.74% Test: 54.74%\n",
      "Epoch: 56, Loss: 0.8391, Train: 55.72%, Valid: 55.72% Test: 55.72%\n",
      "Epoch: 57, Loss: 0.8770, Train: 52.90%, Valid: 52.90% Test: 52.90%\n",
      "Epoch: 58, Loss: 0.8505, Train: 57.43%, Valid: 57.43% Test: 57.43%\n",
      "Epoch: 59, Loss: 0.8364, Train: 57.88%, Valid: 57.88% Test: 57.88%\n",
      "Epoch: 60, Loss: 0.8182, Train: 56.62%, Valid: 56.62% Test: 56.62%\n",
      "Epoch: 61, Loss: 0.9088, Train: 54.76%, Valid: 54.76% Test: 54.76%\n",
      "Epoch: 62, Loss: 0.8582, Train: 54.39%, Valid: 54.39% Test: 54.39%\n",
      "Epoch: 63, Loss: 0.8054, Train: 51.51%, Valid: 51.51% Test: 51.51%\n",
      "Epoch: 64, Loss: 0.8722, Train: 58.32%, Valid: 58.32% Test: 58.32%\n",
      "Epoch: 65, Loss: 0.9087, Train: 56.88%, Valid: 56.88% Test: 56.88%\n",
      "Epoch: 66, Loss: 0.9220, Train: 57.30%, Valid: 57.30% Test: 57.30%\n",
      "Epoch: 67, Loss: 0.9431, Train: 52.60%, Valid: 52.60% Test: 52.60%\n",
      "Epoch: 68, Loss: 0.7225, Train: 52.97%, Valid: 52.97% Test: 52.97%\n",
      "Epoch: 69, Loss: 0.7985, Train: 58.34%, Valid: 58.34% Test: 58.34%\n",
      "Epoch: 70, Loss: 0.8578, Train: 58.82%, Valid: 58.82% Test: 58.82%\n",
      "Epoch: 71, Loss: 0.7400, Train: 58.76%, Valid: 58.76% Test: 58.76%\n",
      "Epoch: 72, Loss: 0.8278, Train: 57.78%, Valid: 57.78% Test: 57.78%\n",
      "Epoch: 73, Loss: 0.7708, Train: 58.01%, Valid: 58.01% Test: 58.01%\n",
      "Epoch: 74, Loss: 0.7822, Train: 57.00%, Valid: 57.00% Test: 57.00%\n",
      "Epoch: 75, Loss: 0.7228, Train: 55.48%, Valid: 55.48% Test: 55.48%\n",
      "Epoch: 76, Loss: 0.7624, Train: 56.94%, Valid: 56.94% Test: 56.94%\n",
      "Epoch: 77, Loss: 0.8141, Train: 58.77%, Valid: 58.77% Test: 58.77%\n",
      "Epoch: 78, Loss: 0.7743, Train: 56.88%, Valid: 56.88% Test: 56.88%\n",
      "Epoch: 79, Loss: 0.7900, Train: 56.86%, Valid: 56.86% Test: 56.86%\n",
      "Epoch: 80, Loss: 0.8650, Train: 54.56%, Valid: 54.56% Test: 54.56%\n",
      "Epoch: 81, Loss: 0.8517, Train: 60.33%, Valid: 60.33% Test: 60.33%\n",
      "Epoch: 82, Loss: 0.8101, Train: 60.34%, Valid: 60.34% Test: 60.34%\n",
      "Epoch: 83, Loss: 0.8915, Train: 54.67%, Valid: 54.67% Test: 54.67%\n",
      "Epoch: 84, Loss: 0.7696, Train: 57.90%, Valid: 57.90% Test: 57.90%\n",
      "Epoch: 85, Loss: 0.7969, Train: 59.38%, Valid: 59.38% Test: 59.38%\n",
      "Epoch: 86, Loss: 0.8455, Train: 57.85%, Valid: 57.85% Test: 57.85%\n",
      "Epoch: 87, Loss: 0.8318, Train: 57.36%, Valid: 57.36% Test: 57.36%\n",
      "Epoch: 88, Loss: 0.9177, Train: 58.56%, Valid: 58.56% Test: 58.56%\n",
      "Epoch: 89, Loss: 0.7443, Train: 59.07%, Valid: 59.07% Test: 59.07%\n",
      "Epoch: 90, Loss: 0.7530, Train: 59.48%, Valid: 59.48% Test: 59.48%\n",
      "Epoch: 91, Loss: 0.7949, Train: 60.21%, Valid: 60.21% Test: 60.21%\n",
      "Epoch: 92, Loss: 0.8090, Train: 56.50%, Valid: 56.50% Test: 56.50%\n",
      "Epoch: 93, Loss: 0.7004, Train: 59.10%, Valid: 59.10% Test: 59.10%\n",
      "Epoch: 94, Loss: 0.8645, Train: 58.53%, Valid: 58.53% Test: 58.53%\n",
      "Epoch: 95, Loss: 0.7972, Train: 59.91%, Valid: 59.91% Test: 59.91%\n",
      "Epoch: 96, Loss: 0.9007, Train: 60.57%, Valid: 60.57% Test: 60.57%\n",
      "Epoch: 97, Loss: 0.8526, Train: 60.24%, Valid: 60.24% Test: 60.24%\n",
      "Epoch: 98, Loss: 0.8108, Train: 54.39%, Valid: 54.39% Test: 54.39%\n",
      "Epoch: 99, Loss: 0.8349, Train: 59.75%, Valid: 59.75% Test: 59.75%\n",
      "Epoch: 100, Loss: 0.7588, Train: 55.85%, Valid: 55.85% Test: 55.85%\n"
     ]
    }
   ],
   "source": [
    "# Please do not change these args\n",
    "# Training should take <10min using GPU runtime\n",
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    # reset the parameters to initial random value\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    loss_fn = F.nll_loss\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "        loss = train(model, data_loader, [], optimizer, loss_fn)\n",
    "        result = test(model, test_loader, [], evaluator)\n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_acc:.2f}%, '\n",
    "              f'Valid: {100 * valid_acc:.2f}% '\n",
    "              f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(model, data_loader, [], evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
